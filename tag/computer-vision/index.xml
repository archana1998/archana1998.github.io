<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computer Vision | Archana Swaminathan</title>
    <link>https://archana1998.github.io/tag/computer-vision/</link>
      <atom:link href="https://archana1998.github.io/tag/computer-vision/index.xml" rel="self" type="application/rss+xml" />
    <description>Computer Vision</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Archana Swaminathan, 2020 Â©</copyright><lastBuildDate>Tue, 25 Aug 2020 18:36:00 +0530</lastBuildDate>
    <image>
      <url>https://archana1998.github.io/images/icon_hu616effff6bc497e1f3ccd40e4a444d66_14554_512x512_fill_lanczos_center_2.png</url>
      <title>Computer Vision</title>
      <link>https://archana1998.github.io/tag/computer-vision/</link>
    </image>
    
    <item>
      <title>Summer School Series: Lecture 6 by Arsha Nagrani</title>
      <link>https://archana1998.github.io/post/arsha-nagrani/</link>
      <pubDate>Tue, 25 Aug 2020 18:36:00 +0530</pubDate>
      <guid>https://archana1998.github.io/post/arsha-nagrani/</guid>
      <description>&lt;p&gt;This final lecture was delivered by &lt;a href =&#34;http://www.robots.ox.ac.uk/~arsha/&#34;&gt;Arsha Nagrani&lt;/a&gt;, a recent Ph.D. graduate from Oxford University&amp;rsquo;s VGG group, and an incoming research scientist at Google Research. Her talk was called &lt;b&gt;Multimodality for Video Understanding&lt;/b&gt;.&lt;/p&gt;
&lt;h3 id=&#34;video-understanding&#34;&gt;Video Understanding&lt;/h3&gt;
&lt;p&gt;Videos provide us with far more information than images. Multimodal refers to many mediums for learning, here it can be time, sound and speech. Videos are all around us (30k newly created content videos are uploaded to YouTube every &lt;b&gt;hour&lt;/b&gt;).
However, these have high dimensionality and are difficult to process and annotate.&lt;/p&gt;
&lt;h4 id=&#34;complementarity-among-signals&#34;&gt;Complementarity among signals&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Vision (scene)&lt;/li&gt;
&lt;li&gt;Sound (content of speech)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;redundancy-between-signals&#34;&gt;Redundancy between signals&lt;/h4&gt;
&lt;p&gt;Helps recognize person, face+sound, thus can be a useful form of weak supervision. The redundant information comes from background sounds, foreground audio, signals identified from speech and the content of speech.&lt;/p&gt;
&lt;p&gt;Thus, best way to exploit multimodal nature of videos is to work with the complementarity and redundancy.&lt;/p&gt;
&lt;h4 id=&#34;suitable-tasks&#34;&gt;Suitable tasks&lt;/h4&gt;
&lt;p&gt;Suitable tasks for video understanding are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Video classification&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;single label&lt;/li&gt;
&lt;li&gt;infinite number of possible classes&lt;/li&gt;
&lt;li&gt;ambiguity in the label space&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Action recognition: more fine grained, the motion is important, human centric&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It is important to note that labelling actions in videos is extremely expensive and existing models do not generalize well to new domains.&lt;/p&gt;
&lt;p&gt;In this context, can we use speech as a form of supervision? For example, narrated video clips and lifestyle Vlogs.&lt;/p&gt;
&lt;h3 id=&#34;movies&#34;&gt;Movies&lt;/h3&gt;
&lt;p&gt;General domain of movies: people speak about their actions. However, sometimes speech is completely unrelated, giving us noise. We need to learn when speech matches action. An example of work in this field is &lt;a href =&#34;https://arxiv.org/abs/1912.06430&#34;&gt;End-to-End Learning of Visual Representations from Uncurated Instructional Videos&lt;/a&gt;. This work reduces noise by using the MIL-NCE loss.&lt;/p&gt;
&lt;p&gt;Can we first train a model to recognize actions and then see if it should be used for supervision? An interesting discovery Arsha made was using Movie Screenplays, that contain both speech segments and scene directions with actions. Using this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can obtain speech-action pairs&lt;/li&gt;
&lt;li&gt;Retrieve speech segments with verbs&lt;/li&gt;
&lt;li&gt;Train the &lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/research/speech2action/&#34;&gt;Speech2Action&lt;/a&gt; model to predict action, with a BERT-Backbone (movie scripts scraped from IMSDB)&lt;/li&gt;
&lt;li&gt;Apply to closed captions of unlabelled videos&lt;/li&gt;
&lt;li&gt;Apply to large movie corpus&lt;/li&gt;
&lt;/ul&gt;





  
  











&lt;figure id=&#34;figure-speech2action-model&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/arsha-nagrani/1_hu3dfb93f108aa0e1c42c1039d245e09c2_102757_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;784&#34; height=&#34;358&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Speech2Action model
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The Speech2Action model recognizes rare actions, and is a visual classifier on weakly labelled data (S3D-G model with cross-entropy loss)&lt;/p&gt;
&lt;p&gt;Evaluation is done on the AVA and HMDB-51 (transfer learning) datasets. It gets abstract actions like &lt;b&gt;count&lt;/b&gt; and &lt;b&gt;follow&lt;/b&gt; too.&lt;/p&gt;
&lt;h3 id=&#34;multimodal-complementarity&#34;&gt;Multimodal Complementarity&lt;/h3&gt;
&lt;p&gt;This refers to fusing info from multiple modalities for video text retrieval, like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Finding video corresponding to text queries&lt;/li&gt;
&lt;li&gt;More to videos than just actions like object, scene etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Supervisions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It&amp;rsquo;s not easy to get the complete combination of captions, this is a very subjective task&lt;/li&gt;
&lt;li&gt;Need extremely large datasets&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What Arsha does is rely on expert models trained for different tasks like object detection, face detection, action recognition, OCR etc. These are all applied to the video and features are extracted. The framework is a joint video text embedding, with the video encoder + text query encoder = joint embedding space (similarity should be really high if related). It is necessary for the video encoder to be discriminative and retain specific information.&lt;/p&gt;
&lt;h3 id=&#34;collaborative-gating&#34;&gt;Collaborative Gating&lt;/h3&gt;
&lt;p&gt;For each expert, generate attention mask by looking at the other experts &lt;a href = &#34;https://bmvc2019.org/wp-content/uploads/papers/0363-paper.pdf&#34;&gt; (Use What You Have: Video Retrieval Using Representations From Collaborative Experts, BMVC 2019)&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Trained using bi-directional max margin ranking loss&lt;/li&gt;
&lt;li&gt;Adding in more experts massively increases performance&lt;/li&gt;
&lt;li&gt;Main boost is from the object embeddings&lt;/li&gt;
&lt;/ul&gt;





  
  











&lt;figure id=&#34;figure-collaborative-gating&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/arsha-nagrani/2_huc44033e910d0af756e6885ccfb6b6932_14850_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;150&#34; height=&#34;197&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Collaborative Gating
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Another paper that Arsha discussed was &lt;a href =&#34;https://arxiv.org/abs/2007.10639&#34;&gt; Multi-modal Transformer for Video Retrieval, ECCV 2020 &lt;/a&gt;. This takes features that are taken at different time stamps for each task and aggregrate for the embeddings. The expert and temporal embeddings are added and summed up.&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;More modalities is better (because more complementarity)&lt;/li&gt;
&lt;li&gt;Time (modelling time along with modalities is interesting, some modalities train faster than the others)&lt;/li&gt;
&lt;li&gt;Mid fusion is better than late (Attention truly is what you need)&lt;/li&gt;
&lt;li&gt;Our world is multimodal, it doesn&amp;rsquo;t make sense to work with modalities in isolation&lt;/li&gt;
&lt;li&gt;Use the redundant and complementary information from vision, audio and speech to massively reduce annotations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Open Research Questions:&lt;/b&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Extended Temporal Sequences (beyond 10s):&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Backprop + memory restricts current video architectures to 64 frames&lt;/li&gt;
&lt;li&gt;For longer we rely on pre-extracted features&lt;/li&gt;
&lt;li&gt;Need new datasets to drive innovation&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Moving away from supervision: is an upper bound on self supervision being appraoched?&lt;/li&gt;
&lt;li&gt;The world is multimodal: how do we design good fusion architectures?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Arsha thus concluded a fantastic talk that described the cutting-edge research that her team at Oxford and Google is conducting. It was tremendously insightful and inspirational.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Summer School Series: Lecture 5 by Rahul Sukthankar</title>
      <link>https://archana1998.github.io/post/rahul-sukthankar/</link>
      <pubDate>Tue, 25 Aug 2020 17:19:29 +0530</pubDate>
      <guid>https://archana1998.github.io/post/rahul-sukthankar/</guid>
      <description>&lt;p&gt;This Lecture was presented by &lt;a href=&#34;https://research.google/people/RahulSukthankar/&#34;&gt;Rahul Sukthankar&lt;/a&gt;, a research scientist at Google Research and an Adjunct Professor at Carnegie Mellon University. It was titled &lt;b&gt;Deep Learning in Computer Vision&lt;/b&gt;.&lt;/p&gt;
&lt;h3 id=&#34;popular-computer-vision-tasks&#34;&gt;Popular Computer Vision tasks&lt;/h3&gt;
&lt;p&gt;Some popular tasks in the domain of computer vision include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Image Classification (assign to one class)&lt;/li&gt;
&lt;li&gt;Image Labelling/Object Recognition (multiple classes)&lt;/li&gt;
&lt;li&gt;Object Detection/Localization (predicts bounding box+label, works well for objects but not for fuzzy concepts)&lt;/li&gt;
&lt;li&gt;Semantic Segmentation (Pixel level dense labelling)&lt;/li&gt;
&lt;li&gt;Image Captioning (Description of image in text)&lt;/li&gt;
&lt;li&gt;Human Body Part Segmentation&lt;/li&gt;
&lt;li&gt;Human Pose Estimation (predicting 2D pose keypoints)&lt;/li&gt;
&lt;li&gt;Generating 3D Human Pose and Body Models from an image&lt;/li&gt;
&lt;li&gt;Depth Prediction from a single image (foreground and background semantic segmentation based on a heatmap)&lt;/li&gt;
&lt;li&gt;3D Scene Understanding&lt;/li&gt;
&lt;li&gt;Autonomous navigation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While thinking about a particular problem statement:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We need to take in specific considerations (such as semantic segmentation, classicaiton, object detection etc)&lt;/li&gt;
&lt;li&gt;What is the output? (binary yes/no, bounding box, label/pixel etc)&lt;/li&gt;
&lt;li&gt;How is the training data labelled? (Fully Supervised/Weakly or Cross-Modal/Self-supervised)&lt;/li&gt;
&lt;li&gt;Architecture: Usually a Convolutional Neural Network, but what is the final layer?&lt;/li&gt;
&lt;li&gt;What loss function do we use?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CMU Navlabs (30 years ago) built a self steering car only with an artificial neural network, in the pre-CNN era (&lt;a href= &#34;https://papers.nips.cc/paper/95-alvinn-an-autonomous-land-vehicle-in-a-neural-network.pdf&#34;&gt; ALVINN: AN AUTONOMOUS LAND VEHICLE IN A NEURAL NETWORK&lt;/a&gt;)&lt;/p&gt;
&lt;h3 id=&#34;convolutional-neural-networks&#34;&gt;Convolutional Neural Networks&lt;/h3&gt;
&lt;p&gt;The structure of a convolutional neural network follows as input + conv, relu, pooling layers (hidden layers) + flatten, fully connected and softmax layers (for classification). Key concepts behind CNNs are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Local connectivity (not connected to every pixel, but just a few)&lt;/li&gt;
&lt;li&gt;Shared weights (translational invariance)&lt;/li&gt;
&lt;li&gt;Pooling (reducing dimensions, leads to local patch becoming bigger (filter size))&lt;/li&gt;
&lt;li&gt;Filter stride (cuts down weights, reduces computations)&lt;/li&gt;
&lt;li&gt;Multiple feature maps&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is essential to choose the right conv layer, pooling layer, activation function, loss function, optimization and regularization methods, etc.&lt;/p&gt;
&lt;h4 id=&#34;convolutions&#34;&gt;Convolutions&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;2D vs 3D convolutions: 3D convolutions are used to capture patterns across 3 dimensions, for example Video Understanding and Medical Imaging.&lt;/li&gt;
&lt;li&gt;1x1 convolution: weighed average across channel axis, feature pooling technique to reduce dimensions&lt;/li&gt;
&lt;li&gt;Other types of convolutions are dilated convolutions, regular vs depth wise separable convolutions, grouped convolutions (AlexNet uses it, it reduces computation)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;famous-architectures&#34;&gt;Famous architectures&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Inceptionv1 (2014):





  
  











&lt;figure id=&#34;figure-inception-v1&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/rahul-sukthankar/1_hu0095bbad7d2e55015cc682d2a4670f59_127543_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;762&#34; height=&#34;294&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Inception v1
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ResNet:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Resnet uses skipped connections with residual blocks, the added paths help solve vanishing gradient problems and gives a shorter route for backpropagation&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-residual-blocks&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/rahul-sukthankar/2_hub70c3df55df6f4f2b8fb68ff07d9a5f0_58255_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;557&#34; height=&#34;332&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Residual blocks
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;object-detection-in-images&#34;&gt;Object Detection in Images&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Object Classification: Task of identifying a picture is a dog&lt;/li&gt;
&lt;li&gt;Object Localization: Involves finding class labels as well as a bounding box to show where an object is located&lt;/li&gt;
&lt;li&gt;Object Detection: Localizing with box&lt;/li&gt;
&lt;li&gt;Semantic Segmentation: Dense pixel labelling&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are two ways to do detection:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Sliding window approach: computationally expensive and unbalanced&lt;/li&gt;
&lt;li&gt;Selective search: guessing promising bounding boxes and selecting the best out of them&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;RCNN did this when they extracted region proposals&lt;/li&gt;
&lt;li&gt;Fast RCNN did class labelling+ bounding box prediction at the same time (softmax+bounding box regression)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bounding box evaluation is commonly done by the Intersection over Union Metric
$$ \text{Intersection over Union} = \frac{\text{Area of Overlap}}{\text{Area of Union}}$$
(Ground truth bounding box and predicted bounding box)&lt;/p&gt;
&lt;h3 id=&#34;classic-cnn-vs-fully-convolutional-net&#34;&gt;Classic CNN vs Fully Convolutional Net&lt;/h3&gt;
&lt;p&gt;A classic CNN comprises of a conv+Fully Connected Layer, a fully convolutional layer contains convolutional blocks that help us retain the same number of weights no matter what the input image size is. An example of a fully convolutional net is the U-Net, that is used extensively for semantic segmentation.&lt;/p&gt;
&lt;p&gt;Other applications of a fully convolutional net are : Residual Encoding Decoding, Dense Prediction, Superresolution, Colorization (self supervised)&lt;/p&gt;
&lt;h3 id=&#34;last-layer-activation-function-and-loss-function-summary&#34;&gt;Last-Layer Activation function and Loss Function Summary&lt;/h3&gt;





  
  











&lt;figure id=&#34;figure-functions-to-use&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/rahul-sukthankar/3_hu44f5fe12bf2e14613e9c883ade35e838_85577_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;712&#34; height=&#34;219&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Functions to use
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Any differentiable function can be used as a loss function: even another neural net! (perceptual loss, GAN loss, differentiable renderer etc)&lt;/p&gt;
&lt;p&gt;Rahul concluded this introduction lecture focused in Computer Vision using fully supervised deep learning, with key concepts on CNNs and their extensions and the importance of choosing the right loss function. It was a wonderful lecture with all the concepts beautifully explained.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Summer School Series: Lecture 4 by Cristian Sminchisescu</title>
      <link>https://archana1998.github.io/post/cristian-sminchisescu/</link>
      <pubDate>Tue, 25 Aug 2020 16:11:56 +0530</pubDate>
      <guid>https://archana1998.github.io/post/cristian-sminchisescu/</guid>
      <description>&lt;p&gt;This talk was presented by &lt;a href =&#34;https://research.google/people/CristianSminchisescu/&#34;&gt;Cristian Sminchisescu&lt;/a&gt;, who is a Research Scientist leading a team at Google, and a Professor at Lund University. His talk was titled &lt;b&gt;End-to-end Generative 3D Human Shape and Pose models, and active human sensing&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;3D Human Sensing has many applications, in the field of animation, sports motion, AR/VR, medical industry etc. It is a known fact that humans are very complex, the body has 600 muscles, 200 bones and 200 joints. Clothing that humans wear have folds and wrinkles, there are many different types of garments and cloth-body interactions.&lt;/p&gt;
&lt;h3 id=&#34;challenges&#34;&gt;Challenges&lt;/h3&gt;
&lt;p&gt;Typical challenges in 3D human sensing include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;High dimensionality, articulation and deformation&lt;/li&gt;
&lt;li&gt;Complex appearance variations, clothing and multiple people&lt;/li&gt;
&lt;li&gt;Self occlusion or occlusion by scene objects&lt;/li&gt;
&lt;li&gt;Observation (depth) uncertainty (especially in monocular images)&lt;/li&gt;
&lt;li&gt;Difficult to obtain accurate supervision of humans&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is where we can exploit the power of machine and deep learning, we aim to come up with a learning model that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Understands large volumes of data&lt;/li&gt;
&lt;li&gt;Connects between images and 3D models&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;problems-that-need-to-be-solved&#34;&gt;Problems that need to be solved&lt;/h3&gt;
&lt;p&gt;It is imperative to &lt;b&gt;FIND THE PEOPLE &lt;/b&gt;. We then need to infer their pose, body shape and clothing. The next step would be to recognize actions, behavioral states and social signals that they make, followed by recognizing what objects they use.&lt;/p&gt;
&lt;h3 id=&#34;visual-human-models&#34;&gt;Visual Human Models&lt;/h3&gt;
&lt;p&gt;Different Data types we take into consideration are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multiple Subjects&lt;/li&gt;
&lt;li&gt;Soft Tissue Dynamics&lt;/li&gt;
&lt;li&gt;Clothing
This is all fed into the learning model&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;generative-human-modeling&#34;&gt;Generative Human Modeling&lt;/h3&gt;
&lt;p&gt;Dynamic Human Scans $\mathbf{\xrightarrow[\text{deep learning}]{\text{end to end}}}$ Full Body articulated generative human models. The Dynamic Human Scans are in the form of very dense 3D Point Clouds.&lt;/p&gt;
&lt;h3 id=&#34;ghum-and-ghuml&#34;&gt;GHUM and GHUML&lt;/h3&gt;
&lt;p&gt;Cristian then talked about his paper &lt;a href = &#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Xu_GHUM__GHUML_Generative_3D_Human_Shape_and_Articulated_Pose_CVPR_2020_paper.pdf&#34;&gt;GHUM &amp;amp; GHUML: Generative 3D Human Shape and Articulated Pose Models&lt;/a&gt;
GHUM is the moderate generative model with 10168 vertices and GHUML is the light version with 3190 vertices, however both have a shared skeleton that has minimal parameterization and anatomical joint limits.&lt;/p&gt;
&lt;p&gt;The model faciliates Automatic 3D Landmark detection with multiview renderings, 2D landmark detection and 3D landmark triangulation. Automatic Registration is able to calculate deformations.&lt;/p&gt;
&lt;h3 id=&#34;end-to-end-training-pipeline&#34;&gt;End to End Training Pipeline&lt;/h3&gt;





  
  











&lt;figure id=&#34;figure-end-to-end-training-pipeline&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/cristian-sminchisescu/1_hu5ce619d08456037865eced6552c42e7e_329781_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;500&#34; height=&#34;500&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    End To End Training Pipeline
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;Once data is mapped to meshes and put into registered format, next step is to encode and decode static shapes (using VAE)&lt;/li&gt;
&lt;li&gt;Kinematics is learned using Normalizing Flow model&lt;/li&gt;
&lt;li&gt;Mesh filter (mask): to integrate close up scans with models, fed into the optimization step&lt;/li&gt;
&lt;li&gt;To train landmarks, we use annotated image data&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;p&gt;For the variational shape and expression autoencoder, VAE works better than OCA, with reconstruction error lying between 0-20mm. Motion Retargeting and Kinematic Priors are done by retargetting models to 2.8M CMU and 2.2M Humans3.6M motion capture frames.&lt;/p&gt;
&lt;h3 id=&#34;normalizing-flows-for-kinematic-priors&#34;&gt;Normalizing Flows for Kinematic Priors&lt;/h3&gt;





  
  











&lt;figure id=&#34;figure-normalizing-flows-for-kinematic-priors&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/cristian-sminchisescu/2_huf2e22827a723675b2a79947a570691ce_140050_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;869&#34; height=&#34;244&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Normalizing Flows for Kinematic Priors
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;A normalizing flow is a sequence of invertible transformations applied to an original distribution&lt;/li&gt;
&lt;li&gt;Use a dataset $\mathcal{D}$ of human kinematic poses $\theta$ as statistics for natural human movements&lt;/li&gt;
&lt;li&gt;Use normalizing flow to warp the distribution of poses into a simple and tractable density function e.g. $\mathbf{z} \sim \mathcal{N}(0 ; \mathbf{I})$&lt;/li&gt;
&lt;li&gt;The flow is bijective, trained by maximizing data log-likelihood
$$\max _{\phi} \sum _{\partial \in \mathcal{D}} \log p _{\phi}(\theta)$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ghum-and-smpl&#34;&gt;GHUM and SMPL&lt;/h3&gt;





  
  











&lt;figure id=&#34;figure-ghum-vs-smpl&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/cristian-sminchisescu/3_hu125e7c529aa96451d645ff796805a88a_150708_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;500&#34; height=&#34;500&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    GHUM vs SMPL
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;GHUM is close (slightly better) to SMPL in skinning visual quality&lt;/li&gt;
&lt;li&gt;The vertex point-to-plane error (body-only) is GHUM: 4.23mm and SMPL: 4.96mm&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;An effective Deep Learning Pipeline to build generative, articulated 3D human shape models&lt;/li&gt;
&lt;li&gt;GHUM and GHUM are two full body human models that are available for research:(&lt;a href=&#34;https://github.com/google-research/google-research/tree/master/ghum)&#34;&gt;https://github.com/google-research/google-research/tree/master/ghum)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;We can jointly sample shape, facial expressions (VAEs) and pose (normalizing flows)&lt;/li&gt;
&lt;li&gt;We have low res and high res models, that are non-linear (linear as special case)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;other-work&#34;&gt;Other Work&lt;/h3&gt;
&lt;p&gt;Some other interesting papers that Cristian pointed out were &lt;a href=&#34;https://arxiv.org/abs/2003.10350&#34;&gt; Weakly Supervised 3D Human Pose and Shape Reconstruction with Normalizing Flows&lt;/a&gt; (ECCV 2020) that works on Full Body Reconstruction in Monocular Images, and &lt;a href =&#34;https://arxiv.org/abs/2008.06910&#34;&gt;Neural Descent for Visual 3D Human Pose and Shape &lt;/a&gt; (submitted to NeurIPS 2020) that talks about Self-Supervised 3D Human Shape and Pose Estimation.&lt;/p&gt;
&lt;h3 id=&#34;human-interactions&#34;&gt;Human Interactions&lt;/h3&gt;
&lt;p&gt;A problem that many 3D deep learning practitioners face is dealing with human interactions during estimation and reconstruction. Contacts are difficult to estimate correctly because of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Uncertainty in 3D monocular depth prediction&lt;/li&gt;
&lt;li&gt;Reduced evidence of contact due to occlusion&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cristian then talked about his paper &lt;a href=&#34;https://openaccess.thecvf.com/content_CVPR_2020/papers/Fieraru_Three-Dimensional_Reconstruction_of_Human_Interactions_CVPR_2020_paper.pdf&#34;&gt; Three-dimensional Reconstruction of Human Interactions &lt;/a&gt; and to move towards accurate reconstruction of interactions we need to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Detect contact&lt;/li&gt;
&lt;li&gt;Predict contact interaction signatures&lt;/li&gt;
&lt;li&gt;3D reconstruction under contact constraints





  
  











&lt;figure id=&#34;figure-modelling-interactions&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/cristian-sminchisescu/4_hud88f9b9f04f1033960e9f73ca89377df_196249_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;930&#34; height=&#34;315&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Modelling interactions
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;conclusion-interactions&#34;&gt;Conclusion (Interactions)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;New models and datasets for contact detection, contact surface signature prediction, and 3d reconstruction under contact constraints&lt;/li&gt;
&lt;li&gt;Annotation has an underlying contact ground truth but not always easy to precisely identify from a single image&lt;/li&gt;
&lt;li&gt;Humans are reasonably consistent at identifying contacts at 9 and 17 region granularity, and contact can be predicted with reasonable accuracy too&lt;/li&gt;
&lt;li&gt;Contact-constrained 3D human reconstruction produces considerably better and more meaningful estimates, compared to non-contact methods&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cristian then concluded his wonderful lecture that talked about the most recent advances in Computer Vision in the 3D Deep learning field. It was a very informative and engaging lecture.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Summer School Series: Lecture 2 by Neil Houlsby</title>
      <link>https://archana1998.github.io/post/neil-houlsby/</link>
      <pubDate>Fri, 21 Aug 2020 23:11:51 +0530</pubDate>
      <guid>https://archana1998.github.io/post/neil-houlsby/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://research.google/people/NeilHoulsby/&#34;&gt;Neil Houlsby&lt;/a&gt; presented a great talk on Large Scale Visual Representation Learning and how Google has come up with solutions to some classical problems.&lt;/p&gt;
&lt;h3 id=&#34;evaluation-of-parameters&#34;&gt;Evaluation of parameters&lt;/h3&gt;
&lt;p&gt;There are two main ways of evaluating parameters from a network, that extracts the parameters. They are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear Evaluation: We freeze the weights and retrain the head&lt;/li&gt;
&lt;li&gt;Transfer Evaluation: We retrain end to end with new head&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;visual-task-adaptation-benchmark-vtab&#34;&gt;Visual Task Adaptation Benchmark (VTAB)&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://ai.googleblog.com/2019/11/the-visual-task-adaptation-benchmark.html&#34;&gt;VTAB&lt;/a&gt; is an evaluation protocal designed to measure progress towards general and useful visual representations, and consists of a suite of evaluation vision tasks that a learning algorithm must solve. We mainly have three types of tasks, &lt;b&gt; Natural tasks, Specialized tasks and Structured Datasets. &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;A query that was posed was how useful ImageNet labels would be for pretrained models to work on these three tasks. It has been seen that ImageNet labels work well for Natural images, and not well for the other two tasks.&lt;/p&gt;
&lt;p&gt;Representation learners pre-trained on ImageNet can be of three forms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GANs and autoencoders&lt;/li&gt;
&lt;li&gt;Self-supervised&lt;/li&gt;
&lt;li&gt;Semi-supervised / Supervised approach&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It has been seen that for natural tasks, representations prove to be more important than obtaining more data, and the supervised approach is far better than the unsupervised approach. For structured tasks, a combination of supervised and self-supervised learning works the best.&lt;/p&gt;
&lt;p&gt;It was also mentioned that by modern standards, ImageNet is of incredibly small-scale, thus scaling models on ImageNet were not proven to be effective.&lt;/p&gt;
&lt;p&gt;Something to specifically keep in mind is that upstream can be expensive, but downstream should be cheap (in terms of both data and compute). For the upstream, examples of suitable large datasets are ImageNet-21k for supervised learning, and YouTube-8M for self-supervised learning.&lt;/p&gt;
&lt;h3 id=&#34;bit-l&#34;&gt;BiT-L&lt;/h3&gt;
&lt;p&gt;Neil introduced the &lt;a href=&#34;https://blog.tensorflow.org/2020/05/bigtransfer-bit-state-of-art-transfer-learning-computer-vision.html&#34;&gt;Big Transfer Learning (BiT-L)&lt;/a&gt; algorithm and talked about it in detail.
The first thing he mentioned about BiT-L was that batch normalization was replaced with &lt;b&gt; group normalization &lt;/b&gt; for ultra-large data. Advantages of this were having no train/test discrepancy, and no state which made it easier to co-train with multiple steps.&lt;/p&gt;
&lt;p&gt;It was highlighted that optimization at scale implies that schedule is crucial and not obvious. Also, early results of models can be misleading.&lt;/p&gt;
&lt;p&gt;To perform cheap tranfer, we need low compute, few/no validation data and diverse tasks. For doing few-shot transfer, pretraining on ImageNet-21k and JFT-300M helps.&lt;/p&gt;
&lt;h4 id=&#34;robustness&#34;&gt;Robustness&lt;/h4&gt;
&lt;p&gt;Models trained with ImageNet aren&amp;rsquo;t necessarily robust most of the times. To test OOD robustness (Out-Of-Distribution), we use datasets like ImageNet C, ImageNet R and ObjectNet.&lt;/p&gt;
&lt;h4 id=&#34;modern-transfer-learning&#34;&gt;Modern Transfer Learning&lt;/h4&gt;
&lt;p&gt;Modern Transfer Learning calls for a big, labelled datset, a big model and careful training (using about 10 optimization recipes)
While testing with OOD, increasing datset size with a fixed model and increasing dataset size leads to an increase in performance, especially in the case of very large models.&lt;/p&gt;
&lt;p&gt;To summarize, Bigger transfer $\rightarrow$ Better Accuracy $\rightarrow$ Better Robustness&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For checking impact on object &lt;b&gt; location &lt;/b&gt; invariance, we see accuracy improves and becomes more uniform across location&lt;/li&gt;
&lt;li&gt;This proves to be the same in the case of impact on object &lt;b&gt;size&lt;/b&gt; invariance&lt;/li&gt;
&lt;li&gt;However, in the case of object rotation invariance for ResNet50, it does not become more uniform across rotation angles, but for ResNet101*3, it maintains uniformity&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Main takeaways from the talk and BiT-L were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scale is one of the key drivers of representation learning performance&lt;/li&gt;
&lt;li&gt;Especially effective for few-shot learning and OOD Robustness&lt;/li&gt;
&lt;li&gt;Also seen and mirrored in language domain&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Links to the GitHub repositories are: &lt;a href =&#34;https://github.com/google-research/big_transfer&#34;&gt; Big Transfer &lt;/a&gt; and &lt;a href=&#34;https://github.com/google-research/task_adaptation&#34;&gt; Visual Task Adaptation Benchmark (VTAB)&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Flipkart Grid 2.0 Hackathon</title>
      <link>https://archana1998.github.io/post/flipkart-grid/</link>
      <pubDate>Mon, 17 Aug 2020 17:39:32 +0530</pubDate>
      <guid>https://archana1998.github.io/post/flipkart-grid/</guid>
      <description>&lt;p&gt;Flipkart recently concluded their 2 month long annual hackathon for students of Indian engineering colleges. This yearâs edition saw over 20,000 participants and boasted of a prize pool of around Rs. 300,000 (-4000 USD). Our team (Gradient Ascent) made it to the 3rd round of the competition and I am writing about our experience in this article.&lt;/p&gt;
&lt;h2 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h2&gt;
&lt;p&gt;A fashion retailer wants to source ongoing and upcoming fashion trends from major online fashion portals and online magazines in a consumable and actionable format, so that they are able to effectively and efficiently design an upcoming fashion product portfolio.&lt;/p&gt;
&lt;p&gt;Deliverables:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Identify products that are better performers (in a rank ordered fashion)&lt;/li&gt;
&lt;li&gt;Help the user view the products that are both trending and lagging&lt;/li&gt;
&lt;li&gt;Identify a logic for classifying products as per their trendiness&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We were asked to complete the challenge for just the t-shirt product vertical, but to ensure that our solution would be scalable to other products as well.&lt;/p&gt;
&lt;h2 id=&#34;initial-analysis&#34;&gt;Initial Analysis&lt;/h2&gt;
&lt;p&gt;We started off by performing a literature review on current research in the field of fashion with respect to deep learning. We looked at previous attempts of learning attributes from fashion images, modelling trends as timeseries data, fashion image encodings, object detection, etc.&lt;/p&gt;
&lt;p&gt;After spending some time on our research, we split the problem into the following
subproblems to tackle independently:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Data Collection&lt;/li&gt;
&lt;li&gt;Object Detection&lt;/li&gt;
&lt;li&gt;Attribute/Feature learning&lt;/li&gt;
&lt;li&gt;Ranking&lt;/li&gt;
&lt;li&gt;Grouping (trending/lagging)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;data-collection&#34;&gt;Data Collection&lt;/h2&gt;
&lt;p&gt;According to the problem statement, we had to extract data from e-Commerce sites and other fashion portals and magazines. We tried our best to include data from all those categories to ensure we had a balanced dataset for our classification and ranking later on. After scouring the web for some good resources, we finally settled on the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Flipkart&lt;/li&gt;
&lt;li&gt;Amazon&lt;/li&gt;
&lt;li&gt;Pinterest (curated collections of fashion trends)&lt;/li&gt;
&lt;li&gt;Vogue India&lt;/li&gt;
&lt;li&gt;Myntra&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We felt this combination of multipurpose e-Commerce sites, well established fashion magazines, social network sites and dedicated fashion shopping sites would ensure we had good representation from all sectors. We collected an average of around 600 images from each website, giving us a total of 3000 to work with.
Web scraping was done in Python using the Selenium framework. The scripts used to scrape data from any website were pretty similar and any new sites could be added with minor modifications, hence this step was easily scalable.
From e-commerce sites, we scraped the images, product names, ratings and the number of reviews to with ranking later on. From the other portals, we extracted just the images.&lt;/p&gt;
&lt;h2 id=&#34;object-detection&#34;&gt;Object Detection&lt;/h2&gt;
&lt;p&gt;One of the biggest problems we faced when extracting images from fashion magazines and social media sites is that they donât limit themselves to just t-shirts. When they put out a catalogue/collection, it has everything ranging from skirts to sweaters to scarves. Furthermore, even in pictures where the shirt was the highlight, other features such as the modelâs pose, skin colour and distance from the camera could confuse our model in the later stages of this project. Keeping all this in mind, we decided to use an object detection model to filter our data to ensure we had only pictures of t-shirts. Additionally, we cropped the images according to their bounding boxes to counter the other aforementioned problems.
This was done using a pretrained YOLOv3 model trained on the DeepFashion2 dataset, implemented using PyTorch.&lt;/p&gt;
&lt;h2 id=&#34;attributefeature-learning&#34;&gt;Attribute/Feature learning&lt;/h2&gt;
&lt;p&gt;This is where we faced our major setback. Our initial plan was to train a model to learn the attributes (neck type, sleeve length, patterns, etc) and to return them back for later use. We were then going to perform FP growth on our set of attributes of each image to obtain the frequent itemsets which would correspond to the most common combination of features and hence, trending/popular styles.&lt;/p&gt;
&lt;p&gt;It didn&amp;rsquo;t work out however, as we couldnât find an appropriate dataset to work with such a task given our time constraints so we had to try out our backup plan.&lt;/p&gt;
&lt;p&gt;Our plan involved getting numeric encodings for the fashion images in place of the attribute list and performing clustering on the encodings. The largest clusters would correspond to the most popular types of clothes, and similarly, the smallest clusters would represent the lagging ones, assuming our calculated encodings are a fair representation of the original image. Since we were working with just images (unlabeled) data, we had to devise an unsupervised approach for learning the image encodings. After considering various options, we decided to go ahead using an autoencoder based on a CNN architecture. We did this for 2 major reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Convolutional layers would help notice particular features of t-shirts such as the necktype length and patterns if any&lt;/li&gt;
&lt;li&gt;We can insight on how accurate our encodings to reconstruct the image&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Hereâs a summary of the model we used:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-frequent-itemset-mining&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/flipkart-grid/1_hud8ee6b60bc41ae2f19ca216ed0cbf7e3_180328_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1265&#34; height=&#34;177&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Frequent Itemset Mining
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;We then plotted some of the reconstructed images side by side with their original counterparts and got pretty good results considering the simplicity of the network and size of the dataset. The encodings were able to capture some important features of the clothes in question.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-model-architecture&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/flipkart-grid/2_huf8d20e2591676411ac2452c151a71b7c_32522_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;352&#34; height=&#34;613&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Model Architecture
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;ranking&#34;&gt;Ranking&lt;/h2&gt;
&lt;p&gt;As far as e-commerce sites go, there are 2 main criteria used to determine how âgoodâ a product is â the number of reviews and the rating it has. What would you consider to be better? 10 reviews with a 5-star rating? Or 50 reviews with a 4.7-star rating? This was the major question we had to answer to be able to rank these products properly. We needed an effective way of combining these 2 into one reliable metric. After doing some research on this area and tying out different methods of combing them, we settled with an approach based on  a Bayesian view of the beta distribution, described beautifully in this video by &lt;a href =&#34; https://www.youtube.com/watch?v=8idr1WZ1A7Q&amp;feature=emb_logo&#34;&gt;3blue1brown&lt;/a&gt;
We used this principle to come up with our own âPopularity Metricâ which was calculated as follows:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-reconstructed-images-from-encodings&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/flipkart-grid/3_hua9d9b1a4f907d92d1804ac04cfd1acaa_208064_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1093&#34; height=&#34;234&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Reconstructed Images from encodings
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;We now had a mechanism to compare and rank products effectively and a way to calculate accurate image encodings. We used both of these to train a model which predicts the Popularity Metric of a given clothing item given an input as the image encoding. We envisioned such a model to be extremely useful for designers that are looking for insight as to how their clothes might fair if they were put up for sale on e-commerce websites. Furthermore, the Popularity Metric could be calculated for all the images from magazines and portals like Vogue and Pinterest, so those products can be ranked and compared too!
The architecture, simplified pipeline and a screenshot of the program in action are shared below.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-popularity-metric&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/flipkart-grid/4_hu67e707b293e8bb2e83945f78f4b29e05_7331_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;238&#34; height=&#34;51&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Popularity Metric
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;center&gt; (n = number of reviews, s = star rating ) &lt;/center&gt;





  
  











&lt;figure id=&#34;figure-popularity-metric-model&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/flipkart-grid/5_hu1fc31ec500f87e3d3c214e201fb0429c_18291_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;540&#34; height=&#34;306&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Popularity Metric Model
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;grouping&#34;&gt;Grouping&lt;/h2&gt;
&lt;p&gt;Since the FP growth idea fell through the roof, we went with clustering as our method of choice for grouping products in such a way that we can obtain the trending and lagging items. To ensure our clustering was done well, we experimented on a variety of clustering algorithms and chose the one with the highest silhouette coefficient. The algorithms tested were â&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;K means&lt;/li&gt;
&lt;li&gt;Gaussian mixture model&lt;/li&gt;
&lt;li&gt;DBSCAN&lt;/li&gt;
&lt;li&gt;Mini batch k means&lt;/li&gt;
&lt;li&gt;Spectral clustering&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Among those, K means had the highest silhouette efficient so we went ahead with that. We then split the data into clusters according to how many images were being considered for clustering (no. of clusters = no. of images/10). The products in the largest cluster could be inferred as the trending/popular products and those in the smallest clusters would be lagging products. We gave the user the option to spec which sources they wanted to consider for their clustering, giving them more flexibility with regards to analyzing whatâs not and whatâs not (whatâs trending on Vogue might not be popular on Amazon).&lt;/p&gt;
&lt;p&gt;To conclude, we were able to come up with a way to rank products properly and to group them based on whether they are trending or lagging. We also ensured that our solution is scalable on 2 fronts:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Getting more data can be done easily with minor modifications to the existing script&lt;/li&gt;
&lt;li&gt;We can expand to different product verticals by changing the object of interest in the object detection model&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The link to the GitHub Repo is at the top of this page.
Hope you found this interesting, thanks for reading!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Structural Damage Detection using ConvNets</title>
      <link>https://archana1998.github.io/project/damage-detection/</link>
      <pubDate>Sun, 16 Aug 2020 21:12:27 +0530</pubDate>
      <guid>https://archana1998.github.io/project/damage-detection/</guid>
      <description>&lt;p&gt;Presented our work at the &lt;a href = &#34;https://cmos.in1touch.org/site/congress_home&#34;&gt;  54th Canadian CMOS conference, 2020. &lt;/a&gt; Worked under the supervision of &lt;a href =&#34;https://scholar.google.co.in/citations?user=42ZAdYUAAAAJ&amp;hl=en&#34;&gt;Dr. S Radhika&lt;/a&gt;, of the Department of Electrical and Electronics Engineering, BITS Pilani. Manuscript accepted and to be published.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
