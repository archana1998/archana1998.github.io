[{"authors":["admin"],"categories":null,"content":"Hi! I am Archana Swaminathan, a final year undergraduate student at BITS Pilani, Hyderabad, India. I am currently working as a Visiting Researcher with the V-SENSE Research Group, Trinity College Dublin , and being mentored by Prof. Aljosa Smolic.  I\u0026rsquo;m part of a group that is investigating 3D Geometry for Deep Learning, where we focus on applications that extend to Image-based reconstruction, pose estimation and visual computing.\nIn the past, I\u0026rsquo;ve had experience with working with Deep Learning in many projects, some of which are: predictive modelling for time series data, image encryption and decryption, image superresolution, semantic segmentation and object recognition.\nI\u0026rsquo;m very interested in research in the areas of Image Processing and Computer Vision, and wish to explore different applications in these fields as I motivate and prepare myself for a career as a researcher.\nI\u0026rsquo;m actively looking for Research Assistantship / PhD opportunities starting fall 2021. Do get in touch with me if you\u0026rsquo;re up for a chat!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://archana1998.github.io/author/archana-swaminathan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/archana-swaminathan/","section":"authors","summary":"Hi! I am Archana Swaminathan, a final year undergraduate student at BITS Pilani, Hyderabad, India. I am currently working as a Visiting Researcher with the V-SENSE Research Group, Trinity College Dublin , and being mentored by Prof.","tags":null,"title":"Archana Swaminathan","type":"authors"},{"authors":[],"categories":[],"content":"","date":1595086145,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595086145,"objectID":"5ccc274eb88bf90f01c7493dcbb6d6e2","permalink":"https://archana1998.github.io/project/structural-damage-detection/","publishdate":"2020-07-18T20:59:05+05:30","relpermalink":"/project/structural-damage-detection/","section":"project","summary":"","tags":[],"title":"Structural Damage Detection","type":"project"},{"authors":["Archana Swaminathan"],"categories":["Undergraduate Projects"],"content":"Introduction The Ramanujan Sums were first proposed by Srinivasan Ramanujan in 1918, and have become exceedingly popular in the fields of signal processing,time-frequency analysis and shape recognition. The sums are by nature, orthogonal. This results in them offering excellent conservation of energy, which is a property shared by Fourier Transform as well.\nWe have used Matrix Multiplication to obtain the Ramanujan Basis, for our computation. The Ramanujan Sums are defined as nth powers of qthprimitive roots of unity, which can be computed using this simple formula:\n\\[c_{q}(n)=\\mu\\left(\\frac{q}{g c d(q, n)}\\right) \\frac{\\varphi(q)}{\\varphi\\left(\\frac{q}{g c d(q, n)}\\right)}\\]\nWhere $q=\\prod_{i} q_{i}^{a_{i}}$,(qi is prime). Then, $\\varphi(q)=q \\prod_{i}\\left(1-\\frac{1}{q i}\\right)$.\n$\\mu(n)$ is the Mobius function, which is equal to 0 if n contains a square number, 1 if n = 1 and (-1)*k if n is a product of k distinct prime numbers.\nThe Ramanujan matrix can be defined as:\n\\[A(q, j)=\\frac{1}{\\varphi(q) M} c_{q}(\\bmod (j-1, q)+1)\\]\nThe 2-D forward Ramanujan Sum Transform is given as:\n\\[Y(p, q)=\\frac{1}{\\varphi(p) \\varphi(q)} \\frac{1}{M N} \\sum_{m=1}^{M} \\sum_{n=1}^{N} x(m, n) C_{p}(m) C_{q}(n)\\]\nwhich in matrix terms can be defined as\n\\[Y=A * A^{\\top}\\]\nand the inverse 2D Ramanujan transform in matrix terms is:\n\\[X=A^{-1} Y\\left(A^{-1}\\right)^{\\top}\\]\nOriginal, Transformed and Inversed Image   Compressive Sensing The principle behind the use of compressive sensing as a signal processing technique, is that most test signals are not actually completely comprised of noise, but most have a great degree of redundancy in them. Sparse representation of signals in a particular domain signifies that most of the signal coefficients are either zero or close to zero.\nCompressive measurements, which are a weighed linear combination of signal samples, are first taken in a basis that is different from the sparse basis.\nAlgorithm We use the generated Ramanujan Basis to do the sparse reconstruction. First, the sparse signal is obtained by multiplyingthe Ramanujan Basis A with the flattened image vector (here we are using the Cameraman Image that has been resized to 5050). Our Ramanujan Basis has dimensions of 25002500.\nThus,\n\\[Z=A^{*} x\\]\nWhere Z is the sparse representation of the cameraman image, and x is the flattened image vector of the original image. We next create a random measurement matrix of dimension m*n, where we keep m = 5000 and n = 2500. This measurement matrix (Phi) is then multiplied by the sparse signal z.\n\\[Y=P h i * Z\\]\nWe then use Orthogonal Matching Pursuit Algorithm, which aims to approximately find the most accurate projections of data in multiple dimensions on to the span of a redundant or overcomplete dictionary. Here, the overcomplete dictionary we use is the Ramanujan Basis A. The orthogonal matching pursuit algorithm is then applied onto the signal Y, and we have considered 1700 iterations.\nThe plot of the original sparse representation and the OMP representation is given below:\nPlot of original sparse representation (blue) and OMP representation (red)   The image is then reconstructed by taking the inverse of the Ramanujan Basis, and multiplying it with the OMP sparse representation.\n\\[R e c =A^{-1} * x w s r\\]\nWhere xwsr is the OMP representation, and Rec is the reconstructed image signal. This is then resized to obtain the final image, which has been compared with the original image below:\nOriginal and Final image   Results We compare and evaluate the performance of the Compressive Sensing Algorithm by using PSNR, SSIM and MSE image evaluation metrics.\n PSNR: Peak Signal to Noise Ratio SSIM: Structural Similarity Index MSE: Mean Square Error  Ideally, high values of PSNR, SSIM(max=1) and MSE show favourable performance of the reconstruction algorithm.The results obtained for this approach are:\n PSNR = 23.1362 SSIM = 0.6265 MSE = 315.8316  ","date":1595077847,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595077847,"objectID":"72d8642b4f9a05c1ccc6178576ab9893","permalink":"https://archana1998.github.io/project/compressive-sensing/","publishdate":"2020-07-18T18:40:47+05:30","relpermalink":"/project/compressive-sensing/","section":"project","summary":"Used the Ramanujan Fourier Transform to do compressive sensing and denoising of images in the Ramanujan domain,using the Ramanujan basis as the overcomplete dictionary and trained the dictionary with K-SVD based on OMP algorithm","tags":["Image Processing"],"title":"Compressive Sensing and Denoising of Images using the Ramanujan Fourier Transform","type":"project"},{"authors":[],"categories":[],"content":"This is a review of the ICLR 2017 paper by Zhang et. al. titled \u0026ldquo;Understanding Deep Learning requires rethinking generalization\u0026quot;Link to paper \nThe paper starts off with aiming to provide an introspection into what distinguishes networks that generalize well, from those who donâ€™t.\nOne of the experiments conducted in the studies of the paper, is checking how well neural networks adapt to training when labels are randomized. Their findings establish that when the true data is completely randomly labelled, the training error that results is zero. Observations from this indicate that the effective capacity of neural networks is more than enough to memorize the entire dataset. Randomization of the labels is only a transformation of the data, and other learning parameters are constant and unchanged still. The resulting training time also increases by only a small factor. However, when this trained network is tested, it does badly. This indicates that just by randomizing labels, the generalization error can shoot up significantly without changing any other parameters of the experiment like the size of the model, the optimizer etc.\nAnother experiment conducted was that when the ground truth images were switched with random noise. This resulted in the networks training to zero training error, even faster than the case with the random labels. Varying the amount of randomization resulted in a steady deterioration of the generalization error, as the noise level increased. There were a wide variety of changes introduced into the dataset, that played with degrees and kinds of randomization with the pixels and labels. All of this still resulted in the networks able to fit the training data perfectly. A key takeaway from this is that the neural networks are able to capture the signals remaining in the data, while fitting the noise and randomization with brute force. The question that still remains unanswered after this is why some models generalize better than others, because it is evident that some decisions made while constructing model architectures do make a difference in its ability to generalize.\nTraditional approaches in statistical learning theory such as Rademacher complexity, VC dimension and uniform stability are threatened by the randomization experiments performed.\nThree specific regularizers are then considered to note the impact of explicit regularization, data augmentation, weight decay and dropout. These are tried out on Inception, Alexnet and MLPs on the CIFAR10 dataset, and later with ImageNet. Regularization helps to improve generalization performance, but the models still generalize well enough with the regularizers turned off. It was then inferred that this is more of a tuning parameter than a fundamental cause of good generalization. A similar result was noted with implicit regularization.\nAn interesting result proved in the paper was that there two layer depth networks of linear size, that can represent any labelling of the training data. A parallel approach in trying to understand the source of regularization for linear models was also not easy to point out. To sum up, this paper presents a thorough insight into how empirically easy optimization does not imply good regularization, and effective capacity of network architectures is better understood and defined.\n","date":1595061682,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595061682,"objectID":"515fa5f87c2da06e89e06122d9b53a0c","permalink":"https://archana1998.github.io/post/project-1/","publishdate":"2020-07-18T14:11:22+05:30","relpermalink":"/post/project-1/","section":"post","summary":"A short and concise review","tags":[],"title":"Understanding Deep Learning requires rethinking generalization","type":"post"}]