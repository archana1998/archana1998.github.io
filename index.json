[{"authors":["admin"],"categories":null,"content":"Hi! I am Archana Swaminathan, a final year undergraduate student at BITS Pilani, Hyderabad, India. I am currently working as a Visiting Researcher with the V-SENSE Research Group, Trinity College Dublin , and being mentored by Prof. Aljosa Smolic. I\u0026rsquo;m part of a group that is investigating 3D Geometry for Deep Learning, where we focus on applications that extend to Image-based reconstruction, pose estimation and visual computing.\nIn the past, I\u0026rsquo;ve had experience with working with Deep Learning in many projects, some of which are: predictive modelling for time series data, image encryption and decryption, compressive image sensing and denoising, semantic segmentation and object recognition.\nI\u0026rsquo;m very interested in research in the areas of Image Processing and Computer Vision, and wish to explore different applications in these fields as I motivate and prepare myself for a career as a researcher.\nIn my free time, I like to read books, play badminton and work out. I love travelling and eating different desserts that mostly comprise of chocolate. I can\u0026rsquo;t wait to go exploring once this pandemic is done!\nI\u0026rsquo;m actively looking for Research Assistantship / Visiting Researcher Opportunities starting fall 2021. I am currently based in Bangalore, India but I\u0026rsquo;m open to relocate. Do get in touch with me if you\u0026rsquo;re up for a chat!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://archana1998.github.io/author/archana-swaminathan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/archana-swaminathan/","section":"authors","summary":"Hi! I am Archana Swaminathan, a final year undergraduate student at BITS Pilani, Hyderabad, India. I am currently working as a Visiting Researcher with the V-SENSE Research Group, Trinity College Dublin , and being mentored by Prof.","tags":null,"title":"Archana Swaminathan","type":"authors"},{"authors":["Archana Swaminathan"],"categories":["Google AI Summer School","Experiences"],"content":"I recently got the opportunity to attend the AI Summer School conducted by Google Research India. I was one of the 150 people selected to attend it, out of over 75,000 applications. Probably one of my most noteworthy achievements till date, if not the most (?). I remember screaming after getting the acceptance mail for over two hours, it was the happiest I have been in a while. I was selected as part of the Computer Vision track, and I was elated as I knew absolutely nothing about the other two tracks (Natural Language Understanding and AI for Social Good)\nThe summer school happened over three days, between August 20 and 22, 2020. Due to the pandemic that taught us that we can do everything over a computer screen, the summer school was held in a virtual mode. The people at Google made us feel very welcome, and sent out a batch of goodies from Google to all the participants (side note: I\u0026rsquo;m a little salty about this as I haven\u0026rsquo;t gotten mine yet, it got lost on the way). Saying I loved the experience would be an understatement of sorts, I was constantly elated after each and every event.\nDay 1 started off with a keynote by Jeff Dean, the head of Google AI Research, at 9 am. Waking up so early was a huge achievement for me in a quarantine-home restricted environment where I sleep late and wake up late. Working remotely at a lab in a different country provides insane flexibility, I am my most productive in the afternoons and evenings. I sat in front of my computer and tuned into the YouTube live stream which was engaging and amazing (see my post) Opening Keynote   After a lunch break, we had our first lecture by Jean-Phillipe Vert, which had so much rigorous math that we were slightly intimidated, however it was a pleasure being taught by someone so amazing all the same. (shameless plug to post again).\nWe had an amazing panel discussion that was titled Why Choose a Career in Research. The panel consisted of eminent names from Google Research. We had a \u0026ldquo;virtual social\u0026rdquo; after that on GatherTown, which was not the easiest to use on Day 1, but it was quite an experience. We had a second lecture after that by Neil Houlsby, finally on computer vision (I loved it, here\u0026rsquo;s my post). And just like that, I was done for the day and had learnt more in these 6 hours than I did in the last semester.\nDay 2 started off well with a lovely talk by Vineet Gupta, on math again :(. But this was nice math, easy to understand and follow and talked about very interesting theoretical math for machine learning that provided very promising results in optimization (once again, here\u0026rsquo;s my post). We had a social before lunch once again, and I got to meet and greet with a lot of people this time, having finally understood how to use the GatherTown UI. I interacted with a lot of my fellow attendees and the Google Lab members, it was super fun.\nAfter lunch, we had our first computer vision-centric lecture by Cristian Sminchisescu that was BEAUTIFUL. The fact that it perfectly aligned to my research interests was a cherry on top of the cake. (post again). We had a panel discussion titled \u0026ldquo;AI For India\u0026rdquo; after that, which was insightful as well. I was done with my second day of the school, and had learned more than I did in half of my math degree.\nDay 3 had lovely lectures, by Rahul Sukthankar and Arsha Nagrani who were so, so, good at presenting their work! Rahul\u0026rsquo;s lecture was simple but beautifully presented, and I loved it! (here\u0026rsquo;s my post). Arsha\u0026rsquo;s talk was about some very interesting research that\u0026rsquo;s probably going to revolutionize multimodal learning (last time, here\u0026rsquo;s my post ) The summer school concluded with a closing keynote delivered by Manish Gupta, the director of Google Research India, who talked about opportunities in Google Research for us. We then had socials that lasted two hours (last day, woohoo) and I, who had mastered navigating GatherTown by then was a proper social butterfly, talking to everyone and anyone and sending connection requests on LinkedIn to stay in touch.\nThat was it! curtain closes The experience was CRAZY, and I never knew I could learn so much in just three days. More than learning new concepts, I got an insight into how these amazing people conduct cutting edge research, and the fact that we have to learn so much to get there was a little inspirational too. I was jumping with happiness and rambled nonstop about how much fun I had to my family and my friends, thankfully for me, they shared my enthusiasm :) I can\u0026rsquo;t wait to experience more things like this in the future!\nPS: Still waiting for my goodies @Google. Thanks again for a lovely time.\n","date":1598363375,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598363375,"objectID":"b7ea38adbacd60645dd621e7f3f67d77","permalink":"https://archana1998.github.io/post/summer-school-sumup/","publishdate":"2020-08-25T19:19:35+05:30","relpermalink":"/post/summer-school-sumup/","section":"post","summary":"A blog post about my experience at the very first edition of the Google AI Summer School","tags":[],"title":"How Google bowled me over with a Googly","type":"post"},{"authors":["Archana Swaminathan"],"categories":["Google AI Summer School","Experiences"],"content":"This final lecture was delivered by Arsha Nagrani, a recent Ph.D. graduate from Oxford University\u0026rsquo;s VGG group, and an incoming research scientist at Google Research. Her talk was called Multimodality for Video Understanding.\nVideo Understanding Videos provide us with far more information than images. Multimodal refers to many mediums for learning, here it can be time, sound and speech. Videos are all around us (30k newly created content videos are uploaded to YouTube every hour). However, these have high dimensionality and are difficult to process and annotate.\nComplementarity among signals  Vision (scene) Sound (content of speech)  Redundancy between signals Helps recognize person, face+sound, thus can be a useful form of weak supervision. The redundant information comes from background sounds, foreground audio, signals identified from speech and the content of speech.\nThus, best way to exploit multimodal nature of videos is to work with the complementarity and redundancy.\nSuitable tasks Suitable tasks for video understanding are:\n Video classification   single label infinite number of possible classes ambiguity in the label space  Action recognition: more fine grained, the motion is important, human centric  It is important to note that labelling actions in videos is extremely expensive and existing models do not generalize well to new domains.\nIn this context, can we use speech as a form of supervision? For example, narrated video clips and lifestyle Vlogs.\nMovies General domain of movies: people speak about their actions. However, sometimes speech is completely unrelated, giving us noise. We need to learn when speech matches action. An example of work in this field is End-to-End Learning of Visual Representations from Uncurated Instructional Videos. This work reduces noise by using the MIL-NCE loss.\nCan we first train a model to recognize actions and then see if it should be used for supervision? An interesting discovery Arsha made was using Movie Screenplays, that contain both speech segments and scene directions with actions. Using this:\n We can obtain speech-action pairs Retrieve speech segments with verbs Train the Speech2Action model to predict action, with a BERT-Backbone (movie scripts scraped from IMSDB) Apply to closed captions of unlabelled videos Apply to large movie corpus  Speech2Action model   The Speech2Action model recognizes rare actions, and is a visual classifier on weakly labelled data (S3D-G model with cross-entropy loss)\nEvaluation is done on the AVA and HMDB-51 (transfer learning) datasets. It gets abstract actions like count and follow too.\nMultimodal Complementarity This refers to fusing info from multiple modalities for video text retrieval, like:\n Finding video corresponding to text queries More to videos than just actions like object, scene etc.  Supervisions:\n It\u0026rsquo;s not easy to get the complete combination of captions, this is a very subjective task Need extremely large datasets  What Arsha does is rely on expert models trained for different tasks like object detection, face detection, action recognition, OCR etc. These are all applied to the video and features are extracted. The framework is a joint video text embedding, with the video encoder + text query encoder = joint embedding space (similarity should be really high if related). It is necessary for the video encoder to be discriminative and retain specific information.\nCollaborative Gating For each expert, generate attention mask by looking at the other experts (Use What You Have: Video Retrieval Using Representations From Collaborative Experts, BMVC 2019)\n Trained using bi-directional max margin ranking loss Adding in more experts massively increases performance Main boost is from the object embeddings  Collaborative Gating   Another paper that Arsha discussed was Multi-modal Transformer for Video Retrieval, ECCV 2020 . This takes features that are taken at different time stamps for each task and aggregrate for the embeddings. The expert and temporal embeddings are added and summed up.\nConclusion  More modalities is better (because more complementarity) Time (modelling time along with modalities is interesting, some modalities train faster than the others) Mid fusion is better than late (Attention truly is what you need) Our world is multimodal, it doesn\u0026rsquo;t make sense to work with modalities in isolation Use the redundant and complementary information from vision, audio and speech to massively reduce annotations  Open Research Questions:\n Extended Temporal Sequences (beyond 10s):   Backprop + memory restricts current video architectures to 64 frames For longer we rely on pre-extracted features Need new datasets to drive innovation  Moving away from supervision: is an upper bound on self supervision being appraoched? The world is multimodal: how do we design good fusion architectures?  Arsha thus concluded a fantastic talk that described the cutting-edge research that her team at Oxford and Google is conducting. It was tremendously insightful and inspirational.\n","date":1598360760,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598360760,"objectID":"3f03c1e896c192fc04ea01889e11da7c","permalink":"https://archana1998.github.io/post/arsha-nagrani/","publishdate":"2020-08-25T18:36:00+05:30","relpermalink":"/post/arsha-nagrani/","section":"post","summary":"This lecture was delivered by Arsha Nagrani, a recent Ph.D. graduate from Oxford University's VGG group, and an incoming research scientist at Google Research, titled Multimodality for Video Understanding","tags":["Computer Vision"],"title":"Summer School Series: Lecture 6 by Arsha Nagrani","type":"post"},{"authors":["Archana Swaminathan"],"categories":["Google AI Summer School","Experiences"],"content":"This Lecture was presented by Rahul Sukthankar, a research scientist at Google Research and an Adjunct Professor at Carnegie Mellon University. It was titled Deep Learning in Computer Vision.\nPopular Computer Vision tasks Some popular tasks in the domain of computer vision include:\n Image Classification (assign to one class) Image Labelling/Object Recognition (multiple classes) Object Detection/Localization (predicts bounding box+label, works well for objects but not for fuzzy concepts) Semantic Segmentation (Pixel level dense labelling) Image Captioning (Description of image in text) Human Body Part Segmentation Human Pose Estimation (predicting 2D pose keypoints) Generating 3D Human Pose and Body Models from an image Depth Prediction from a single image (foreground and background semantic segmentation based on a heatmap) 3D Scene Understanding Autonomous navigation  While thinking about a particular problem statement:\n We need to take in specific considerations (such as semantic segmentation, classicaiton, object detection etc) What is the output? (binary yes/no, bounding box, label/pixel etc) How is the training data labelled? (Fully Supervised/Weakly or Cross-Modal/Self-supervised) Architecture: Usually a Convolutional Neural Network, but what is the final layer? What loss function do we use?  CMU Navlabs (30 years ago) built a self steering car only with an artificial neural network, in the pre-CNN era (ALVINN: AN AUTONOMOUS LAND VEHICLE IN A NEURAL NETWORK)\nConvolutional Neural Networks The structure of a convolutional neural network follows as input + conv, relu, pooling layers (hidden layers) + flatten, fully connected and softmax layers (for classification). Key concepts behind CNNs are:\n Local connectivity (not connected to every pixel, but just a few) Shared weights (translational invariance) Pooling (reducing dimensions, leads to local patch becoming bigger (filter size)) Filter stride (cuts down weights, reduces computations) Multiple feature maps  It is essential to choose the right conv layer, pooling layer, activation function, loss function, optimization and regularization methods, etc.\nConvolutions  2D vs 3D convolutions: 3D convolutions are used to capture patterns across 3 dimensions, for example Video Understanding and Medical Imaging. 1x1 convolution: weighed average across channel axis, feature pooling technique to reduce dimensions Other types of convolutions are dilated convolutions, regular vs depth wise separable convolutions, grouped convolutions (AlexNet uses it, it reduces computation)  Famous architectures   Inceptionv1 (2014): Inception v1     ResNet:\n  Resnet uses skipped connections with residual blocks, the added paths help solve vanishing gradient problems and gives a shorter route for backpropagation\nResidual blocks   Object Detection in Images  Object Classification: Task of identifying a picture is a dog Object Localization: Involves finding class labels as well as a bounding box to show where an object is located Object Detection: Localizing with box Semantic Segmentation: Dense pixel labelling  There are two ways to do detection:\n Sliding window approach: computationally expensive and unbalanced Selective search: guessing promising bounding boxes and selecting the best out of them   RCNN did this when they extracted region proposals Fast RCNN did class labelling+ bounding box prediction at the same time (softmax+bounding box regression)  Bounding box evaluation is commonly done by the Intersection over Union Metric $$ \\text{Intersection over Union} = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}}$$ (Ground truth bounding box and predicted bounding box)\nClassic CNN vs Fully Convolutional Net A classic CNN comprises of a conv+Fully Connected Layer, a fully convolutional layer contains convolutional blocks that help us retain the same number of weights no matter what the input image size is. An example of a fully convolutional net is the U-Net, that is used extensively for semantic segmentation.\nOther applications of a fully convolutional net are : Residual Encoding Decoding, Dense Prediction, Superresolution, Colorization (self supervised)\nLast-Layer Activation function and Loss Function Summary Functions to use   Any differentiable function can be used as a loss function: even another neural net! (perceptual loss, GAN loss, differentiable renderer etc)\nRahul concluded this introduction lecture focused in Computer Vision using fully supervised deep learning, with key concepts on CNNs and their extensions and the importance of choosing the right loss function. It was a wonderful lecture with all the concepts beautifully explained.\n","date":1598356169,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598356169,"objectID":"82a8a63e1d929a7a21ff5215e5ef06a3","permalink":"https://archana1998.github.io/post/rahul-sukthankar/","publishdate":"2020-08-25T17:19:29+05:30","relpermalink":"/post/rahul-sukthankar/","section":"post","summary":"This Lecture was presented by Rahul Sukthankar, a research scientist at Google Research and Adjunct Prof. at CMU. It was titled Deep Learning in Computer Vision","tags":["Computer Vision"],"title":"Summer School Series: Lecture 5 by Rahul Sukthankar","type":"post"},{"authors":["Archana Swaminathan"],"categories":["Google AI Summer School","Experiences"],"content":"This talk was presented by Cristian Sminchisescu, who is a Research Scientist leading a team at Google, and a Professor at Lund University. His talk was titled End-to-end Generative 3D Human Shape and Pose models, and active human sensing\n3D Human Sensing has many applications, in the field of animation, sports motion, AR/VR, medical industry etc. It is a known fact that humans are very complex, the body has 600 muscles, 200 bones and 200 joints. Clothing that humans wear have folds and wrinkles, there are many different types of garments and cloth-body interactions.\nChallenges Typical challenges in 3D human sensing include:\n High dimensionality, articulation and deformation Complex appearance variations, clothing and multiple people Self occlusion or occlusion by scene objects Observation (depth) uncertainty (especially in monocular images) Difficult to obtain accurate supervision of humans  This is where we can exploit the power of machine and deep learning, we aim to come up with a learning model that:\n Understands large volumes of data Connects between images and 3D models  Problems that need to be solved It is imperative to FIND THE PEOPLE . We then need to infer their pose, body shape and clothing. The next step would be to recognize actions, behavioral states and social signals that they make, followed by recognizing what objects they use.\nVisual Human Models Different Data types we take into consideration are:\n Multiple Subjects Soft Tissue Dynamics Clothing This is all fed into the learning model  Generative Human Modeling Dynamic Human Scans $\\mathbf{\\xrightarrow[\\text{deep learning}]{\\text{end to end}}}$ Full Body articulated generative human models. The Dynamic Human Scans are in the form of very dense 3D Point Clouds.\nGHUM and GHUML Cristian then talked about his paper GHUM \u0026amp; GHUML: Generative 3D Human Shape and Articulated Pose Models GHUM is the moderate generative model with 10168 vertices and GHUML is the light version with 3190 vertices, however both have a shared skeleton that has minimal parameterization and anatomical joint limits.\nThe model faciliates Automatic 3D Landmark detection with multiview renderings, 2D landmark detection and 3D landmark triangulation. Automatic Registration is able to calculate deformations.\nEnd to End Training Pipeline End To End Training Pipeline    Once data is mapped to meshes and put into registered format, next step is to encode and decode static shapes (using VAE) Kinematics is learned using Normalizing Flow model Mesh filter (mask): to integrate close up scans with models, fed into the optimization step To train landmarks, we use annotated image data  Evaluation For the variational shape and expression autoencoder, VAE works better than OCA, with reconstruction error lying between 0-20mm. Motion Retargeting and Kinematic Priors are done by retargetting models to 2.8M CMU and 2.2M Humans3.6M motion capture frames.\nNormalizing Flows for Kinematic Priors Normalizing Flows for Kinematic Priors    A normalizing flow is a sequence of invertible transformations applied to an original distribution Use a dataset $\\mathcal{D}$ of human kinematic poses $\\theta$ as statistics for natural human movements Use normalizing flow to warp the distribution of poses into a simple and tractable density function e.g. $\\mathbf{z} \\sim \\mathcal{N}(0 ; \\mathbf{I})$ The flow is bijective, trained by maximizing data log-likelihood $$\\max _{\\phi} \\sum _{\\partial \\in \\mathcal{D}} \\log p _{\\phi}(\\theta)$$  GHUM and SMPL GHUM vs SMPL    GHUM is close (slightly better) to SMPL in skinning visual quality The vertex point-to-plane error (body-only) is GHUM: 4.23mm and SMPL: 4.96mm  Conclusions  An effective Deep Learning Pipeline to build generative, articulated 3D human shape models GHUM and GHUM are two full body human models that are available for research:(https://github.com/google-research/google-research/tree/master/ghum). We can jointly sample shape, facial expressions (VAEs) and pose (normalizing flows) We have low res and high res models, that are non-linear (linear as special case)  Other Work Some other interesting papers that Cristian pointed out were Weakly Supervised 3D Human Pose and Shape Reconstruction with Normalizing Flows (ECCV 2020) that works on Full Body Reconstruction in Monocular Images, and Neural Descent for Visual 3D Human Pose and Shape  (submitted to NeurIPS 2020) that talks about Self-Supervised 3D Human Shape and Pose Estimation.\nHuman Interactions A problem that many 3D deep learning practitioners face is dealing with human interactions during estimation and reconstruction. Contacts are difficult to estimate correctly because of:\n Uncertainty in 3D monocular depth prediction Reduced evidence of contact due to occlusion  Cristian then talked about his paper Three-dimensional Reconstruction of Human Interactions  and to move towards accurate reconstruction of interactions we need to:\n Detect contact Predict contact interaction signatures 3D reconstruction under contact constraints Modelling interactions     Conclusion (Interactions)  New models and datasets for contact detection, contact surface signature prediction, and 3d reconstruction under contact constraints Annotation has an underlying contact ground truth but not always easy to precisely identify from a single image Humans are reasonably consistent at identifying contacts at 9 and 17 region granularity, and contact can be predicted with reasonable accuracy too Contact-constrained 3D human reconstruction produces considerably better and more meaningful estimates, compared to non-contact methods  Cristian then concluded his wonderful lecture that talked about the most recent advances in Computer Vision in the 3D Deep learning field. It was a very informative and engaging lecture.\n","date":1598352116,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598352116,"objectID":"33b6c319362e1d106efccc0969919795","permalink":"https://archana1998.github.io/post/cristian-sminchisescu/","publishdate":"2020-08-25T16:11:56+05:30","relpermalink":"/post/cristian-sminchisescu/","section":"post","summary":"This lecture was presented by Cristian Sminchisescu, a professor at Lund University and working at Google Research. The lecture was titled End-to-end Generative 3D Human Shape and Pose models, and active human sensing","tags":["Computer Vision","3D Reconstruction"],"title":"Summer School Series: Lecture 4 by Cristian Sminchisescu","type":"post"},{"authors":["Archana Swaminathan"],"categories":["Google AI Summer School","Experiences"],"content":"This talk was delivered by Vineet Gupta , a research scientist at Google Brain, Mountain View California. His talk was titled Adaptive Optimization.\nThe optimization problem The optimization problem aims to learn the best function from a class of functions. $$\\operatorname{Class} : { \\hat{y} = M(x | w), for \\space w \\in \\mathbb{R}^{n} }\\ $$\nA class is most often specified as a neural network, parameterized by w. If the class is too large, overfitting happens. If the class is too small, well we end up getting bad results.\nThe most common function to find the best function is supervised learning.\nTraining examples: input output pairs such as (x1, y1),\u0026hellip;.(xn, yn)\nLearning rule: Estimating $w$ such that $\\hat{y_{i}} = M(x_{i}|w) \\approx y_{i}$, and $w$ approximately minimizes $ F(w) = \\sum_{i=1}^{n} l(\\hat{y_{i}},y_{i})$ (the loss function)\nIn a feed-forward Deep Neural Network, gradient descent for the entire training is expensive. For this reason, we sample points and find the gradient for them.\nStochastic Optimization The optimizer starts with the network denoted as $M(x|w)$.\nAt each round t: (the goal is to minimize $F(w)$)\n Optimizer has decided upon $w_{t}$ Optimizer receives the input $ [x_{i} ]_{i=1}^{k}$ Optimizer makes prediction $[\\hat{y_{i}}= M(x_{i}|w_{t})]_{i=1}^{k}$ Optimizer receives the true outcome Optimizer computes the loss $l_{t} = \\sum_{i} l(y_{i},\\hat{y_{i}})$ and gradient $g_{t} = \\frac{\\partial }{\\partial w} \\sum_{i} l(y_{i},\\hat{y_{i}})$ Optimizer uses $g_{t}$ to update $w_{t}$ to get $w_{t+1}$  We stop when the gradients vanish or run out of time (or epochs).\nRegret Convergence can be defined as the average loss compared to the optimum $w^{*}$\n$$ R_{T} = \\frac{1}{T} \\sum_{t=1}^{T} l_{t} (w_{t}) - \\frac{1}{T} \\sum_{t=1}^{T} l_{t}(w^{*})$$\nThe proof of convergence can be picked up when $R_{T} \\rightarrow 0 \\text{ as } T \\rightarrow 0$. This is a very strong  requirement, regret tending to 0.\nIn convex optimization, $R_{T}$ can be computed in $O(\\frac{1}{\\sqrt{T}})$ time. Convex problems in SGD include faster convergence implies better condition number.\nMomentum What happens when the gradients become very noisy? To solve this, we can take a running average of the gradients. $$ \\bar{g_{t}} = \\gamma \\bar{g_{t}} + (1-\\gamma)g_{t}$$ Thus the momentum step becomes: $$w_{t+1} = w_{t}-\\eta_{t} \\bar{g_{t}}$$ The momentum approach works very well and is extremely popular, till date.\nAnother way to solve the problem is by using second order methods. To minimize $F(w)$,\n$$F(w) \\approx F(w_{t}) + (w - w_{t})^{T} \\nabla F(w_{T}) + \\frac{1}{2} (w - w_{t})^{T} \\nabla^{2} F(w_{t}) (w - w_{t}) $$ (first two terms of the Taylor series).\nThe minimum is at: $w_{t+1} = w_{t} - \\nabla^{2} F(w_{t})^{-1} \\nabla F(w_{t})$\nThe biggest problem with this is computing the $\\nabla^{2} F(w_{t})$ (Hessian) is very expensive, as it is a $n*n$ matrix with $n$ number of parameters.\nAdaGrad For gradient $g_{i}$ $$ H_{t} = \\sqrt{(\\sum_{s\\leq{t}} g_{s} g_{s}^T)} $$\nThis is used as matrix for the Mahalnobis metric, that will be used. $$ \\therefore w_{t+1} = \\operatorname{argmin}_{w} \\frac{1}{2\\eta} ||w - w _{t}|| _{H _{t}}^{2} +\\hat{l _{t}}(w) $$\nThe AdaGrad update rule is: $w_{t+1} = w_{t} - \\eta H_{t}^{-1} g_{t}$. This is again very expensive, $O(n^{2})$ storage and $O(n^{3})$ time complexity per step.\nThe solution One way to solve this is by the diagonal approximation, by taking only the diagonal matrix of the Hessian instead of the entire matrix. $$H_{t} = \\operatorname{diag}{(\\sum_{s\\leq{t}}g_{s}g_{s}^{T}+\\epsilon\\operatorname{I})}^{\\frac{1}{2}}$$\nThis take $O(n)$ space and $O(n)$ time per step.\nAdaGrad has been so successful that there have been plenty of variants like AdaDelta/RMS Prop and Adam.\nFull-matrix Preconditioning AdaGrad Preconditioner For $w_{t}$ of size 100 * 200, $g_{t}$ flattens to a 20,000 vector and then becomes 20k * 20k in size.\nThe Kronecker Product Given a $m * n$ matrix $A$ and $p * q$ matrix $B$, their Kronecker Product $C$ is defined as $$C = A \\bigotimes B $$ This is also called the matrix direct product, and is a $(mp)*(nq)$ matrix (every element of $A$ multiplied with $B$). It commutes with standard matrix product along with exponentials.\nThe Shampoo Preconditioner Decomposed Matrix   The Shampoo Update: Adagrad update: ${w} _{t+1} ={w} _{t}-\\eta H _{t}^{-1} {g} _{t}$\nShampoo factorization: $w_{t+1}=w_{t}-\\eta\\left(L_{i}^{\\frac{1}{4}} \\otimes R_{t}^{\\frac{1}{4}}\\right)^{-1} g_{t}$\nShampoo update: $W_{t+1}=W_{t}-\\eta L_{t}^{-\\frac{1}{4}} G_{t} R_{t}^{-\\frac{1}{4}}$ Theorem (convergence): If ${G} _{1}, \\mathrm{G} _{2}, \\ldots, \\mathrm{G} _{\\mathrm{T}}$ of rank $\\leq \\mathrm{r},$ then the rate of convergence is:\n$$\\frac{\\sqrt{\\mathrm{r}}}{\\mathrm{T}} \\operatorname{Tr}\\left(\\mathrm{L} _{\\mathrm{T}}^{\\frac{1}{4}}\\right) \\operatorname{Tr}\\left(\\mathrm{R} _{\\mathrm{T}}^{\\frac{1}{4}}\\right)=\\mathrm{O}\\left(\\frac{1}{\\sqrt{\\mathrm{T}}}\\right)$$\n$$({R_{t}}=\\sum_{s \\leq t} G_{s}^{\\top} G_{s}$ and $L_{t}=\\sum_{s \\leq t} G_{s} G_{s}^{T})$$\nImplementing Shampoo The training system can be of two types:\n Asynchronous (accelerators don\u0026rsquo;t need to talk to each other, however it is hard for the parameter servers to handle) Synchronous (accelerator sends gradients to all the other accelerators, for them to average and update)  Challenges  Tensorflow and PyTorch focus on 1st order optimizations Computing $L_{t}^{-\\frac{1}{4}}$ and $R_{t}^{-\\frac{1}{4}}$ is expensive L, R have large condition numbers (upto the order of 1013). SVD is very expensive: $O(n^{3})$ in largest dimension Large layers are still impossible to precondition  Solutions  Using high precision arithmetic (float 64), not performing the computations on a TPU. Computing preconditioners every 1000 steps is alright. Replace SVD with an iterative method Only matrix multiplications needed  Warm start: use previous preconditioner Reduce condition number, remove top singular values   Optimization for large layers  Precondition only one dimension Block partioning the layer works better    Shampoo implementation and conclusion Shampoo gets implemented on a TPU+CPU. It is a little more expensive than AdaGrad but waay faster (saves 40% of the training time with 1.95 times fewer steps). Shampoo works well in language and speech domains, it isn\u0026rsquo;t suitable for image classication yet (for this Adam and AdaGrad work much better).\nThe Shampoo paper can be found here \n","date":1598178196,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598178196,"objectID":"e1efd9fe2500b09e5ba2af73c883bd5b","permalink":"https://archana1998.github.io/post/vineet-gupta/","publishdate":"2020-08-23T15:53:16+05:30","relpermalink":"/post/vineet-gupta/","section":"post","summary":"This lecture was delivered by Vineet Gupta, a scientist working in the Machine Learning theory group in Google Brain, on Adaptive Optimization","tags":["Machine Learning","Mathematics"],"title":"Summer School Series: Lecture 3 by Vineet Gupta","type":"post"},{"authors":["Archana Swaminathan"],"categories":["Google AI Summer School","Experiences"],"content":"Neil Houlsby presented a great talk on Large Scale Visual Representation Learning and how Google has come up with solutions to some classical problems.\nEvaluation of parameters There are two main ways of evaluating parameters from a network, that extracts the parameters. They are:\n Linear Evaluation: We freeze the weights and retrain the head Transfer Evaluation: We retrain end to end with new head  Visual Task Adaptation Benchmark (VTAB) VTAB is an evaluation protocal designed to measure progress towards general and useful visual representations, and consists of a suite of evaluation vision tasks that a learning algorithm must solve. We mainly have three types of tasks,  Natural tasks, Specialized tasks and Structured Datasets. \nA query that was posed was how useful ImageNet labels would be for pretrained models to work on these three tasks. It has been seen that ImageNet labels work well for Natural images, and not well for the other two tasks.\nRepresentation learners pre-trained on ImageNet can be of three forms:\n GANs and autoencoders Self-supervised Semi-supervised / Supervised approach  It has been seen that for natural tasks, representations prove to be more important than obtaining more data, and the supervised approach is far better than the unsupervised approach. For structured tasks, a combination of supervised and self-supervised learning works the best.\nIt was also mentioned that by modern standards, ImageNet is of incredibly small-scale, thus scaling models on ImageNet were not proven to be effective.\nSomething to specifically keep in mind is that upstream can be expensive, but downstream should be cheap (in terms of both data and compute). For the upstream, examples of suitable large datasets are ImageNet-21k for supervised learning, and YouTube-8M for self-supervised learning.\nBiT-L Neil introduced the Big Transfer Learning (BiT-L) algorithm and talked about it in detail. The first thing he mentioned about BiT-L was that batch normalization was replaced with  group normalization  for ultra-large data. Advantages of this were having no train/test discrepancy, and no state which made it easier to co-train with multiple steps.\nIt was highlighted that optimization at scale implies that schedule is crucial and not obvious. Also, early results of models can be misleading.\nTo perform cheap tranfer, we need low compute, few/no validation data and diverse tasks. For doing few-shot transfer, pretraining on ImageNet-21k and JFT-300M helps.\nRobustness Models trained with ImageNet aren\u0026rsquo;t necessarily robust most of the times. To test OOD robustness (Out-Of-Distribution), we use datasets like ImageNet C, ImageNet R and ObjectNet.\nModern Transfer Learning Modern Transfer Learning calls for a big, labelled datset, a big model and careful training (using about 10 optimization recipes) While testing with OOD, increasing datset size with a fixed model and increasing dataset size leads to an increase in performance, especially in the case of very large models.\nTo summarize, Bigger transfer $\\rightarrow$ Better Accuracy $\\rightarrow$ Better Robustness\n For checking impact on object  location  invariance, we see accuracy improves and becomes more uniform across location This proves to be the same in the case of impact on object size invariance However, in the case of object rotation invariance for ResNet50, it does not become more uniform across rotation angles, but for ResNet101*3, it maintains uniformity  Conclusion Main takeaways from the talk and BiT-L were:\n Scale is one of the key drivers of representation learning performance Especially effective for few-shot learning and OOD Robustness Also seen and mirrored in language domain  Links to the GitHub repositories are: Big Transfer  and Visual Task Adaptation Benchmark (VTAB)\n","date":1598031711,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598031711,"objectID":"f7a7deb15194eaf12ab641675b754b7e","permalink":"https://archana1998.github.io/post/neil-houlsby/","publishdate":"2020-08-21T23:11:51+05:30","relpermalink":"/post/neil-houlsby/","section":"post","summary":"This lecture was delivered by Neil Houlsby, currently working in Google Brain, Zurich, on Large Scale Visual Representation Learning","tags":["Machine Learning","Computer Vision"],"title":"Summer School Series: Lecture 2 by Neil Houlsby","type":"post"},{"authors":["Archana Swaminathan"],"categories":["Google AI Summer School","Experiences"],"content":"This is an article about what Jean-Phillipe Vert talked about at the Google Research India-AI Summer School 2020. The lecture was titled  Differentiable Ranking and Sorting  and lasted about 2 hours.\nDifferentiable Programming What is machine learning and deep learning?\nMachine learning is just to give trained data to a program and get better results for complex problems. For example:\nA neural network to recognize cats and dogs   These networks usually use vectors to do the computations within the network, however in recent research models are getting extended to non-vector objects (strings, graphs etc.)\nJean then gave an introduction to permutations and rankings and what he aspired to do, informally. Permutations are not vectors/graphs, but something else entirely. Some data are permutations (input, output etc) and some operations may involve ranking (histogram equalization, quantile normalization)\nWhat do these operations aspire to do?\n Rank pixels Extract a permutation and assign values to pixels only based on rankings  Permutations A permutation is formally defined as a bijection, that is:\n$$\\sigma:[1, N] \\rightarrow[1, N]$$\n  Over here, $\\sigma(i)=$ rank of item $i$\n  The composition property is defined as: $\\left(\\sigma_{1} \\sigma_{2}\\right)(i)=\\sigma_{1}\\left(\\sigma_{2}(i)\\right)$\n  $\\mathrm{S}_{N}$ is the symmetric group and\n  $\\left|\\mathbb{S}_{N}\\right|=N !$\n  Goal Our primary goal is:\nMoving between spaces   Some definitions here are:\n Embed:   To define/optimize $f_{\\theta}(\\sigma)=g_{\\theta}($embed$(\\sigma))$ for $\\sigma \\in \\mathbb{S}_{N}$ E.g., $\\sigma$ given as input or output  Differentiate:   To define/optimize $h_{\\theta}(x)=f_{\\theta}($argsort$(x))$ for $x \\in \\mathbb{R}^{n}$ E.g., normalization layer or rank-based loss  Argmax To put it in simple words, the argmax function identifies the dimension in a vector with the largest value. For example, $\\operatorname{argmax}(2.1, -0.4, 5.8) = 3$\nIt is not differentiable because:\n As a function, $\\mathbb{R}^{n} \\rightarrow[1,n]$, the output space is  not continuous  It is piecewise constant (i.e, gradient = 0 almost everywhere even if the output space was continuous)  Softmax It is a differentiable function that maps from $\\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$, where\n$$\\operatorname{softmax}_ {\\epsilon} (x)_ {i} =\\frac{e^{x_{i} / \\epsilon}}{\\sum_{j=1}^{n} e^{x_{j} / \\epsilon}}$$\nFor example, $\\operatorname{softmax}(2.1, -0.4, 5.8) = (0.027, 0.02, 0.972)$\nMoving from Softmax to Argmax $$\\lim _ {\\epsilon \\rightarrow 0} \\operatorname{softmax}_{\\epsilon}(2.1,-0.4, 5.8)=(0,0,1)=\\Psi(3)$$\nwhere $\\psi:[1, n] \\rightarrow \\mathbb{R}^{n}$ is the one-hot encoding. More generally, $$ \\forall x \\in \\mathbb{R}^{n}, \\quad \\lim_ {\\epsilon \\rightarrow 0} \\operatorname{softmax}_{\\epsilon}(x)=\\Psi(\\operatorname{argmax}(x)) $$\nMoving from Argmax to Softmax 1. Embedding Let the simplex $$ \\Delta_{n-1}=\\operatorname{conv}({\\Psi(y): y \\in[1, n]}) $$ Then we have a variational characterization (exercice left to us): $$ \\Psi(\\operatorname{argmax}(x))=\\underset{z \\in \\Delta_{n-1}}{\\operatorname{argmax}}\\left(x^{\\top} z\\right) $$\nSimplex representation   2a. Regularization Let the entropy be defined as $H(z)=-\\sum_{i=1}^{n} z_{i} \\ln \\left(z_{i}\\right)$ for $z_{i} \\in \\Delta_{n-1}$\nThen we have (exercise left to us): $$ \\operatorname{softmax}_ {\\epsilon}(x)=\\underset{z \\in \\Delta_{n-1}}{\\operatorname{argmax}}\\left[x^{\\top} z+\\epsilon H(z)\\right] $$\nThe entropy is maximum at the middle and minimum as the corners, as displayed below\nEntropy in the simplex   2b. Pertubation Let $G=\\left(G_{1}, \\ldots, G_{n}\\right)$ be i.i.d. Gumbel (0,1) random variables. Then we have (exercice): $$ \\operatorname{softmax}_{\\epsilon}(x)=E \\underset{z \\in \\Delta_{n-1}}{\\operatorname{argmax}}\\left[x^{\\top}(z+\\epsilon G)\\right] $$\nSummary From moving between argmax and softmax, we can:\n  Embed, such that $$ \\Psi(\\operatorname{argmax}(x))=\\underset{z \\in \\Delta_{n-1}}{\\operatorname{argmax}}\\left(x^{\\top} z\\right) $$\n  Regularize or pertub: $$ \\operatorname{softmax}_ {\\epsilon}(x)=\\underset{z \\in \\Delta_{n-1}}{\\operatorname{argmax}}\\left[x^{\\top} z+\\epsilon H(z)\\right] = E \\underset{z \\in \\Delta_{n-1}}{\\operatorname{argmax}}\\left[x^{\\top}(z+\\epsilon G)\\right] $$\n  Both of these lead to efficient and stochastic Jacobian estimates. We can generalize this to other discrete operations such as rankings, by various techniques, examples being the SUQUAN and Kendall embeddings.\nWe then have to make a differentiable approximation to $\\Phi (\\operatorname{argsort}(x))$, which can be done using Optimal Transport and Entropic Regularization. It has been experimentally proven that this works faster than neural sort for sorting 5 numbers between 0 and 9999.\nJean concluded saying that:\n Machine learning can exist beyond vectors, strings and graphs We can calculate different embeddings of symmetric groups Differentiable sorting and ranking can be done through regularization and perturbation This can be generalized to other discrete operations  ","date":1598017037,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598017037,"objectID":"6672a061b96e7a0f74e0ee89b687b754","permalink":"https://archana1998.github.io/post/jean-vert/","publishdate":"2020-08-21T19:07:17+05:30","relpermalink":"/post/jean-vert/","section":"post","summary":"This lecture was conducted by Jean-Phillipe Vert, a research scientist at Google AI, Paris, on differentiable ranking and sorting","tags":["Machine Learning","Mathematics"],"title":"Summer School Series: Lecture 1 by Jean-Phillipe Vert","type":"post"},{"authors":["Archana Swaminathan"],"categories":["Google AI Summer School","Experiences"],"content":"This article has been written from notes I took throughout the Opening Keynote at the Google AI Summer School. The opening keynote was delivered by Jeff Dean, Head of Google AI and moderated by Manish Gupta, Director of Google AI Research, Bangalore. The Keynote was titled  Deep Learning to Solve Challenging Problems. \nIntroduction Jeff Dean is a Senior Fellow at Google and the global head of Google AI. He saved Google at a very critical time and is essential to what contributed to make the Google Search Engine the best and the fastest in the world today. He is currently doing exciting research in the field of explainable AI for problems that the world is facing. He also helped create Tensorflow, the world\u0026rsquo;s most used Machine Learning Library.\nNotes from the Talk: The marvel of Deep Learning Deep Learning has revolutionized the way of solving challenging problems. There are over 130 new papers on Machine Learning on Arxiv every day. Deep Learning can be considered a modern reincarnation of Artificial Neural Networks. Key benefits and features of Deep Learning are:\n Availability of new network architectures Ability to scale to larger datasets and efficient computation of the math Learns features from raw, noisy, heterogenous data No explicit feature engineering required  Deep Learning architectures are remarkably flexible with taking in inputs and giving outputs of various forms, some examples are getting a categorical label from a pixel input (image), an audio input translating to a phrase that is a string, and language translation from one language to another.\nDeep Learning has also helped us come up with solutions to problems where the computer can achieve better results than a human. One such example is the Imagenet challenge, that Stanford conducts every year that classifies images into classes.\n In 2011, the winner of the challenge was able to achieve 26% error, where humans were able to do the same task with 5% error. In 2012, Geoffrey Hinton and his team used Deep Learning for the very first time in this challenge, and was the pioneer of bringing deep convolutional networks for the image classification task. Following his attempt, Deep Learning became very popular in further editions of the challenge. In 2017, the winner of the challenge was able to achieve 3% error on the Imagenet dataset, finally beating the human error of 5%.  Deep Learning to solve world problems One thing that Jeff emphasized on, is how Deep Learning is being used to tackle the Grand Engineering Challenges of the 21st century One of the primary challenges that are under focus are restoring and improving urban infrastructure.\nA key advancement in this field is autonomous driving, which Deep Learning has aided to such an extent that the autonomous driving is far safer than the usual human driver, with 360-degree vision utilizing around 18 cameras to form a dense LiDAR point cloud.\nAnother field that Deep Learning revolutionalized is combining vision with robotics. For the task of a robot arm picking up an unseen object,\n In 2015, there was a 65% grasp success rate In 2016, with the robot trained to pick up multiple categories of objects, the accuracy rose up to 78% In 2018, this accuracy shot up to 96% when Deep Learning was introduced into the mix Self supervised imitation learning also uses deep learning, which is the ability of a robot to imitate actions from pixels (human footage) without supervision  Another of these challenges was Advancing Health Informatics. We got an insight into what Google AI is working on for this field.\n One of these is diagnosing diabetic retinopathy, the fastest growing cause of preventable blindness Screening of the individual can prevent blindness, however it is extremely specialized so most MD\u0026rsquo;s cannot do it. Google came up with a model that could diagnose the disease from image scans of the eye, in 2016 it was at par with the performance of general opthamologists, and in 2017 the accuracy became State of the Art, with accuracy matching that of Retinal Specialists  Many of these challenges and advances in the field of engineering and technology depend on the ability to understand text. The 2017 Tranformer Paper: Attention is all you need! was a revolutionary step in this direction, which was followed by BERT in 2018. BERT introduced principles for training that was very popular and appreciated, that are:\n Pre train a model on the \u0026ldquo;fill in the blanks\u0026rdquo; task, using large amounts of self supervised text. This model is then fine tuned on individual language tasks, on a smaller scale.  This brought in light the desire to have large model architectures that are sparsely activated, that desirably have huge remembering capacity but utilize only a small fraction of the model while testing with individual examples An example of this is the Per-Example Routing architecture.\nJeff highlighted one of the most major contributions from Google towards deep learning, the introduction of the open-source deep learning library Tensorflow. It remains the most popular and most downloaded Deep Learning Library until date, and has a vibrant open-source community, to the extent that only 1/3rd of the current contributors are employees of Google!\nComputer architecture for Deep Learning There was a time in the past where complex problems couldn\u0026rsquo;t be solved because of the lack of computational power. We have finally made strides that do not restrict the power available to us, so optimizing this is an important task. Google AI has been focusing on redesigning computers, as Deep Learning has transformed this field completely. They kept two main things in mind while devising a computer to do deep learning:\n First is, reduced precision is okay. The computer does not have to calculate results acccurately to the 10th or 20th decimal point. Second is, there are mostly only a handful of specific operations that constitute the math of Deep Learning, for example matrix multiplication, dot products, etc.  Keeping these in mind, Google introduced the Tensor Processing Unit, that does just this. We can connect TPUs together to form Pods, that are currently available to the public on cloud services. Pods can be connected together to make supercomputers, that can train architectures like ResNet50 and Inceptionv2 in under 30 seconds! TPUs are being designed for edge applications also, to do deep learning on smartphones.\nProblems of doing Machine Learning Today The usual flow of work for a machine learning specialist is to collect the data, use his ML expertise (data augmentation, hyperparameter tuning etc) and train and test the model. A rather new approach that reduces human intervention here is AutoML, where the \u0026ldquo;ML expertise\u0026rdquo; is tuned and tested by automatic methods.\nProblems that still remain are:\n We still start with little to no knowledge about the problem and have to rely on random initialization New problems need significant data and compute power Transfer learning and multi-task learning help with this, but are done modestly  What is desired to be achieved  Large but sparsely activated neural network architectures A single model that can be used to solve many tasks, by activating different parts of the network Dynamically adapting to new problems Adding new tasks easily  Thus concluded the Keynote. It was fantastic and insightful. A couple of Q\u0026amp;A that I found interesting have been mentioned below:\nQ: What advice do you have for young researchers?\nA: Focus on problems that matter to you, and learn as much as you can. Create a constellation of techniques and ideas that can help you gather and organize your thoughts\nQ: How do you read new papers and get a gist of it?\nA: You\u0026rsquo;ll find many discussions on LinkedIn and Twitter about the paper, sometimes just reading this will give you a gist of what\u0026rsquo;s going on in the paper\nQ: Something unrealistic that you wish would happen in the field of AI in the future?\nA: The creation of a system that can absorb the world\u0026rsquo;s knowledge and solve all our problems\nQ: Hyperparameter tuning is expensive for large models, how do researchers work on this?\nA: Scaling down the problem to probably 1% of it and training and tuning that completely, and marginal scaling up to the level you desire is the best approach\n","date":1597901033,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597901033,"objectID":"b3c6a1437ac92b7901b23733b78642a2","permalink":"https://archana1998.github.io/post/opening-keynote/","publishdate":"2020-08-20T10:53:53+05:30","relpermalink":"/post/opening-keynote/","section":"post","summary":" The opening keynote was delivered by Jeff Dean, Head of Google AI and moderated by Manish Gupta, Director of Google AI Research, Bangalore","tags":["Machine Learning","Deep Learning"],"title":"Opening Keynote at the Google AI Summer School, 2020","type":"post"},{"authors":["Archana Swaminathan","Rushabh Musthayala"],"categories":["Experiences"],"content":"Flipkart recently concluded their 2 month long annual hackathon for students of Indian engineering colleges. This year’s edition saw over 20,000 participants and boasted of a prize pool of around Rs. 300,000 (-4000 USD). Our team (Gradient Ascent) made it to the 3rd round of the competition and I am writing about our experience in this article.\nProblem Statement A fashion retailer wants to source ongoing and upcoming fashion trends from major online fashion portals and online magazines in a consumable and actionable format, so that they are able to effectively and efficiently design an upcoming fashion product portfolio.\nDeliverables:\n Identify products that are better performers (in a rank ordered fashion) Help the user view the products that are both trending and lagging Identify a logic for classifying products as per their trendiness  We were asked to complete the challenge for just the t-shirt product vertical, but to ensure that our solution would be scalable to other products as well.\nInitial Analysis We started off by performing a literature review on current research in the field of fashion with respect to deep learning. We looked at previous attempts of learning attributes from fashion images, modelling trends as timeseries data, fashion image encodings, object detection, etc.\nAfter spending some time on our research, we split the problem into the following subproblems to tackle independently:\n Data Collection Object Detection Attribute/Feature learning Ranking Grouping (trending/lagging)  Data Collection According to the problem statement, we had to extract data from e-Commerce sites and other fashion portals and magazines. We tried our best to include data from all those categories to ensure we had a balanced dataset for our classification and ranking later on. After scouring the web for some good resources, we finally settled on the following:\n Flipkart Amazon Pinterest (curated collections of fashion trends) Vogue India Myntra  We felt this combination of multipurpose e-Commerce sites, well established fashion magazines, social network sites and dedicated fashion shopping sites would ensure we had good representation from all sectors. We collected an average of around 600 images from each website, giving us a total of 3000 to work with. Web scraping was done in Python using the Selenium framework. The scripts used to scrape data from any website were pretty similar and any new sites could be added with minor modifications, hence this step was easily scalable. From e-commerce sites, we scraped the images, product names, ratings and the number of reviews to with ranking later on. From the other portals, we extracted just the images.\nObject Detection One of the biggest problems we faced when extracting images from fashion magazines and social media sites is that they don’t limit themselves to just t-shirts. When they put out a catalogue/collection, it has everything ranging from skirts to sweaters to scarves. Furthermore, even in pictures where the shirt was the highlight, other features such as the model’s pose, skin colour and distance from the camera could confuse our model in the later stages of this project. Keeping all this in mind, we decided to use an object detection model to filter our data to ensure we had only pictures of t-shirts. Additionally, we cropped the images according to their bounding boxes to counter the other aforementioned problems. This was done using a pretrained YOLOv3 model trained on the DeepFashion2 dataset, implemented using PyTorch.\nAttribute/Feature learning This is where we faced our major setback. Our initial plan was to train a model to learn the attributes (neck type, sleeve length, patterns, etc) and to return them back for later use. We were then going to perform FP growth on our set of attributes of each image to obtain the frequent itemsets which would correspond to the most common combination of features and hence, trending/popular styles.\nIt didn\u0026rsquo;t work out however, as we couldn’t find an appropriate dataset to work with such a task given our time constraints so we had to try out our backup plan.\nOur plan involved getting numeric encodings for the fashion images in place of the attribute list and performing clustering on the encodings. The largest clusters would correspond to the most popular types of clothes, and similarly, the smallest clusters would represent the lagging ones, assuming our calculated encodings are a fair representation of the original image. Since we were working with just images (unlabeled) data, we had to devise an unsupervised approach for learning the image encodings. After considering various options, we decided to go ahead using an autoencoder based on a CNN architecture. We did this for 2 major reasons:\n Convolutional layers would help notice particular features of t-shirts such as the necktype length and patterns if any We can insight on how accurate our encodings to reconstruct the image  Here’s a summary of the model we used:\nFrequent Itemset Mining   We then plotted some of the reconstructed images side by side with their original counterparts and got pretty good results considering the simplicity of the network and size of the dataset. The encodings were able to capture some important features of the clothes in question.\nModel Architecture   Ranking As far as e-commerce sites go, there are 2 main criteria used to determine how “good” a product is – the number of reviews and the rating it has. What would you consider to be better? 10 reviews with a 5-star rating? Or 50 reviews with a 4.7-star rating? This was the major question we had to answer to be able to rank these products properly. We needed an effective way of combining these 2 into one reliable metric. After doing some research on this area and tying out different methods of combing them, we settled with an approach based on a Bayesian view of the beta distribution, described beautifully in this video by 3blue1brown We used this principle to come up with our own “Popularity Metric” which was calculated as follows:\nReconstructed Images from encodings   We now had a mechanism to compare and rank products effectively and a way to calculate accurate image encodings. We used both of these to train a model which predicts the Popularity Metric of a given clothing item given an input as the image encoding. We envisioned such a model to be extremely useful for designers that are looking for insight as to how their clothes might fair if they were put up for sale on e-commerce websites. Furthermore, the Popularity Metric could be calculated for all the images from magazines and portals like Vogue and Pinterest, so those products can be ranked and compared too! The architecture, simplified pipeline and a screenshot of the program in action are shared below.\nPopularity Metric    (n = number of reviews, s = star rating )  Popularity Metric Model   Grouping Since the FP growth idea fell through the roof, we went with clustering as our method of choice for grouping products in such a way that we can obtain the trending and lagging items. To ensure our clustering was done well, we experimented on a variety of clustering algorithms and chose the one with the highest silhouette coefficient. The algorithms tested were –\n K means Gaussian mixture model DBSCAN Mini batch k means Spectral clustering  Among those, K means had the highest silhouette efficient so we went ahead with that. We then split the data into clusters according to how many images were being considered for clustering (no. of clusters = no. of images/10). The products in the largest cluster could be inferred as the trending/popular products and those in the smallest clusters would be lagging products. We gave the user the option to spec which sources they wanted to consider for their clustering, giving them more flexibility with regards to analyzing what’s not and what’s not (what’s trending on Vogue might not be popular on Amazon).\nTo conclude, we were able to come up with a way to rank products properly and to group them based on whether they are trending or lagging. We also ensured that our solution is scalable on 2 fronts:\n Getting more data can be done easily with minor modifications to the existing script We can expand to different product verticals by changing the object of interest in the object detection model  The link to the GitHub Repo is at the top of this page. Hope you found this interesting, thanks for reading!\n","date":1597666172,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597666172,"objectID":"5e2270a05581471e88f667d7451c48d0","permalink":"https://archana1998.github.io/post/flipkart-grid/","publishdate":"2020-08-17T17:39:32+05:30","relpermalink":"/post/flipkart-grid/","section":"post","summary":"An article about our experience of taking part in our first hackathon","tags":["Machine Learning","Computer Vision"],"title":"Flipkart Grid 2.0 Hackathon","type":"post"},{"authors":["Archana Swaminathan"],"categories":["Undergraduate Projects"],"content":"Presented our work at the 54th Canadian CMOS conference, 2020.  Worked under the supervision of Dr. S Radhika, of the Department of Electrical and Electronics Engineering, BITS Pilani. Manuscript accepted and to be published.\n","date":1597592547,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597592547,"objectID":"c7ff7d07ddaf566b360c64808b8ec014","permalink":"https://archana1998.github.io/project/damage-detection/","publishdate":"2020-08-16T21:12:27+05:30","relpermalink":"/project/damage-detection/","section":"project","summary":"Used ConvNets to detect tornado damage done to buildings, using semantic segmentation.","tags":["Computer Vision","Machine Learning"],"title":"Structural Damage Detection using ConvNets","type":"project"},{"authors":["Archana Swaminathan"],"categories":["Undergraduate Projects"],"content":"Introduction Gesture Recognition Systems are commonly utilized as an interface between computers and humans, along with interacting with many electronic instruments. These systems can be classified into three classes as follows:\n Motion-based: When the user holds a device or a controller that detects the gesture made. Touch-based: When the system includes a touch-screen and the positions and directions of the finger or equivalent tool of the user are mapped, thus recognizing the gesture. Vision-based: When the system makes use of image and signal processing to detect gestures made without touching any device.  The first two types of systems need the users to hold and contact certain devices, for the gesture recognition, and vision-based systems use camera setups, image processing and techniques that involve computer vision. These systems are difficult to set up for small scale use and are also expensive and extremely power hungry. For building a system that needs to function when there are limited resources available, it is important that the setup cost, power consumption and ease and size of setup is taken into consideration. Keeping this in mind, we have built a contactless gesture recognition system that consists of a couple of digital infrared sensors, that have been programmed to do the gesture recognition using a custom algorithm, with an Arduino Uno Microcontroller.\nProblem Solving Methodology Components Used and Setup: The components we used are:\n Breadboard Arduino Uno Microcontroller 2 digital IR Sensors Jumper wires Laptop for interfacing Languages used: Arduino IDE (Based on C++) Python (for interfacing sensor output with VLC)  Setup   We connected two IR sensors to the Breadboard, placed at a distance of approximately 3 cm from each other. These were then interfaced with the Arduino Uno, which was connected to the laptop.\nVoltage is applied to the pair of IR LEDs, which in succession emit Infrared light. This light propagates through the air and once it hits the hand (or object), which acts as a hurdle, it is reflected back to the receiver. The LED on the diode glows, thus indicating that an object has been detected.\nA digital sensor system consists of the sensor itself, a cable, and a transmitter. The sensor has an electronic chip. The measuring signal is directly converted into a digital signal inside the sensor. The data transmission through the cable is also digital. This digital data transmission is not sensitive to cable length, cable resistance or impedance.\nUsing the concept of states and delay as in Digital Design, we have created two states of the two sensors each placed on the left and right. These two states are defined and calibrated using a time delay of a few hundred microseconds in the gesture classification algorithm written using the Arduino IDE.\nWe have taken two states in the algorithm into consideration namely Q(t) and Q(t+d) where d is the delay defined. The algorithm is defined such that left sensor and right sensor digital values are checked first and then after the defined delay, both sensors are checked for their Boolean values again and therefore the gesture is recognized and printed on the screen after running the code in the Arduino software. The chip on the Arduino Uno board plugs straight into the laptop’s USB port and supports the computer as a virtual serial port.\nState table   To make the gesture recognition feature interactive, we have interfaced the output with VLC Media player, so that we can pause, play and rewind/go forward with the playback. To do this interfacing, we have written a Python script, importing the Python library pyautogui, that provides functionality of control of the computer’s keyboard.\nResults and Conclusions We tested the gesture recognition system for accuracy by using the precision-recall matric. We took in 30 different samples for input. The precision is calculated as TP/(TP+FP), where TP denotes the number of true positives and FP denotes the number of false positives. The precision of our system is = 90% (27 TPs and 3 FPs)\nThe recall is calculated as TP/(TP+FN), where TP denotes the number of true positives, and FN denotes the number of false negatives. The recall of our system is = 86.6% (26 TPs and 4 FNs)\nHigh precision implies that there is less chances of getting false alarms, and recall expresses the ability to find all relevant information from the dataset. Practical Applications We have integrated the recognition system with VLC, to control playback of the video. Similarly, the setup can easily be integrated with any mobile device that is low on resources, as well as used with complex devices, as IR sensors are fundamental in building light reflection systems and are extremely versatile.\nFurther scope  Friendly user interface that can be easily understood by any user and eventually its application can be extended to more applications like PDF reader, video games etc. Computationally inexpensive and low power consuming hardware and software setup, that makes it ideal for integrating with any device, both simple and complex.  Limitations  Ambient light obstructs the functioning as is the case with infrared sensors, as they are extremely sensitive. A proper optical barrier must be used to prevent this. We have assumed values of time delays between gestures according to what worked well for our test dataset. This leads to the system being slightly inflexible with different speeds of gestures.  ","date":1597555992,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597555992,"objectID":"eeeaa0beb10035c11ba6cc5daaa06b5e","permalink":"https://archana1998.github.io/project/gesture-recognition/","publishdate":"2020-08-16T11:03:12+05:30","relpermalink":"/project/gesture-recognition/","section":"project","summary":"A contactless gesture recognition system that uses IR Proximity Sensors to classify different hand gestures. Interfaced with VLC Media player to pause and play videos.","tags":["Electronics","Signal Processing"],"title":"Contactless Gesture Recognition","type":"project"},{"authors":["Archana Swaminathan"],"categories":["Undergraduate Projects"],"content":"Introduction This project is inspired from the schematic described in the paper by I. A. Ismail and Galal H. Galal-Edeen. A multilayer perceptron network is used for both the encryption and decryption of images. The keys used for decryption are the fixed bias vectors, which remain constant throughout training. Multiplicative neural networks are used to help generate this constant vector, which is derived from a vector specified by the sender. The images are sent into the neural network and the output of the hidden layer gives the cipher. The cipher on being passed into the output layer gives the decrypted image. The weights are trained and updated using the backpropagation algorithm for learning, while the bias vector remains constant.\nTo get the constant bias vectors, the sender of the image specifies a numeric vector of the same size as the layer it is a bias of. The vector is broken down into subvectors and subsequent permutations of this vector are fed into a multiplicative neuron. The output of the multiplicative neural network will be added to the initial bias vector specified by the sender of the images. Since the bias vector is now a constant that is entirely dependent on the way the initial 6bias vector is arranged, it provides an additional level of security over the existing paradigm that employs a sender specified bias vector without any modifications done to it. All experiments have been done on, and results have been obtained from MATLAB R2018b.\nMultiplicative Neural Network A general structure for the multiplicative neuron is given below:\nStructure of multiplicative neuron   The input vector is (x 1 , x 2 ,\u0026hellip;..,x n ) which is a permutation of the initial specified bias vector. The weights vector is (w 1 , w 2, \u0026hellip;.., wn) and the bias vector (for this multiplicative neural network) is (b 1 , b 2, \u0026hellip;.., bn). Ω is a multiplicative operator and has the formula\n$$\\Omega=\\prod_{i=1}^{n}\\left(w_{i} x_{i}+b_{i}\\right)$$\nThe output of the neuron is then processed using ƒ(u) which is the logsig function $y=\\frac{1}{1+e^{-u}}$.\nTraining Algorithm The standard backpropagation algorithm has been modified for the training of the multiplicative neural network, which is used in optimizing the weights and biases. It is based on the popular steepest gradient descent approach. The error function E is defined as\n$$E=\\frac{1}{2 N} \\sum_{p=1}^{N}\\left(y_{p}-y_{p}^{d}\\right)^{2}$$\nwhere $\\ y_{p}^{d}$ is the desired output and yp is the actual output for the p th input that is fed into the multiplicative neural network. The weights and biases of the model are updated using the following rules:\n$$w_{i}^{\\text {new}}=w_{i}^{\\text {old}}+\\Delta w_{i}$$\n$$b_{i}^{\\text {new}}=b_{i}^{\\text {old}}+\\Delta b_{i}$$\nwhere\n$$\\Delta w_{i}=-\\eta \\frac{d \\boldsymbol{E}}{d w_{i}}=-\\eta \\frac{1}{N} \\sum_{p=1}^{N}\\left(\\left(y_{p}-y_{p}^{d}\\right) y_{p}\\left(1-y_{p}\\right) \\frac{u}{w_{i} x_{i}+b_{i}} x_{i}\\right)$$\n$$\\Delta b_{i}=-\\eta \\frac{d \\boldsymbol{E}}{d b_{i}}=-\\eta \\frac{1}{N} \\sum_{p=1}^{N}\\left(\\left(y_{p}-y_{p}^{d}\\right) y_{p}\\left(1-y_{p}\\right) \\frac{u}{w_{i} x_{i}+b_{i}}\\right)$$\n$\\eta$ is the learning rate parameter. The main purpose of this parameter is to control the convergent speed as desired.\nProcedure Two multiplicative neural network structures are used to generate the bias vector for the hidden layer and output layer of the multilayer perceptron network respectively. It is necessary to have a separate model for each vector as the layers are of different dimensions (different number of neurons) and thus, the bias vectors will be of different dimensions for both the hidden layer and the output layer. The number of elements in the bias vector will be the size of the input into the multiplicative neural network model, hence we require two different models.\nThe sender of the images first specifies a vector containing the same number of elements as the number of neurons of the (hidden/output) layer of the MLP. This is now broken down into subvectors (if the number of elements is 16, it can be broken into 4 subvectors of 4 elements each, etc.). This breaking down is essential as it becomes computationally difficult to calculate the permutations of a combination of numbers greater than 10. Once the subvectors are obtained, the individual permutations of each of the subvectors is stored into a matrix, which are then concatenated to form a bigger matrix of size p*q where p is the number of subvectors, and q is the dimension of the initially specified bias vector. A target vector (dummy vector, but must remain constant and not be generated randomly) is also specified by the sender.\nThis matrix is now fed into as input to the multiplicative neural network, where each row depicts an input sample. The network is trained using the algorithm specified, and the output is stored. This output is now added to the initial vector specified by the sender for the MLP, and the result thus becomes the new bias vector, which remains a constant throughout the experiment.\nThis procedure is repeated for defining and training the second multiplicative neural network, which generates the second bias vector for the MLP. Both these vectors are essential for proper image encryption and decryption.\nMLP used for image encryption and decryption The network has a structure of one input layer, one hidden layer, and one output layer. Adding further hidden layers can help in achieving image compression as dimensionality of the image is being reduced. The output of the hidden layer gives the cipher and the output of the output layer gives the decipher of the image. There are N elements in the input layer that are fed to the next (hidden layer), which consists of M neurons. The output layer has the same number of neurons as the input layer.\nThe MLP structure used in this project is given below.\nStructure of MLP   The network configuration is of NxMxN neurons which represent the input layer, hidden layer and output layer respectively. The sigmoid function (logsig) is used to generate the output of the hidden layer, which is defined as\n$$\\text { Logsig function: } Z=\\frac{1}{1+e^{\\left[-\\left(\\left(\\sum_{i=1}^{n} w_{1 i} x_{i}\\right)+b_{1 i}\\right)\\right]}}$$\nwhere w1i denotes the weight vector for the hidden layer, and b1i denotes the bias vector for the hidden layer.\nThe output of the output layer is calculated using a linear function (purelin in MATLAB), which is defined as\n$$\\text { Purelin function: } Y=m\\left[\\left(\\sum_{i=1}^{n} w_{2 i} z_{i}\\right)+b_{2 i}\\right]+c$$\nwhere w2i denotes the weight vector for the output layer, and b2i denotes the bias vector for the output layer.\nTraining Algorithm The error of the output of the network in each step ‘n’ while training, (the difference between the desired value and the actual value) is calculated by the following formula.\n$$\\Delta(n)=\\left[\\sum_{i=1}^{N}\\left(x_{i}-y_{i}\\right)^{2}\\right]^{1 / 2}$$\nThe weights are calculated using the following rules:\n  For the hidden-output layer:\na. The error signal for the q th neuron in the output layer is\n$$\\delta_{q}=m\\left(x_{q}-y_{q}\\right)$$\nb. The updated weight w2(p, q) is calculated as:\n$$\\begin{array}{c}w_{2(p, q)}(n+1)=w_{2(p, q)}(n)+\\Delta w_{2(p, q)}(n+1)\\end{array}$$\n$$\\begin{array}{c}\\Delta w_{2(p, q)}(n+1)=\\eta \\delta_{q} z_{p}+\\alpha\\left[\\Delta w_{2(p, q)}(n)\\right]\\end{array}$$\n  For the input-hidden layer:\na. The error signal for pth hidden neuron is calculated using:\n$$\\delta_{p}=z_{p}\\left(1-z_{p}\\right)\\left[\\sum_{k=1}^{N} \\delta_{k} w_{1(p,k)}\\right]$$\nb. The weight vector w1(i,p)(n) is calculated similarly as the above formula for the adjustment of weights, with zp being replaced with xi and $\\delta_{q}$ with $\\delta_{p}$ .\n  After one epoch, let $\\Delta(n)$ and $\\Delta(n+1)$ be the previous and current errors of the outputs of the neural network respectively. The rule that is followed whether to decide if the weights are being updated or not, is:\na. If $\\Delta(n+1)\u0026gt;1.04[\\Delta(n)]$, The new weights, output, keys (always constant), error are unchanged, and $\\alpha$ is changed to $0.7 \\alpha$\nb. If $\\Delta(n+1)\u0026lt;=1.04[\\Delta(n)]$, All the variables except the keys are updated to their new values, and $\\alpha$ is modified to $1.05 \\alpha$\n  These steps are carried out and repeated in each epoch, until the maximum number of epochs has been reached, or the error becomes less than a value that is predefined.\nProcedure The keys obtained from the multiplicative neural network are first normalized to lie within the range of (0,1). The normalization is simply done by dividing the elements of the bias vector by the maximum value of the elements of the vector.\nThe images that are fed into the neural network must all be of the same dimension, irrespective of them being training images or test images. For this project, images of various dimensions (256 x 256, 512 x 512 etc) have been scaled down to a dimension of 50 x 50. The images of this specified dimension are now segmented into sub images, of the number L (for the purpose of this project, L=100). The size of each sub image is x times x = N pixels, which makes N = 25. The segmentation is done using a custom defined segmentation function, that also converts each sub image into a 1-dimensional vector, and creates a matrix of dimension N x L (25 x 100) which is then fed in as input to the neural network.\nOnce the network is trained with the training set, it is ready to encrypt and decrypt images. The test images are segmented using the same segmentation function which was used to segment the training images, and are fed into the trained neural network as input.\nThe output of the hidden layer is computed using the output function (logsig), which was previously defined. Since the size of the input image is NL, the output of the hidden layer is a matrix of size ML. (For this project, N = 25 and M = 16). The encrypted image (cipher) is then obtained after the output matrix is transformed into a 2-D matrix, for which the segmented images must be properly arranged back.\nThe decrypted image is obtained when the output of the hidden layer is fed into the output layer. In short, it is the final output of the neural network and can directly be computed by feeding the input test image into the neural network.\nExperiments and results The project was done on MATLAB version R2018b on a computer with Intel Core i5 6 th generation processor.\nThe neural network was trained using 38 test images, out of which 14 were colour images, all downloaded from the USC Vision Database. The 14 colour images and any subsequent images henceforth used for testing were all converted into single channel images. The test images were segmented and passed into the neural network, which took approximately 40 seconds to train. The initial bias vectors were specified by the programmer and then was input into the two multiplicative neural networks to generate new elements, which were then added with the previous bias vector. Only after this, the bias vector for the MLP was fixed with this value and the MLP was made to train.\nEncryption and decryption of a test image of the Earth was extremely fast, with the NPCR and UACI tests giving scores of 99.9665% and 0.34916 respectively. The PSNR ratios for the original image with the decrypted image and the cipher were 39.4156 and 39.3973 respectively.\n","date":1596090959,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596090959,"objectID":"447045694cd9d481c0198b4259569dc0","permalink":"https://archana1998.github.io/project/encryption-decryption/","publishdate":"2020-07-30T12:05:59+05:30","relpermalink":"/project/encryption-decryption/","section":"project","summary":"Developed a novel algorithm for image encryption using Artificial Neural Networks.","tags":["Cryptography","Machine Learning"],"title":"Image Encryption and Decryption using Artificial Neural Networks","type":"project"},{"authors":["Archana Swaminathan"],"categories":["Undergraduate Projects"],"content":"Introduction The Ramanujan Sums were first proposed by Srinivasan Ramanujan in 1918, and have become exceedingly popular in the fields of signal processing,time-frequency analysis and shape recognition. The sums are by nature, orthogonal. This results in them offering excellent conservation of energy, which is a property shared by Fourier Transform as well.\nWe have used Matrix Multiplication to obtain the Ramanujan Basis, for our computation. The Ramanujan Sums are defined as nth powers of qthprimitive roots of unity, which can be computed using this simple formula:\n$$c_{q}(n)=\\mu\\left(\\frac{q}{g c d(q, n)}\\right) \\frac{\\varphi(q)}{\\varphi\\left(\\frac{q}{g c d(q, n)}\\right)}$$\nWhere $q=\\prod_{i} q_{i}^{a_{i}}$,(qi is prime). Then, $\\varphi(q)=q \\prod_{i}\\left(1-\\frac{1}{q i}\\right)$.\n$\\mu(n)$ is the Mobius function, which is equal to 0 if n contains a square number, 1 if n = 1 and (-1)*k if n is a product of k distinct prime numbers.\nThe Ramanujan matrix can be defined as:\n$$A(q, j)=\\frac{1}{\\varphi(q) M} c_{q}(\\bmod (j-1, q)+1)$$\nThe 2-D forward Ramanujan Sum Transform is given as:\n$$Y(p, q)=\\frac{1}{\\varphi(p) \\varphi(q)} \\frac{1}{M N} \\sum_{m=1}^{M} \\sum_{n=1}^{N} x(m, n) C_{p}(m) C_{q}(n)$$\nwhich in matrix terms can be defined as\n$$Y=A * A^{\\top}$$\nand the inverse 2D Ramanujan transform in matrix terms is:\n$$X=A^{-1} Y\\left(A^{-1}\\right)^{\\top}$$\nOriginal, Transformed and Inversed Image   Compressive Sensing The principle behind the use of compressive sensing as a signal processing technique, is that most test signals are not actually completely comprised of noise, but most have a great degree of redundancy in them. Sparse representation of signals in a particular domain signifies that most of the signal coefficients are either zero or close to zero.\nCompressive measurements, which are a weighed linear combination of signal samples, are first taken in a basis that is different from the sparse basis.\nAlgorithm We use the generated Ramanujan Basis to do the sparse reconstruction. First, the sparse signal is obtained by multiplyingthe Ramanujan Basis A with the flattened image vector (here we are using the Cameraman Image that has been resized to 50 * 50). Our Ramanujan Basis has dimensions of 2500 * 2500.\nThus,\n$$Z=A^{*} x$$\nWhere Z is the sparse representation of the cameraman image, and x is the flattened image vector of the original image. We next create a random measurement matrix of dimension m*n, where we keep m = 5000 and n = 2500. This measurement matrix (Phi) is then multiplied by the sparse signal z.\n$$Y=P h i * Z$$\nWe then use Orthogonal Matching Pursuit Algorithm, which aims to approximately find the most accurate projections of data in multiple dimensions on to the span of a redundant or overcomplete dictionary. Here, the overcomplete dictionary we use is the Ramanujan Basis A. The orthogonal matching pursuit algorithm is then applied onto the signal Y, and we have considered 1700 iterations.\nThe plot of the original sparse representation and the OMP representation is given below:\nPlot of original sparse representation (blue) and OMP representation (red)   The image is then reconstructed by taking the inverse of the Ramanujan Basis, and multiplying it with the OMP sparse representation.\n$$R e c =A^{-1} * x w s r$$\nWhere xwsr is the OMP representation, and Rec is the reconstructed image signal. This is then resized to obtain the final image, which has been compared with the original image below:\nOriginal and Final image   Results We compare and evaluate the performance of the Compressive Sensing Algorithm by using PSNR, SSIM and MSE image evaluation metrics.\n PSNR: Peak Signal to Noise Ratio SSIM: Structural Similarity Index MSE: Mean Square Error  Ideally, high values of PSNR, SSIM(max=1) and MSE show favourable performance of the reconstruction algorithm.The results obtained for this approach are:\n PSNR = 23.1362 SSIM = 0.6265 MSE = 315.8316  Image Denoising Image denoising is commonly analysed and solved as an inverse problem. A method of doing this is to decompose the image signal in a sparse way, over a dictionary that is overcomplete. We use the Ramanujan Dictionary here to do the denoising, which is trained with three images using the K-SVD algorithm, based on Orthogonal Matching Pursuit (OMP).\nK-SVD Algorithm The K-SVD algorithm is a type of K-means clustering, which has been generalized. The k- means clustering is also considered a method of doing representation of sparse signals. This implies solving the equation below, to find the best code to represent the signal data {yi} from i=1 to M\n$$\\min {D, X}\\left{|Y-D X|{F}^{2}\\right}, \\text { subject to } \\forall i,\\left|x_{i}\\right|_{0}=1$$\nF here is the Frobenius norm. The K-SVD algorithm is similar to the K-means in terms of the process of construction, but differs in the sense of the relaxation of the sparsity term in the constraint. This helps achieve a linear combination of the dictionary atoms. The relaxation is that the number of entries that are not zero in each column can be greater than 1, but less than a defined number T0.\nThus, the objective function hence becomes\n$$\\min {D, X}\\left{|Y-D X|{F}^{2}\\right} \\text { , subject to } \\forall i,\\left|x_{i}\\right|_{0} \\leq T_{0}$$\nIn the algorithm, the dictionary D is first fixed, and the aim is to find the perfect coefficient matrix X. To find this, a pursuit method that does the approximation of the optimal X is used. OMP was chosen as the suitable method to calculate the coefficients of the matrix here.\nImplementation and Results In the implementation for training the dictionary nd the sparse data representation, the parameters for the training are as follows:\n Patch Size = 15 Percentage of Overlap = 0.5 Sparsity Threshold = 6 Error Tolerance = 11.5 Three images, Boat, Lena and Barbara were used for the dictionary training. Patches were made of these images and stacked. The number of iterations was the size of the stacked images that formed the training data. In this case, the number was 2883. The dictionary and sparse data representation were trained using the K-SVD algorithm and saved, the training process took approximately 5.5 hours on an Intel i5 6 th gen processor, with a Nvidia 940mx GPU (personal laptop, using MATLAB R2019a).  Original and Reconstructed Boat image   We added Gaussian Noise to a Lena Image and denoised it using the trained dictionary.\nOriginal, Noisy and Denoised Lena image    MSE for noisy image = 68.0747 MSE for denoised image = 55.2772 PSNR for noisy image = 29.8009 PSNR for denoised image = 29.9672  Summary The Ramanujan Transform is a powerful transform and basis dictionary that can be used for sparse representation of an image. It can be trained efficiently and reconstructs images in a better way as compared to using DCT dictionary. For compressive sensing algorithm, the training time of the Ramanujan dictionary is more compared to the DCT dictionary training time, but it is more efficient in reconstruction. It is also good at denoising images, and is efficiently trained using the K-SVD algorithm.\n","date":1595077847,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595077847,"objectID":"72d8642b4f9a05c1ccc6178576ab9893","permalink":"https://archana1998.github.io/project/compressive-sensing/","publishdate":"2020-07-18T18:40:47+05:30","relpermalink":"/project/compressive-sensing/","section":"project","summary":"Used the Ramanujan Fourier Transform to do compressive sensing and denoising of images in the Ramanujan domain","tags":["Image Processing","Signal Processing"],"title":"Compressive Sensing and Denoising of Images using the Ramanujan Fourier Transform","type":"project"},{"authors":["Archana Swaminathan"],"categories":["Reviews"],"content":"This is a review of the ICLR 2017 paper by Zhang et. al. titled \u0026ldquo;Understanding Deep Learning requires rethinking generalization\u0026quot;Link to paper \nThe paper starts off with aiming to provide an introspection into what distinguishes networks that generalize well, from those who don’t.\nOne of the experiments conducted in the studies of the paper, is checking how well neural networks adapt to training when labels are randomized. Their findings establish that when the true data is completely randomly labelled, the training error that results is zero. Observations from this indicate that the effective capacity of neural networks is more than enough to memorize the entire dataset. Randomization of the labels is only a transformation of the data, and other learning parameters are constant and unchanged still. The resulting training time also increases by only a small factor. However, when this trained network is tested, it does badly. This indicates that just by randomizing labels, the generalization error can shoot up significantly without changing any other parameters of the experiment like the size of the model, the optimizer etc.\nAnother experiment conducted was that when the ground truth images were switched with random noise. This resulted in the networks training to zero training error, even faster than the case with the random labels. Varying the amount of randomization resulted in a steady deterioration of the generalization error, as the noise level increased. There were a wide variety of changes introduced into the dataset, that played with degrees and kinds of randomization with the pixels and labels. All of this still resulted in the networks able to fit the training data perfectly. A key takeaway from this is that the neural networks are able to capture the signals remaining in the data, while fitting the noise and randomization with brute force. The question that still remains unanswered after this is why some models generalize better than others, because it is evident that some decisions made while constructing model architectures do make a difference in its ability to generalize.\nTraditional approaches in statistical learning theory such as Rademacher complexity, VC dimension and uniform stability are threatened by the randomization experiments performed.\nThree specific regularizers are then considered to note the impact of explicit regularization, data augmentation, weight decay and dropout. These are tried out on Inception, Alexnet and MLPs on the CIFAR10 dataset, and later with ImageNet. Regularization helps to improve generalization performance, but the models still generalize well enough with the regularizers turned off. It was then inferred that this is more of a tuning parameter than a fundamental cause of good generalization. A similar result was noted with implicit regularization.\nAn interesting result proved in the paper was that there two layer depth networks of linear size, that can represent any labelling of the training data. A parallel approach in trying to understand the source of regularization for linear models was also not easy to point out. To sum up, this paper presents a thorough insight into how empirically easy optimization does not imply good regularization, and effective capacity of network architectures is better understood and defined.\n","date":1595061682,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595061682,"objectID":"5a3367d62d391224abafb9e8797a3c3e","permalink":"https://archana1998.github.io/post/regularization/","publishdate":"2020-07-18T14:11:22+05:30","relpermalink":"/post/regularization/","section":"post","summary":"A short and concise review of the ICLR paper","tags":["Machine Learning"],"title":"Understanding Deep Learning requires rethinking generalization","type":"post"}]