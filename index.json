[{"authors":["admin"],"categories":null,"content":"Hi! I am Archana Swaminathan, a final year undergraduate student at BITS Pilani, Hyderabad, India. I am currently working as a Visiting Researcher with the V-SENSE Research Group, Trinity College Dublin , and being mentored by Prof. Aljosa Smolic. I\u0026rsquo;m part of a group that is investigating 3D Geometry for Deep Learning, where we focus on applications that extend to Image-based reconstruction, pose estimation and visual computing.\nIn the past, I\u0026rsquo;ve had experience with working with Deep Learning in many projects, some of which are: predictive modelling for time series data, image encryption and decryption, image superresolution, semantic segmentation and object recognition.\nI\u0026rsquo;m very interested in research in the areas of Image Processing and Computer Vision, and wish to explore different applications in these fields as I motivate and prepare myself for a career as a researcher.\nIn my free time, I like to read books, play badminton and work out. I like taking long walks and I\u0026rsquo;m waiting to be able to go on treks and hikes once the pandemic blows over!\nI\u0026rsquo;m actively looking for Research Assistantship / PhD opportunities starting fall 2021. Do get in touch with me if you\u0026rsquo;re up for a chat!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://archana1998.github.io/author/archana-swaminathan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/archana-swaminathan/","section":"authors","summary":"Hi! I am Archana Swaminathan, a final year undergraduate student at BITS Pilani, Hyderabad, India. I am currently working as a Visiting Researcher with the V-SENSE Research Group, Trinity College Dublin , and being mentored by Prof.","tags":null,"title":"Archana Swaminathan","type":"authors"},{"authors":["Archana Swaminathan"],"categories":["Undergraduate Projects"],"content":"Introduction Gesture Recognition Systems are commonly utilized as an interface between computers and humans, along with interacting with many electronic instruments. These systems can be classified into three classes as follows:\n Motion-based: When the user holds a device or a controller that detects the gesture made. Touch-based: When the system includes a touch-screen and the positions and directions of the finger or equivalent tool of the user are mapped, thus recognizing the gesture. Vision-based: When the system makes use of image and signal processing to detect gestures made without touching any device.  The first two types of systems need the users to hold and contact certain devices, for the gesture recognition, and vision-based systems use camera setups, image processing and techniques that involve computer vision. These systems are difficult to set up for small scale use and are also expensive and extremely power hungry. For building a system that needs to function when there are limited resources available, it is important that the setup cost, power consumption and ease and size of setup is taken into consideration. Keeping this in mind, we have built a contactless gesture recognition system that consists of a couple of digital infrared sensors, that have been programmed to do the gesture recognition using a custom algorithm, with an Arduino Uno Microcontroller.\nProblem Solving Methodology Components Used and Setup: The components we used are: 1. Breadboard 2. Arduino Uno Microcontroller 3. 2 digital IR Sensors 4. Jumper wires 5. Laptop for interfacing Languages used: 1. Arduino IDE (Based on C++) 2. Python (for interfacing sensor output with VLC)\nWe connected two IR sensors to the Breadboard, placed at a distance of approximately 3 cm from each other. These were then interfaced with the Arduino Uno, which was connected to the laptop.\nVoltage is applied to the pair of IR LEDs, which in succession emit Infrared light. This light propagates through the air and once it hits the hand (or object), which acts as a hurdle, it is reflected back to the receiver. The LED on the diode glows, thus indicating that an object has been detected.\nA digital sensor system consists of the sensor itself, a cable, and a transmitter. The sensor has an electronic chip. The measuring signal is directly converted into a digital signal inside the sensor. The data transmission through the cable is also digital. This digital data transmission is not sensitive to cable length, cable resistance or impedance.\nUsing the concept of states and delay as in Digital Design, we have created two states of the two sensors each placed on the left and right. These two states are defined and calibrated using a time delay of a few hundred microseconds in the gesture classification algorithm written using the Arduino IDE.\nWe have taken two states in the algorithm into consideration namely Q(t) and Q(t+d) where d is the delay defined. The algorithm is defined such that left sensor and right sensor digital values are checked first and then after the defined delay, both sensors are checked for their Boolean values again and therefore the gesture is recognized and printed on the screen after running the code in the Arduino software. The chip on the Arduino Uno board plugs straight into the laptop’s USB port and supports the computer as a virtual serial port.\nTo make the gesture recognition feature interactive, we have interfaced the output with VLC Media player, so that we can pause, play and rewind/go forward with the playback. To do this interfacing, we have written a Python script, importing the Python library pyautogui, that provides functionality of control of the computer’s keyboard.\nSetup   Results and Conclusions We tested the gesture recognition system for accuracy by using the precision-recall matric. We took in 30 different samples for input. The precision is calculated as TP/(TP+FP), where TP denotes the number of true positives and FP denotes the number of false positives. The precision of our system is = 90% (27 TPs and 3 FPs)\nThe recall is calculated as TP/(TP+FN), where TP denotes the number of true positives, and FN denotes the number of false negatives. The recall of our system is = 86.6% (26 TPs and 4 FNs)\nHigh precision implies that there is less chances of getting false alarms, and recall expresses the ability to find all relevant information from the dataset. Practical Applications We have integrated the recognition system with VLC, to control playback of the video. Similarly, the setup can easily be integrated with any mobile device that is low on resources, as well as used with complex devices, as IR sensors are fundamental in building light reflection systems and are extremely versatile.\nFurther scope 1. Friendly user interface that can be easily understood by any user and eventually its application can be extended to more applications like PDF reader, video games etc. 2. Computationally inexpensive and low power consuming hardware and software setup, that makes it ideal for integrating with any device, both simple and complex.  Limitations 1. Ambient light obstructs the functioning as is the case with infrared sensors, as they are extremely sensitive. A proper optical barrier must be used to prevent this. 2. We have assumed values of time delays between gestures according to what worked well for our test dataset. This leads to the system being slightly inflexible with different speeds of gestures.  ","date":1597555992,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597555992,"objectID":"eeeaa0beb10035c11ba6cc5daaa06b5e","permalink":"https://archana1998.github.io/project/gesture-recognition/","publishdate":"2020-08-16T11:03:12+05:30","relpermalink":"/project/gesture-recognition/","section":"project","summary":"Introduction Gesture Recognition Systems are commonly utilized as an interface between computers and humans, along with interacting with many electronic instruments. These systems can be classified into three classes as follows:","tags":["Electronics"],"title":"Contactless Gesture Recognition","type":"project"},{"authors":["Archana Swaminathan"],"categories":["Undergraduate Projects"],"content":"Introduction This project is inspired from the schematic described in the paper by I. A. Ismail and Galal H. Galal-Edeen. A multilayer perceptron network is used for both the encryption and decryption of images. The keys used for decryption are the fixed bias vectors, which remain constant throughout training. Multiplicative neural networks are used to help generate this constant vector, which is derived from a vector specified by the sender. The images are sent into the neural network and the output of the hidden layer gives the cipher. The cipher on being passed into the output layer gives the decrypted image. The weights are trained and updated using the backpropagation algorithm for learning, while the bias vector remains constant.\nTo get the constant bias vectors, the sender of the image specifies a numeric vector of the same size as the layer it is a bias of. The vector is broken down into subvectors and subsequent permutations of this vector are fed into a multiplicative neuron. The output of the multiplicative neural network will be added to the initial bias vector specified by the sender of the images. Since the bias vector is now a constant that is entirely dependent on the way the initial 6bias vector is arranged, it provides an additional level of security over the existing paradigm that employs a sender specified bias vector without any modifications done to it. All experiments have been done on, and results have been obtained from MATLAB R2018b.\nMultiplicative Neural Network A general structure for the multiplicative neuron is given below:\nStructure of multiplicative neuron   The input vector is (x 1 , x 2 ,\u0026hellip;..,x n ) which is a permutation of the initial specified bias vector. The weights vector is (w 1 , w 2, \u0026hellip;.., wn) and the bias vector (for this multiplicative neural network) is (b 1 , b 2, \u0026hellip;.., bn). Ω is a multiplicative operator and has the formula\n$$\\Omega=\\prod_{i=1}^{n}\\left(w_{i} x_{i}+b_{i}\\right)$$\nThe output of the neuron is then processed using ƒ(u) which is the logsig function $y=\\frac{1}{1+e^{-u}}$.\nTraining Algorithm The standard backpropagation algorithm has been modified for the training of the multiplicative neural network, which is used in optimizing the weights and biases. It is based on the popular steepest gradient descent approach. The error function E is defined as\n$$E=\\frac{1}{2 N} \\sum_{p=1}^{N}\\left(y_{p}-y_{p}^{d}\\right)^{2}$$\nwhere $\\ y_{p}^{d}$ is the desired output and yp is the actual output for the p th input that is fed into the multiplicative neural network. The weights and biases of the model are updated using the following rules:\n$$w_{i}^{\\text {new}}=w_{i}^{\\text {old}}+\\Delta w_{i}$$\n$$b_{i}^{\\text {new}}=b_{i}^{\\text {old}}+\\Delta b_{i}$$\nwhere\n$$\\Delta w_{i}=-\\eta \\frac{d \\boldsymbol{E}}{d w_{i}}=-\\eta \\frac{1}{N} \\sum_{p=1}^{N}\\left(\\left(y_{p}-y_{p}^{d}\\right) y_{p}\\left(1-y_{p}\\right) \\frac{u}{w_{i} x_{i}+b_{i}} x_{i}\\right)$$\n$$\\Delta b_{i}=-\\eta \\frac{d \\boldsymbol{E}}{d b_{i}}=-\\eta \\frac{1}{N} \\sum_{p=1}^{N}\\left(\\left(y_{p}-y_{p}^{d}\\right) y_{p}\\left(1-y_{p}\\right) \\frac{u}{w_{i} x_{i}+b_{i}}\\right)$$\n$\\eta$ is the learning rate parameter. The main purpose of this parameter is to control the convergent speed as desired.\nProcedure Two multiplicative neural network structures are used to generate the bias vector for the hidden layer and output layer of the multilayer perceptron network respectively. It is necessary to have a separate model for each vector as the layers are of different dimensions (different number of neurons) and thus, the bias vectors will be of different dimensions for both the hidden layer and the output layer. The number of elements in the bias vector will be the size of the input into the multiplicative neural network model, hence we require two different models.\nThe sender of the images first specifies a vector containing the same number of elements as the number of neurons of the (hidden/output) layer of the MLP. This is now broken down into subvectors (if the number of elements is 16, it can be broken into 4 subvectors of 4 elements each, etc.). This breaking down is essential as it becomes computationally difficult to calculate the permutations of a combination of numbers greater than 10. Once the subvectors are obtained, the individual permutations of each of the subvectors is stored into a matrix, which are then concatenated to form a bigger matrix of size p*q where p is the number of subvectors, and q is the dimension of the initially specified bias vector. A target vector (dummy vector, but must remain constant and not be generated randomly) is also specified by the sender.\nThis matrix is now fed into as input to the multiplicative neural network, where each row depicts an input sample. The network is trained using the algorithm specified, and the output is stored. This output is now added to the initial vector specified by the sender for the MLP, and the result thus becomes the new bias vector, which remains a constant throughout the experiment.\nThis procedure is repeated for defining and training the second multiplicative neural network, which generates the second bias vector for the MLP. Both these vectors are essential for proper image encryption and decryption.\nMLP used for image encryption and decryption The network has a structure of one input layer, one hidden layer, and one output layer. Adding further hidden layers can help in achieving image compression as dimensionality of the image is being reduced. The output of the hidden layer gives the cipher and the output of the output layer gives the decipher of the image. There are N elements in the input layer that are fed to the next (hidden layer), which consists of M neurons. The output layer has the same number of neurons as the input layer.\nThe MLP structure used in this project is given below.\nStructure of MLP   The network configuration is of NxMxN neurons which represent the input layer, hidden layer and output layer respectively. The sigmoid function (logsig) is used to generate the output of the hidden layer, which is defined as\n$$\\text { Logsig function: } Z=\\frac{1}{1+e^{\\left[-\\left(\\left(\\sum_{i=1}^{n} w_{1 i} x_{i}\\right)+b_{1 i}\\right)\\right]}}$$\nwhere w1i denotes the weight vector for the hidden layer, and b1i denotes the bias vector for the hidden layer.\nThe output of the output layer is calculated using a linear function (purelin in MATLAB), which is defined as\n$$\\text { Purelin function: } Y=m\\left[\\left(\\sum_{i=1}^{n} w_{2 i} z_{i}\\right)+b_{2 i}\\right]+c$$\nwhere w2i denotes the weight vector for the output layer, and b2i denotes the bias vector for the output layer.\nTraining Algorithm The error of the output of the network in each step ‘n’ while training, (the difference between the desired value and the actual value) is calculated by the following formula.\n$$\\Delta(n)=\\left[\\sum_{i=1}^{N}\\left(x_{i}-y_{i}\\right)^{2}\\right]^{1 / 2}$$\nThe weights are calculated using the following rules:\n  For the hidden-output layer:\na. The error signal for the q th neuron in the output layer is\n$$\\delta_{q}=m\\left(x_{q}-y_{q}\\right)$$\nb. The updated weight w2(p, q) is calculated as:\n$$\\begin{array}{c}w_{2(p, q)}(n+1)=w_{2(p, q)}(n)+\\Delta w_{2(p, q)}(n+1)\\end{array}$$\n$$\\begin{array}{c}\\Delta w_{2(p, q)}(n+1)=\\eta \\delta_{q} z_{p}+\\alpha\\left[\\Delta w_{2(p, q)}(n)\\right]\\end{array}$$\n  For the input-hidden layer:\na. The error signal for pth hidden neuron is calculated using:\n$$\\delta_{p}=z_{p}\\left(1-z_{p}\\right)\\left[\\sum_{k=1}^{N} \\delta_{k} w_{1(p,k)}\\right]$$\nb. The weight vector w1(i,p)(n) is calculated similarly as the above formula for the adjustment of weights, with zp being replaced with xi and $\\delta_{q}$ with $\\delta_{p}$ .\n  After one epoch, let $\\Delta(n)$ and $\\Delta(n+1)$ be the previous and current errors of the outputs of the neural network respectively. The rule that is followed whether to decide if the weights are being updated or not, is:\na. If $\\Delta(n+1)\u0026gt;1.04[\\Delta(n)]$, The new weights, output, keys (always constant), error are unchanged, and $\\alpha$ is changed to $0.7 \\alpha$\nb. If $\\Delta(n+1)\u0026lt;=1.04[\\Delta(n)]$, All the variables except the keys are updated to their new values, and $\\alpha$ is modified to $1.05 \\alpha$\n  These steps are carried out and repeated in each epoch, until the maximum number of epochs has been reached, or the error becomes less than a value that is predefined.\nProcedure The keys obtained from the multiplicative neural network are first normalized to lie within the range of (0,1). The normalization is simply done by dividing the elements of the bias vector by the maximum value of the elements of the vector.\nThe images that are fed into the neural network must all be of the same dimension, irrespective of them being training images or test images. For this project, images of various dimensions (256 x 256, 512 x 512 etc) have been scaled down to a dimension of 50 x 50. The images of this specified dimension are now segmented into sub images, of the number L (for the purpose of this project, L=100). The size of each sub image is x times x = N pixels, which makes N = 25. The segmentation is done using a custom defined segmentation function, that also converts each sub image into a 1-dimensional vector, and creates a matrix of dimension N x L (25 x 100) which is then fed in as input to the neural network.\nOnce the network is trained with the training set, it is ready to encrypt and decrypt images. The test images are segmented using the same segmentation function which was used to segment the training images, and are fed into the trained neural network as input.\nThe output of the hidden layer is computed using the output function (logsig), which was previously defined. Since the size of the input image is NL, the output of the hidden layer is a matrix of size ML. (For this project, N = 25 and M = 16). The encrypted image (cipher) is then obtained after the output matrix is transformed into a 2-D matrix, for which the segmented images must be properly arranged back.\nThe decrypted image is obtained when the output of the hidden layer is fed into the output layer. In short, it is the final output of the neural network and can directly be computed by feeding the input test image into the neural network.\nExperiments and results The project was done on MATLAB version R2018b on a computer with Intel Core i5 6 th generation processor.\nThe neural network was trained using 38 test images, out of which 14 were colour images, all downloaded from the USC Vision Database. The 14 colour images and any subsequent images henceforth used for testing were all converted into single channel images. The test images were segmented and passed into the neural network, which took approximately 40 seconds to train. The initial bias vectors were specified by the programmer and then was input into the two multiplicative neural networks to generate new elements, which were then added with the previous bias vector. Only after this, the bias vector for the MLP was fixed with this value and the MLP was made to train.\nEncryption and decryption of a test image of the Earth was extremely fast, with the NPCR and UACI tests giving scores of 99.9665% and 0.34916 respectively. The PSNR ratios for the original image with the decrypted image and the cipher were 39.4156 and 39.3973 respectively.\n","date":1596090959,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596090959,"objectID":"447045694cd9d481c0198b4259569dc0","permalink":"https://archana1998.github.io/project/encryption-decryption/","publishdate":"2020-07-30T12:05:59+05:30","relpermalink":"/project/encryption-decryption/","section":"project","summary":"Developed a novel algorithm for image encryption using Artificial Neural Networks. Used a Product Neural Network to generate a unique key which served as the bias for the initial ANN, which encrypted and decrypted the image.","tags":["Cryptography"],"title":"Image Encryption and Decryption using Artificial Neural Networks","type":"project"},{"authors":["Archana Swaminathan"],"categories":["Undergraduate Projects"],"content":"Introduction The Ramanujan Sums were first proposed by Srinivasan Ramanujan in 1918, and have become exceedingly popular in the fields of signal processing,time-frequency analysis and shape recognition. The sums are by nature, orthogonal. This results in them offering excellent conservation of energy, which is a property shared by Fourier Transform as well.\nWe have used Matrix Multiplication to obtain the Ramanujan Basis, for our computation. The Ramanujan Sums are defined as nth powers of qthprimitive roots of unity, which can be computed using this simple formula:\n\\[c_{q}(n)=\\mu\\left(\\frac{q}{g c d(q, n)}\\right) \\frac{\\varphi(q)}{\\varphi\\left(\\frac{q}{g c d(q, n)}\\right)}\\]\nWhere $q=\\prod_{i} q_{i}^{a_{i}}$,(qi is prime). Then, $\\varphi(q)=q \\prod_{i}\\left(1-\\frac{1}{q i}\\right)$.\n$\\mu(n)$ is the Mobius function, which is equal to 0 if n contains a square number, 1 if n = 1 and (-1)*k if n is a product of k distinct prime numbers.\nThe Ramanujan matrix can be defined as:\n\\[A(q, j)=\\frac{1}{\\varphi(q) M} c_{q}(\\bmod (j-1, q)+1)\\]\nThe 2-D forward Ramanujan Sum Transform is given as:\n\\[Y(p, q)=\\frac{1}{\\varphi(p) \\varphi(q)} \\frac{1}{M N} \\sum_{m=1}^{M} \\sum_{n=1}^{N} x(m, n) C_{p}(m) C_{q}(n)\\]\nwhich in matrix terms can be defined as\n\\[Y=A * A^{\\top}\\]\nand the inverse 2D Ramanujan transform in matrix terms is:\n\\[X=A^{-1} Y\\left(A^{-1}\\right)^{\\top}\\]\nOriginal, Transformed and Inversed Image   Compressive Sensing The principle behind the use of compressive sensing as a signal processing technique, is that most test signals are not actually completely comprised of noise, but most have a great degree of redundancy in them. Sparse representation of signals in a particular domain signifies that most of the signal coefficients are either zero or close to zero.\nCompressive measurements, which are a weighed linear combination of signal samples, are first taken in a basis that is different from the sparse basis.\nAlgorithm We use the generated Ramanujan Basis to do the sparse reconstruction. First, the sparse signal is obtained by multiplyingthe Ramanujan Basis A with the flattened image vector (here we are using the Cameraman Image that has been resized to 50 * 50). Our Ramanujan Basis has dimensions of 2500 * 2500.\nThus,\n\\[Z=A^{*} x\\]\nWhere Z is the sparse representation of the cameraman image, and x is the flattened image vector of the original image. We next create a random measurement matrix of dimension m*n, where we keep m = 5000 and n = 2500. This measurement matrix (Phi) is then multiplied by the sparse signal z.\n\\[Y=P h i * Z\\]\nWe then use Orthogonal Matching Pursuit Algorithm, which aims to approximately find the most accurate projections of data in multiple dimensions on to the span of a redundant or overcomplete dictionary. Here, the overcomplete dictionary we use is the Ramanujan Basis A. The orthogonal matching pursuit algorithm is then applied onto the signal Y, and we have considered 1700 iterations.\nThe plot of the original sparse representation and the OMP representation is given below:\nPlot of original sparse representation (blue) and OMP representation (red)   The image is then reconstructed by taking the inverse of the Ramanujan Basis, and multiplying it with the OMP sparse representation.\n\\[R e c =A^{-1} * x w s r\\]\nWhere xwsr is the OMP representation, and Rec is the reconstructed image signal. This is then resized to obtain the final image, which has been compared with the original image below:\nOriginal and Final image   Results We compare and evaluate the performance of the Compressive Sensing Algorithm by using PSNR, SSIM and MSE image evaluation metrics.\n PSNR: Peak Signal to Noise Ratio SSIM: Structural Similarity Index MSE: Mean Square Error  Ideally, high values of PSNR, SSIM(max=1) and MSE show favourable performance of the reconstruction algorithm.The results obtained for this approach are:\n PSNR = 23.1362 SSIM = 0.6265 MSE = 315.8316  Image Denoising Image denoising is commonly analysed and solved as an inverse problem. A method of doing this is to decompose the image signal in a sparse way, over a dictionary that is overcomplete. We use the Ramanujan Dictionary here to do the denoising, which is trained with three images using the K-SVD algorithm, based on Orthogonal Matching Pursuit (OMP).\nK-SVD Algorithm The K-SVD algorithm is a type of K-means clustering, which has been generalized. The k- means clustering is also considered a method of doing representation of sparse signals. This implies solving the equation below, to find the best code to represent the signal data {yi} from i=1 to M\n\\[\\min _{D, X}\\left\\{\\|Y-D X\\|_{F}^{2}\\right\\}, \\text { subject to } \\forall i,\\left\\|x_{i}\\right\\|_{0}=1\\]\nF here is the Frobenius norm. The K-SVD algorithm is similar to the K-means in terms of the process of construction, but differs in the sense of the relaxation of the sparsity term in the constraint. This helps achieve a linear combination of the dictionary atoms. The relaxation is that the number of entries that are not zero in each column can be greater than 1, but less than a defined number T0.\nThus, the objective function hence becomes\n\\[\\min _{D, X}\\left\\{\\|Y-D X\\|_{F}^{2}\\right\\} \\text { , subject to } \\forall i,\\left\\|x_{i}\\right\\|_{0} \\leq T_{0}\\]\nIn the algorithm, the dictionary D is first fixed, and the aim is to find the perfect coefficient matrix X. To find this, a pursuit method that does the approximation of the optimal X is used. OMP was chosen as the suitable method to calculate the coefficients of the matrix here.\nImplementation and Results In the implementation for training the dictionary nd the sparse data representation, the parameters for the training are as follows:\n Patch Size = 15 Percentage of Overlap = 0.5 Sparsity Threshold = 6 Error Tolerance = 11.5 Three images, Boat, Lena and Barbara were used for the dictionary training. Patches were made of these images and stacked. The number of iterations was the size of the stacked images that formed the training data. In this case, the number was 2883. The dictionary and sparse data representation were trained using the K-SVD algorithm and saved, the training process took approximately 5.5 hours on an Intel i5 6 th gen processor, with a Nvidia 940mx GPU (personal laptop, using MATLAB R2019a).  Original and Reconstructed Boat image   We added Gaussian Noise to a Lena Image and denoised it using the trained dictionary.\nOriginal, Noisy and Denoised Lena image    MSE for noisy image = 68.0747 MSE for denoised image = 55.2772 PSNR for noisy image = 29.8009 PSNR for denoised image = 29.9672  Summary The Ramanujan Transform is a powerful transform and basis dictionary that can be used for sparse representation of an image. It can be trained efficiently and reconstructs images in a better way as compared to using DCT dictionary. For compressive sensing algorithm, the training time of the Ramanujan dictionary is more compared to the DCT dictionary training time, but it is more efficient in reconstruction. It is also good at denoising images, and is efficiently trained using the K-SVD algorithm.\n","date":1595077847,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595077847,"objectID":"72d8642b4f9a05c1ccc6178576ab9893","permalink":"https://archana1998.github.io/project/compressive-sensing/","publishdate":"2020-07-18T18:40:47+05:30","relpermalink":"/project/compressive-sensing/","section":"project","summary":"Used the Ramanujan Fourier Transform to do compressive sensing and denoising of images in the Ramanujan domain,using the Ramanujan basis as the overcomplete dictionary and trained the dictionary with K-SVD based on OMP algorithm","tags":["Image Processing"],"title":"Compressive Sensing and Denoising of Images using the Ramanujan Fourier Transform","type":"project"},{"authors":[],"categories":[],"content":"This is a review of the ICLR 2017 paper by Zhang et. al. titled \u0026ldquo;Understanding Deep Learning requires rethinking generalization\u0026quot;Link to paper \nThe paper starts off with aiming to provide an introspection into what distinguishes networks that generalize well, from those who don’t.\nOne of the experiments conducted in the studies of the paper, is checking how well neural networks adapt to training when labels are randomized. Their findings establish that when the true data is completely randomly labelled, the training error that results is zero. Observations from this indicate that the effective capacity of neural networks is more than enough to memorize the entire dataset. Randomization of the labels is only a transformation of the data, and other learning parameters are constant and unchanged still. The resulting training time also increases by only a small factor. However, when this trained network is tested, it does badly. This indicates that just by randomizing labels, the generalization error can shoot up significantly without changing any other parameters of the experiment like the size of the model, the optimizer etc.\nAnother experiment conducted was that when the ground truth images were switched with random noise. This resulted in the networks training to zero training error, even faster than the case with the random labels. Varying the amount of randomization resulted in a steady deterioration of the generalization error, as the noise level increased. There were a wide variety of changes introduced into the dataset, that played with degrees and kinds of randomization with the pixels and labels. All of this still resulted in the networks able to fit the training data perfectly. A key takeaway from this is that the neural networks are able to capture the signals remaining in the data, while fitting the noise and randomization with brute force. The question that still remains unanswered after this is why some models generalize better than others, because it is evident that some decisions made while constructing model architectures do make a difference in its ability to generalize.\nTraditional approaches in statistical learning theory such as Rademacher complexity, VC dimension and uniform stability are threatened by the randomization experiments performed.\nThree specific regularizers are then considered to note the impact of explicit regularization, data augmentation, weight decay and dropout. These are tried out on Inception, Alexnet and MLPs on the CIFAR10 dataset, and later with ImageNet. Regularization helps to improve generalization performance, but the models still generalize well enough with the regularizers turned off. It was then inferred that this is more of a tuning parameter than a fundamental cause of good generalization. A similar result was noted with implicit regularization.\nAn interesting result proved in the paper was that there two layer depth networks of linear size, that can represent any labelling of the training data. A parallel approach in trying to understand the source of regularization for linear models was also not easy to point out. To sum up, this paper presents a thorough insight into how empirically easy optimization does not imply good regularization, and effective capacity of network architectures is better understood and defined.\n","date":1595061682,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595061682,"objectID":"5a3367d62d391224abafb9e8797a3c3e","permalink":"https://archana1998.github.io/post/regularization/","publishdate":"2020-07-18T14:11:22+05:30","relpermalink":"/post/regularization/","section":"post","summary":"A short and concise review","tags":[],"title":"Understanding Deep Learning requires rethinking generalization","type":"post"}]