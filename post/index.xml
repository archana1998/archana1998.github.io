<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Archana Swaminathan</title>
    <link>https://archana1998.github.io/post/</link>
      <atom:link href="https://archana1998.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Archana Swaminathan ©</copyright><lastBuildDate>Sun, 23 Aug 2020 15:53:16 +0530</lastBuildDate>
    <image>
      <url>https://archana1998.github.io/images/icon_hu616effff6bc497e1f3ccd40e4a444d66_14554_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>https://archana1998.github.io/post/</link>
    </image>
    
    <item>
      <title>Summer School Series: Lecture 3 by Vineet Gupta</title>
      <link>https://archana1998.github.io/post/vineet-gupta/</link>
      <pubDate>Sun, 23 Aug 2020 15:53:16 +0530</pubDate>
      <guid>https://archana1998.github.io/post/vineet-gupta/</guid>
      <description>&lt;p&gt;This talk was delivered by &lt;a href=&#34;http://www-cs-students.stanford.edu/~vgupta/&#34;&gt; Vineet Gupta &lt;/a&gt;, a research scientist at Google Brain, Mountain View California.
His talk was titled &lt;b&gt;Adaptive Optimization&lt;/b&gt;.&lt;/p&gt;
&lt;h3 id=&#34;the-optimization-problem&#34;&gt;The optimization problem&lt;/h3&gt;
&lt;p&gt;The optimization problem aims to learn the best function from a class of functions.
$$\operatorname{Class} :  { \hat{y} = M(x | w), for \space w \in \mathbb{R}^{n} }\ $$&lt;/p&gt;
&lt;p&gt;A class is most often specified as a neural network, parameterized by w. If the class is too large, overfitting happens. If the class is too small, well we end up getting bad results.&lt;/p&gt;
&lt;p&gt;The most common function to find the best function is supervised learning.&lt;/p&gt;
&lt;p&gt;Training examples: input output pairs such as (x&lt;sub&gt;1&lt;/sub&gt;, y&lt;sub&gt;1&lt;/sub&gt;),&amp;hellip;.(x&lt;sub&gt;n&lt;/sub&gt;, y&lt;sub&gt;n&lt;/sub&gt;)&lt;/p&gt;
&lt;p&gt;Learning rule: Estimating $w$ such that $\hat{y_{i}} = M(x_{i}|w) \approx y_{i}$, and $w$ approximately minimizes $ F(w) = \sum_{i=1}^{n} l(\hat{y_{i}},y_{i})$ (the loss function)&lt;/p&gt;
&lt;p&gt;In a feed-forward Deep Neural Network, gradient descent for the entire training is expensive. For this reason, we sample points and find the gradient for them.&lt;/p&gt;
&lt;h3 id=&#34;stochastic-optimization&#34;&gt;Stochastic Optimization&lt;/h3&gt;
&lt;p&gt;The optimizer starts with the network denoted as $M(x|w)$.&lt;/p&gt;
&lt;p&gt;At each round t: (the goal is to minimize $F(w)$)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optimizer has decided upon $w_{t}$&lt;/li&gt;
&lt;li&gt;Optimizer receives the input $ [x_{i} ]_{i=1}^{k}$&lt;/li&gt;
&lt;li&gt;Optimizer makes prediction $[\hat{y_{i}}= M(x_{i}|w_{t})]_{i=1}^{k}$&lt;/li&gt;
&lt;li&gt;Optimizer receives the true outcome&lt;/li&gt;
&lt;li&gt;Optimizer computes the loss $l_{t} = \sum_{i} l(y_{i},\hat{y_{i}})$ and gradient $g_{t} = \frac{\partial }{\partial w} \sum_{i} l(y_{i},\hat{y_{i}})$&lt;/li&gt;
&lt;li&gt;Optimizer uses $g_{t}$ to update $w_{t}$ to get $w_{t+1}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We stop when the gradients vanish or run out of time (or epochs).&lt;/p&gt;
&lt;h3 id=&#34;regret&#34;&gt;Regret&lt;/h3&gt;
&lt;p&gt;Convergence can be defined as the average loss compared to the optimum $w^{*}$&lt;/p&gt;
&lt;p&gt;$$ R_{T} = \frac{1}{T} \sum_{t=1}^{T} l_{t} (w_{t}) - \frac{1}{T} \sum_{t=1}^{T} l_{t}(w^{*})$$&lt;/p&gt;
&lt;p&gt;The proof of convergence can be picked up when $R_{T} \rightarrow 0 \text{ as } T \rightarrow 0$. This is a very &lt;b&gt;strong &lt;/b&gt; requirement, regret tending to 0.&lt;/p&gt;
&lt;p&gt;In convex optimization, $R_{T}$ can be computed in $O(\frac{1}{\sqrt{T}})$ time. Convex problems in SGD include faster convergence implies better condition number.&lt;/p&gt;
&lt;h3 id=&#34;momentum&#34;&gt;Momentum&lt;/h3&gt;
&lt;p&gt;What happens when the gradients become very noisy? To solve this, we can take a running average of the gradients.
$$ \bar{g_{t}} = \gamma \bar{g_{t}} + (1-\gamma)g_{t}$$
Thus the momentum step becomes:
$$w_{t+1} = w_{t}-\eta_{t} \bar{g_{t}}$$
The momentum approach works very well and is extremely popular, till date.&lt;/p&gt;
&lt;p&gt;Another way to solve the problem is by using second order methods.
To minimize $F(w)$,&lt;/p&gt;
&lt;p&gt;$$F(w) \approx F(w_{t}) + (w - w_{t})^{T} \nabla F(w_{T}) + \frac{1}{2} (w - w_{t})^{T} \nabla^{2} F(w_{t}) (w - w_{t}) $$ (first two terms of the Taylor series).&lt;/p&gt;
&lt;p&gt;The minimum is at: $w_{t+1} = w_{t} - \nabla^{2} F(w_{t})^{-1} \nabla F(w_{t})$&lt;/p&gt;
&lt;p&gt;The biggest problem with this is computing the $\nabla^{2} F(w_{t})$ (Hessian) is very expensive, as it is a $n*n$ matrix with $n$ number of parameters.&lt;/p&gt;
&lt;h3 id=&#34;adagrad&#34;&gt;AdaGrad&lt;/h3&gt;
&lt;p&gt;For gradient $g_{i}$
$$ H_{t} = \sqrt{(\sum_{s\leq{t}} g_{s} g_{s}^T)} $$&lt;/p&gt;
&lt;p&gt;This is used as matrix for the Mahalnobis metric, that will be used.
$$ \therefore w_{t+1} = \operatorname{argmin}_{w} \frac{1}{2\eta} ||w - w _{t}|| _{H _{t}}^{2} +\hat{l _{t}}(w) $$&lt;/p&gt;
&lt;p&gt;The AdaGrad update rule is: $w_{t+1} = w_{t} - \eta H_{t}^{-1} g_{t}$.
This is again very expensive, $O(n^{2})$ storage and $O(n^{3})$ time complexity per step.&lt;/p&gt;
&lt;h4 id=&#34;the-solution&#34;&gt;The solution&lt;/h4&gt;
&lt;p&gt;One way to solve this is by the diagonal approximation, by taking only the diagonal matrix of the Hessian instead of the entire matrix. $$H_{t} = \operatorname{diag}{(\sum_{s\leq{t}}g_{s}g_{s}^{T}+\epsilon\operatorname{I})}^{\frac{1}{2}}$$&lt;/p&gt;
&lt;p&gt;This take $O(n)$ space and $O(n)$ time per step.&lt;/p&gt;
&lt;p&gt;AdaGrad has been so successful that there have been plenty of variants like AdaDelta/RMS Prop and Adam.&lt;/p&gt;
&lt;h3 id=&#34;full-matrix-preconditioning&#34;&gt;Full-matrix Preconditioning&lt;/h3&gt;
&lt;h4 id=&#34;adagrad-preconditioner&#34;&gt;AdaGrad Preconditioner&lt;/h4&gt;
&lt;p&gt;For $w_{t}$ of size 100 * 200, $g_{t}$ flattens to a 20,000 vector and then becomes 20k * 20k in size.&lt;/p&gt;
&lt;h4 id=&#34;the-kronecker-product&#34;&gt;The Kronecker Product&lt;/h4&gt;
&lt;p&gt;Given a $m * n$ matrix $A$ and $p * q$ matrix $B$, their &lt;b&gt;Kronecker Product&lt;/b&gt; $C$ is defined as $$C = A \bigotimes B $$
This is also called the matrix direct product, and is a $(mp)*(nq)$ matrix (every element of $A$ multiplied with $B$). It commutes with standard matrix product along with exponentials.&lt;/p&gt;
&lt;h3 id=&#34;the-shampoo-preconditioner&#34;&gt;The Shampoo Preconditioner&lt;/h3&gt;





  
  











&lt;figure id=&#34;figure-decomposed-matrix&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/vineet-gupta/1_hu82cf2efea7539d0f27dd878118672ff1_8584_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;500&#34; height=&#34;500&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Decomposed Matrix
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;the-shampoo-update&#34;&gt;The Shampoo Update:&lt;/h3&gt;
&lt;p&gt;&lt;b&gt;Adagrad update&lt;/b&gt;: ${w} _{t+1} ={w} _{t}-\eta H _{t}^{-1} {g} _{t}$&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Shampoo factorization&lt;/b&gt;: $w_{t+1}=w_{t}-\eta\left(L_{i}^{\frac{1}{4}} \otimes R_{t}^{\frac{1}{4}}\right)^{-1} g_{t}$&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Shampoo update&lt;/b&gt;: $W_{t+1}=W_{t}-\eta L_{t}^{-\frac{1}{4}} G_{t} R_{t}^{-\frac{1}{4}}$
&lt;b&gt;Theorem (convergence)&lt;/b&gt;:
If ${G} _{1}, \mathrm{G} _{2}, \ldots, \mathrm{G} _{\mathrm{T}}$ of rank $\leq \mathrm{r},$ then the rate of convergence is:&lt;/p&gt;
&lt;p&gt;$$\frac{\sqrt{\mathrm{r}}}{\mathrm{T}} \operatorname{Tr}\left(\mathrm{L} _{\mathrm{T}}^{\frac{1}{4}}\right) \operatorname{Tr}\left(\mathrm{R} _{\mathrm{T}}^{\frac{1}{4}}\right)=\mathrm{O}\left(\frac{1}{\sqrt{\mathrm{T}}}\right)$$&lt;/p&gt;
&lt;p&gt;$$({R_{t}}=\sum_{s \leq t} G_{s}^{\top} G_{s}$ and $L_{t}=\sum_{s \leq t} G_{s} G_{s}^{T})$$&lt;/p&gt;
&lt;h3 id=&#34;implementing-shampoo&#34;&gt;Implementing Shampoo&lt;/h3&gt;
&lt;p&gt;The training system can be of two types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Asynchronous (accelerators don&amp;rsquo;t need to talk to each other, however it is hard for the parameter servers to handle)&lt;/li&gt;
&lt;li&gt;Synchronous (accelerator sends gradients to all the other accelerators, for them to average and update)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;challenges&#34;&gt;Challenges&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Tensorflow and PyTorch focus on 1&lt;sup&gt;st&lt;/sup&gt; order optimizations&lt;/li&gt;
&lt;li&gt;Computing $L_{t}^{-\frac{1}{4}}$ and $R_{t}^{-\frac{1}{4}}$ is expensive&lt;/li&gt;
&lt;li&gt;L, R have large condition numbers (upto the order of 10&lt;sup&gt;13&lt;/sup&gt;).&lt;/li&gt;
&lt;li&gt;SVD is very expensive: $O(n^{3})$ in largest dimension&lt;/li&gt;
&lt;li&gt;Large layers are still impossible to precondition&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;solutions&#34;&gt;Solutions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Using high precision arithmetic (float 64), not performing the computations on a TPU.&lt;/li&gt;
&lt;li&gt;Computing preconditioners every 1000 steps is alright.&lt;/li&gt;
&lt;li&gt;Replace SVD with an iterative method&lt;/li&gt;
&lt;li&gt;Only matrix multiplications needed
&lt;ul&gt;
&lt;li&gt;Warm start: use previous preconditioner&lt;/li&gt;
&lt;li&gt;Reduce condition number, remove top singular values&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Optimization for large layers
&lt;ul&gt;
&lt;li&gt;Precondition only one dimension&lt;/li&gt;
&lt;li&gt;Block partioning the layer works better&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;shampoo-implementation-and-conclusion&#34;&gt;Shampoo implementation and conclusion&lt;/h3&gt;
&lt;p&gt;Shampoo gets implemented on a TPU+CPU. It is a little more expensive than AdaGrad but waay faster (saves 40% of the training time with 1.95 times fewer steps). Shampoo works well in language and speech domains, it isn&amp;rsquo;t suitable for image classication yet (for this Adam and AdaGrad work much better).&lt;/p&gt;
&lt;p&gt;The Shampoo paper can be found &lt;a href =&#34;https://arxiv.org/abs/1802.09568&#34;&gt; here &lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Summer School Series: Lecture 2 by Neil Houlsby</title>
      <link>https://archana1998.github.io/post/neil-houlsby/</link>
      <pubDate>Fri, 21 Aug 2020 23:11:51 +0530</pubDate>
      <guid>https://archana1998.github.io/post/neil-houlsby/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://research.google/people/NeilHoulsby/&#34;&gt;Neil Houlsby&lt;/a&gt; presented a great talk on Large Scale Visual Representation Learning and how Google has come up with solutions to some classical problems.&lt;/p&gt;
&lt;h3 id=&#34;evaluation-of-parameters&#34;&gt;Evaluation of parameters&lt;/h3&gt;
&lt;p&gt;There are two main ways of evaluating parameters from a network, that extracts the parameters. They are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear Evaluation: We freeze the weights and retrain the head&lt;/li&gt;
&lt;li&gt;Transfer Evaluation: We retrain end to end with new head&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;visual-task-adaptation-benchmark-vtab&#34;&gt;Visual Task Adaptation Benchmark (VTAB)&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://ai.googleblog.com/2019/11/the-visual-task-adaptation-benchmark.html&#34;&gt;VTAB&lt;/a&gt; is an evaluation protocal designed to measure progress towards general and useful visual representations, and consists of a suite of evaluation vision tasks that a learning algorithm must solve. We mainly have three types of tasks, &lt;b&gt; Natural tasks, Specialized tasks and Structured Datasets. &lt;/b&gt;&lt;/p&gt;
&lt;p&gt;A query that was posed was how useful ImageNet labels would be for pretrained models to work on these three tasks. It has been seen that ImageNet labels work well for Natural images, and not well for the other two tasks.&lt;/p&gt;
&lt;p&gt;Representation learners pre-trained on ImageNet can be of three forms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GANs and autoencoders&lt;/li&gt;
&lt;li&gt;Self-supervised&lt;/li&gt;
&lt;li&gt;Semi-supervised / Supervised approach&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It has been seen that for natural tasks, representations prove to be more important than obtaining more data, and the supervised approach is far better than the unsupervised approach. For structured tasks, a combination of supervised and self-supervised learning works the best.&lt;/p&gt;
&lt;p&gt;It was also mentioned that by modern standards, ImageNet is of incredibly small-scale, thus scaling models on ImageNet were not proven to be effective.&lt;/p&gt;
&lt;p&gt;Something to specifically keep in mind is that upstream can be expensive, but downstream should be cheap (in terms of both data and compute). For the upstream, examples of suitable large datasets are ImageNet-21k for supervised learning, and YouTube-8M for self-supervised learning.&lt;/p&gt;
&lt;h3 id=&#34;bit-l&#34;&gt;BiT-L&lt;/h3&gt;
&lt;p&gt;Neil introduced the &lt;a href=&#34;https://blog.tensorflow.org/2020/05/bigtransfer-bit-state-of-art-transfer-learning-computer-vision.html&#34;&gt;Big Transfer Learning (BiT-L)&lt;/a&gt; algorithm and talked about it in detail.
The first thing he mentioned about BiT-L was that batch normalization was replaced with &lt;b&gt; group normalization &lt;/b&gt; for ultra-large data. Advantages of this were having no train/test discrepancy, and no state which made it easier to co-train with multiple steps.&lt;/p&gt;
&lt;p&gt;It was highlighted that optimization at scale implies that schedule is crucial and not obvious. Also, early results of models can be misleading.&lt;/p&gt;
&lt;p&gt;To perform cheap tranfer, we need low compute, few/no validation data and diverse tasks. For doing few-shot transfer, pretraining on ImageNet-21k and JFT-300M helps.&lt;/p&gt;
&lt;h4 id=&#34;robustness&#34;&gt;Robustness&lt;/h4&gt;
&lt;p&gt;Models trained with ImageNet aren&amp;rsquo;t necessarily robust most of the times. To test OOD robustness (Out-Of-Distribution), we use datasets like ImageNet C, ImageNet R and ObjectNet.&lt;/p&gt;
&lt;h4 id=&#34;modern-transfer-learning&#34;&gt;Modern Transfer Learning&lt;/h4&gt;
&lt;p&gt;Modern Transfer Learning calls for a big, labelled datset, a big model and careful training (using about 10 optimization recipes)
While testing with OOD, increasing datset size with a fixed model and increasing dataset size leads to an increase in performance, especially in the case of very large models.&lt;/p&gt;
&lt;p&gt;To summarize, Bigger transfer $\rightarrow$ Better Accuracy $\rightarrow$ Better Robustness&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For checking impact on object &lt;b&gt; location &lt;/b&gt; invariance, we see accuracy improves and becomes more uniform across location&lt;/li&gt;
&lt;li&gt;This proves to be the same in the case of impact on object &lt;b&gt;size&lt;/b&gt; invariance&lt;/li&gt;
&lt;li&gt;However, in the case of object rotation invariance for ResNet50, it does not become more uniform across rotation angles, but for ResNet101*3, it maintains uniformity&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Main takeaways from the talk and BiT-L were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scale is one of the key drivers of representation learning performance&lt;/li&gt;
&lt;li&gt;Especially effective for few-shot learning and OOD Robustness&lt;/li&gt;
&lt;li&gt;Also seen and mirrored in language domain&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Links to the GitHub repositories are: &lt;a href =&#34;https://github.com/google-research/big_transfer&#34;&gt; Big Transfer &lt;/a&gt; and &lt;a href=&#34;https://github.com/google-research/task_adaptation&#34;&gt; Visual Task Adaptation Benchmark (VTAB)&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Summer School Series: Lecture 1 by Jean-Phillipe Vert</title>
      <link>https://archana1998.github.io/post/jean-vert/</link>
      <pubDate>Fri, 21 Aug 2020 19:07:17 +0530</pubDate>
      <guid>https://archana1998.github.io/post/jean-vert/</guid>
      <description>&lt;p&gt;This is an article about what &lt;a href =&#34;http://members.cbio.mines-paristech.fr/~jvert/&#34;&gt;Jean-Phillipe Vert&lt;/a&gt; talked about at the Google Research India-AI Summer School 2020. The lecture was titled &lt;b&gt; Differentiable Ranking and Sorting &lt;/b&gt; and lasted about 2 hours.&lt;/p&gt;
&lt;h3 id=&#34;differentiable-programming&#34;&gt;Differentiable Programming&lt;/h3&gt;
&lt;p&gt;What is machine learning and deep learning?&lt;/p&gt;
&lt;p&gt;Machine learning is just to give trained data to a program and get better results for complex problems. For example:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-a-neural-network-to-recognize-cats-and-dogs&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/jean-vert/fig1_hu2361eef24ba1250aaf0d087e444736ee_322268_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1166&#34; height=&#34;626&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    A neural network to recognize cats and dogs
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;These networks usually use &lt;b&gt;vectors&lt;/b&gt; to do the computations within the network, however in recent research models are getting extended to non-vector objects (strings, graphs etc.)&lt;/p&gt;
&lt;p&gt;Jean then gave an introduction to permutations and rankings and what he aspired to do, informally. Permutations are not vectors/graphs, but something else entirely. Some data are permutations (input, output etc) and some operations may involve ranking (histogram equalization, quantile normalization)&lt;/p&gt;
&lt;p&gt;What do these operations aspire to do?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rank pixels&lt;/li&gt;
&lt;li&gt;Extract a permutation and assign values to pixels only based on rankings&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;permutations&#34;&gt;Permutations&lt;/h3&gt;
&lt;p&gt;A permutation is formally defined as a bijection, that is:&lt;/p&gt;
&lt;p&gt;$$\sigma:[1, N] \rightarrow[1, N]$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Over here, $\sigma(i)=$ rank of item $i$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The composition property is defined as: $\left(\sigma_{1} \sigma_{2}\right)(i)=\sigma_{1}\left(\sigma_{2}(i)\right)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\mathrm{S}_{N}$ is the symmetric group and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\left|\mathbb{S}_{N}\right|=N !$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;goal&#34;&gt;Goal&lt;/h3&gt;
&lt;p&gt;Our primary goal is:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-moving-between-spaces&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/jean-vert/2_huacbb429b45f880db3bfef78880895f1e_33174_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1021&#34; height=&#34;294&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Moving between spaces
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Some definitions here are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Embed:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;To define/optimize $f_{\theta}(\sigma)=g_{\theta}($embed$(\sigma))$ for $\sigma \in \mathbb{S}_{N}$&lt;/li&gt;
&lt;li&gt;E.g., $\sigma$ given as input or output&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Differentiate:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;To define/optimize $h_{\theta}(x)=f_{\theta}($argsort$(x))$ for $x \in \mathbb{R}^{n}$&lt;/li&gt;
&lt;li&gt;E.g., normalization layer or rank-based loss&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;argmax&#34;&gt;Argmax&lt;/h3&gt;
&lt;p&gt;To put it in simple words, the argmax function identifies the dimension in a vector with the largest value. For example, $\operatorname{argmax}(2.1, -0.4, 5.8) = 3$&lt;/p&gt;
&lt;p&gt;It is not differentiable because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;As a function, $\mathbb{R}^{n} \rightarrow[1,n]$, the output space is &lt;b&gt; not continuous &lt;/b&gt;&lt;/li&gt;
&lt;li&gt;It is &lt;b&gt;piecewise constant&lt;/b&gt; (i.e, gradient = 0 almost everywhere even if the output space was continuous)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;softmax&#34;&gt;Softmax&lt;/h3&gt;
&lt;p&gt;It is a &lt;b&gt;differentiable&lt;/b&gt; function that maps from $\mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$, where&lt;/p&gt;
&lt;p&gt;$$\operatorname{softmax}_ {\epsilon} (x)_ {i} =\frac{e^{x_{i} / \epsilon}}{\sum_{j=1}^{n} e^{x_{j} / \epsilon}}$$&lt;/p&gt;
&lt;p&gt;For example, $\operatorname{softmax}(2.1, -0.4, 5.8) = (0.027, 0.02, 0.972)$&lt;/p&gt;
&lt;h3 id=&#34;moving-from-softmax-to-argmax&#34;&gt;Moving from Softmax to Argmax&lt;/h3&gt;
&lt;p&gt;$$\lim _ {\epsilon \rightarrow 0} \operatorname{softmax}_{\epsilon}(2.1,-0.4, 5.8)=(0,0,1)=\Psi(3)$$&lt;/p&gt;
&lt;p&gt;where $\psi:[1, n] \rightarrow \mathbb{R}^{n}$ is the one-hot encoding. More generally,
$$
\forall x \in \mathbb{R}^{n}, \quad \lim_ {\epsilon \rightarrow 0} \operatorname{softmax}_{\epsilon}(x)=\Psi(\operatorname{argmax}(x))
$$&lt;/p&gt;
&lt;h3 id=&#34;moving-from-argmax-to-softmax&#34;&gt;Moving from Argmax to Softmax&lt;/h3&gt;
&lt;h4 id=&#34;1-embedding&#34;&gt;1. Embedding&lt;/h4&gt;
&lt;p&gt;Let the simplex
$$
\Delta_{n-1}=\operatorname{conv}({\Psi(y): y \in[1, n]})
$$
Then we have a variational characterization (exercice left to us):
$$
\Psi(\operatorname{argmax}(x))=\underset{z \in \Delta_{n-1}}{\operatorname{argmax}}\left(x^{\top} z\right)
$$&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-simplex-representation&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/jean-vert/fig3_hu5c4a640c4ff8ae0e0520f62ab190a639_28338_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;528&#34; height=&#34;371&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Simplex representation
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;2a-regularization&#34;&gt;2a. Regularization&lt;/h4&gt;
&lt;p&gt;Let the entropy be defined as $H(z)=-\sum_{i=1}^{n} z_{i} \ln \left(z_{i}\right)$ for $z_{i} \in \Delta_{n-1}$&lt;/p&gt;
&lt;p&gt;Then we have (exercise left to us):
$$
\operatorname{softmax}_ {\epsilon}(x)=\underset{z \in \Delta_{n-1}}{\operatorname{argmax}}\left[x^{\top} z+\epsilon H(z)\right]
$$&lt;/p&gt;
&lt;p&gt;The entropy is maximum at the middle and minimum as the corners, as displayed below&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-entropy-in-the-simplex&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/jean-vert/4_hud40178e92a0e11db369c09c80b6c13e7_135667_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;485&#34; height=&#34;356&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Entropy in the simplex
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;2b-pertubation&#34;&gt;2b. Pertubation&lt;/h4&gt;
&lt;p&gt;Let $G=\left(G_{1}, \ldots, G_{n}\right)$ be i.i.d. Gumbel (0,1) random variables. Then we have (exercice):
$$
\operatorname{softmax}_{\epsilon}(x)=E \underset{z \in \Delta_{n-1}}{\operatorname{argmax}}\left[x^{\top}(z+\epsilon G)\right]
$$&lt;/p&gt;
&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;
&lt;p&gt;From moving between argmax and softmax, we can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Embed, such that
$$
\Psi(\operatorname{argmax}(x))=\underset{z \in \Delta_{n-1}}{\operatorname{argmax}}\left(x^{\top} z\right)
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Regularize or pertub:
$$
\operatorname{softmax}_ {\epsilon}(x)=\underset{z \in \Delta_{n-1}}{\operatorname{argmax}}\left[x^{\top} z+\epsilon H(z)\right] = E \underset{z \in \Delta_{n-1}}{\operatorname{argmax}}\left[x^{\top}(z+\epsilon G)\right]
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both of these lead to efficient and stochastic Jacobian estimates. We can generalize this to other discrete operations such as rankings, by various techniques, examples being the SUQUAN and Kendall embeddings.&lt;/p&gt;
&lt;p&gt;We then have to make a differentiable approximation to $\Phi (\operatorname{argsort}(x))$, which can be done using &lt;b&gt;Optimal Transport&lt;/b&gt; and &lt;b&gt;Entropic Regularization&lt;/b&gt;. It has been experimentally proven that this works faster than neural sort for sorting 5 numbers between 0 and 9999.&lt;/p&gt;
&lt;p&gt;Jean concluded saying that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Machine learning can exist beyond vectors, strings and graphs&lt;/li&gt;
&lt;li&gt;We can calculate different embeddings of symmetric groups&lt;/li&gt;
&lt;li&gt;Differentiable sorting and ranking can be done through regularization and perturbation&lt;/li&gt;
&lt;li&gt;This can be generalized to other discrete operations&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Opening Keynote at the Google AI Summer School, 2020</title>
      <link>https://archana1998.github.io/post/opening-keynote/</link>
      <pubDate>Thu, 20 Aug 2020 10:53:53 +0530</pubDate>
      <guid>https://archana1998.github.io/post/opening-keynote/</guid>
      <description>&lt;p&gt;This article has been written from notes I took throughout the Opening Keynote at the Google AI Summer School. The opening keynote was delivered by &lt;a href=&#34;https://research.google/people/jeff/&#34;&gt; Jeff Dean&lt;/a&gt;, Head of Google AI and moderated by &lt;a href=&#34;https://research.google/people/106704/&#34;&gt;Manish Gupta&lt;/a&gt;, Director of Google AI Research, Bangalore. The Keynote was titled &lt;b&gt; Deep Learning to Solve Challenging Problems. &lt;/b&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Jeff Dean is a Senior Fellow at Google and the global head of Google AI. He saved Google at a very critical time and is essential to what contributed to make the Google Search Engine the best and the fastest in the world today. He is currently doing exciting research in the field of explainable AI for problems that the world is facing. He also helped create Tensorflow, the world&amp;rsquo;s most used Machine Learning Library.&lt;/p&gt;
&lt;h2 id=&#34;notes-from-the-talk&#34;&gt;Notes from the Talk:&lt;/h2&gt;
&lt;h3 id=&#34;the-marvel-of-deep-learning&#34;&gt;The marvel of Deep Learning&lt;/h3&gt;
&lt;p&gt;Deep Learning has revolutionized the way of solving challenging problems. There are over 130 new papers on Machine Learning on Arxiv every day. Deep Learning can be considered a modern reincarnation of Artificial Neural Networks. Key benefits and features of Deep Learning are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Availability of new network architectures&lt;/li&gt;
&lt;li&gt;Ability to scale to larger datasets and efficient computation of the math&lt;/li&gt;
&lt;li&gt;Learns features from raw, noisy, heterogenous data&lt;/li&gt;
&lt;li&gt;No explicit feature engineering required&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Deep Learning architectures are remarkably flexible with taking in inputs and giving outputs of various forms, some examples are getting a categorical label from a pixel input (image), an audio input translating to a phrase that is a string, and language translation from one language to another.&lt;/p&gt;
&lt;p&gt;Deep Learning has also helped us come up with solutions to problems where the computer can achieve better results than a human. One such example is the &lt;a href=&#34;http://www.image-net.org/challenges/LSVRC/&#34;&gt;Imagenet challenge&lt;/a&gt;, that Stanford conducts every year that classifies images into classes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In 2011, the winner of the challenge was able to achieve 26% error, where humans were able to do the same task with 5% error.&lt;/li&gt;
&lt;li&gt;In 2012, &lt;a href= &#34;https://scholar.google.co.uk/citations?hl=en&amp;user=JicYPdAAAAAJ&#34;&gt;Geoffrey Hinton&lt;/a&gt; and his team used Deep Learning for the very first time in this challenge, and was the pioneer of bringing deep convolutional networks for the image classification task. Following his attempt, Deep Learning became very popular in further editions of the challenge.&lt;/li&gt;
&lt;li&gt;In 2017, the winner of the challenge was able to achieve 3% error on the Imagenet dataset, finally beating the human error of 5%.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deep-learning-to-solve-world-problems&#34;&gt;Deep Learning to solve world problems&lt;/h3&gt;
&lt;p&gt;One thing that Jeff emphasized on, is how Deep Learning is being used to tackle the &lt;a href=&#34;http://www.engineeringchallenges.org/challenges.aspx&#34;&gt;Grand Engineering Challenges of the 21st century&lt;/a&gt;
One of the primary challenges that are under focus are restoring and improving urban infrastructure.&lt;/p&gt;
&lt;p&gt;A key advancement in this field is autonomous driving, which Deep Learning has aided to such an extent that the autonomous driving is far safer than the usual human driver, with 360-degree vision utilizing around 18 cameras to form a dense LiDAR point cloud.&lt;/p&gt;
&lt;p&gt;Another field that Deep Learning revolutionalized is combining vision with robotics. For the task of a robot arm picking up an unseen object,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In 2015, there was a 65% grasp success rate&lt;/li&gt;
&lt;li&gt;In 2016, with the robot trained to pick up multiple categories of objects, the accuracy rose up to 78%&lt;/li&gt;
&lt;li&gt;In 2018, this accuracy shot up to 96% when Deep Learning was introduced into the mix
Self supervised imitation learning also uses deep learning, which is the ability of a robot to imitate actions from pixels (human footage) without supervision&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Another of these challenges was Advancing Health Informatics. We got an insight into what Google AI is working on for this field.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One of these is diagnosing diabetic retinopathy, the fastest growing cause of preventable blindness&lt;/li&gt;
&lt;li&gt;Screening of the individual can prevent blindness, however it is extremely specialized so most MD&amp;rsquo;s cannot do it.&lt;/li&gt;
&lt;li&gt;Google came up with a &lt;a href=&#34;https://ai.googleblog.com/2018/12/improving-effectiveness-of-diabetic.html&#34;&gt;model&lt;/a&gt; that could diagnose the disease from image scans of the eye, in 2016 it was at par with the performance of general opthamologists, and in 2017 the accuracy became State of the Art, with accuracy matching that of Retinal Specialists&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many of these challenges and advances in the field of engineering and technology depend on the ability to understand text. The 2017 Tranformer Paper: &lt;a href=&#34;https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&#34;&gt; Attention is all you need!&lt;/a&gt; was a revolutionary step in this direction, which was followed by &lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;BERT&lt;/a&gt; in 2018. BERT introduced principles for training that was very popular and appreciated, that are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pre train a model on the &amp;ldquo;fill in the blanks&amp;rdquo; task, using large amounts of self supervised text.&lt;/li&gt;
&lt;li&gt;This model is then fine tuned on individual language tasks, on a smaller scale.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This brought in light the desire to have large model architectures that are sparsely activated, that desirably have huge remembering capacity but utilize only a small fraction of the model while testing with individual examples
An example of this is the Per-Example Routing architecture.&lt;/p&gt;
&lt;p&gt;Jeff highlighted one of the most major contributions from Google towards deep learning, the introduction of the open-source deep learning library &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;Tensorflow&lt;/a&gt;. It remains the most popular and most downloaded Deep Learning Library until date, and has a vibrant open-source community, to the extent that only 1/3&lt;sup&gt;rd&lt;/sup&gt; of the current contributors are employees of Google!&lt;/p&gt;
&lt;h3 id=&#34;computer-architecture-for-deep-learning&#34;&gt;Computer architecture for Deep Learning&lt;/h3&gt;
&lt;p&gt;There was a time in the past where complex problems couldn&amp;rsquo;t be solved because of the lack of computational power. We have finally made strides that do not restrict the power available to us, so optimizing this is an important task.
Google AI has been focusing on redesigning computers, as Deep Learning has transformed this field completely. They kept two main things in mind while devising a computer to do deep learning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First is, reduced precision is okay. The computer does not have to calculate results acccurately to the 10&lt;sup&gt;th&lt;/sup&gt; or 20&lt;sup&gt;th&lt;/sup&gt; decimal point.&lt;/li&gt;
&lt;li&gt;Second is, there are mostly only a handful of specific operations that constitute the math of Deep Learning, for example matrix multiplication, dot products, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Keeping these in mind, Google introduced the &lt;a href=&#34;https://cloud.google.com/tpu/docs/tpus&#34;&gt;Tensor Processing Unit&lt;/a&gt;, that does just this. We can connect TPUs together to form Pods, that are currently available to the public on cloud services. Pods can be connected together to make supercomputers, that can train architectures like ResNet50 and Inceptionv2 in under 30 seconds! TPUs are being designed for edge applications also, to do deep learning on smartphones.&lt;/p&gt;
&lt;h3 id=&#34;problems-of-doing-machine-learning-today&#34;&gt;Problems of doing Machine Learning Today&lt;/h3&gt;
&lt;p&gt;The usual flow of work for a machine learning specialist is to collect the data, use his ML expertise (data augmentation, hyperparameter tuning etc) and train and test the model. A rather new approach that reduces human intervention here is &lt;a href=&#34;https://en.wikipedia.org/wiki/Automated_machine_learning&#34;&gt;AutoML&lt;/a&gt;, where the &amp;ldquo;ML expertise&amp;rdquo; is tuned and tested by automatic methods.&lt;/p&gt;
&lt;p&gt;Problems that still remain are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We still start with little to no knowledge about the problem and have to rely on random initialization&lt;/li&gt;
&lt;li&gt;New problems need significant data and compute power&lt;/li&gt;
&lt;li&gt;Transfer learning and multi-task learning help with this, but are done modestly&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;what-is-desired-to-be-achieved&#34;&gt;What is desired to be achieved&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Large but sparsely activated neural network architectures&lt;/li&gt;
&lt;li&gt;A single model that can be used to solve many tasks, by activating different parts of the network&lt;/li&gt;
&lt;li&gt;Dynamically adapting to new problems&lt;/li&gt;
&lt;li&gt;Adding new tasks easily&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus concluded the Keynote. It was fantastic and insightful. A couple of Q&amp;amp;A that I found interesting have been mentioned below:&lt;/p&gt;
&lt;p&gt;Q: What advice do you have for young researchers?&lt;/p&gt;
&lt;p&gt;A: Focus on problems that matter to you, and learn as much as you can. Create a constellation of techniques and ideas that can help you gather and organize your thoughts&lt;/p&gt;
&lt;p&gt;Q: How do you read new papers and get a gist of it?&lt;/p&gt;
&lt;p&gt;A: You&amp;rsquo;ll find many discussions on LinkedIn and Twitter about the paper, sometimes just reading this will give you a gist of what&amp;rsquo;s going on in the paper&lt;/p&gt;
&lt;p&gt;Q: Something unrealistic that you wish would happen in the field of AI in the future?&lt;/p&gt;
&lt;p&gt;A: The creation of a system that can absorb the world&amp;rsquo;s knowledge and solve all our problems&lt;/p&gt;
&lt;p&gt;Q: Hyperparameter tuning is expensive for large models, how do researchers work on this?&lt;/p&gt;
&lt;p&gt;A: Scaling down the problem to probably 1% of it and training and tuning that completely, and marginal scaling up to the level you desire is the best approach&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Flipkart Grid 2.0 Hackathon</title>
      <link>https://archana1998.github.io/post/flipkart-grid/</link>
      <pubDate>Mon, 17 Aug 2020 17:39:32 +0530</pubDate>
      <guid>https://archana1998.github.io/post/flipkart-grid/</guid>
      <description>&lt;p&gt;Flipkart recently concluded their 2 month long annual hackathon for students of Indian engineering colleges. This year’s edition saw over 20,000 participants and boasted of a prize pool of around Rs. 300,000 (-4000 USD). Our team (Gradient Ascent) made it to the 3rd round of the competition and I am writing about our experience in this article.&lt;/p&gt;
&lt;h2 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h2&gt;
&lt;p&gt;A fashion retailer wants to source ongoing and upcoming fashion trends from major online fashion portals and online magazines in a consumable and actionable format, so that they are able to effectively and efficiently design an upcoming fashion product portfolio.&lt;/p&gt;
&lt;p&gt;Deliverables:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Identify products that are better performers (in a rank ordered fashion)&lt;/li&gt;
&lt;li&gt;Help the user view the products that are both trending and lagging&lt;/li&gt;
&lt;li&gt;Identify a logic for classifying products as per their trendiness&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We were asked to complete the challenge for just the t-shirt product vertical, but to ensure that our solution would be scalable to other products as well.&lt;/p&gt;
&lt;h2 id=&#34;initial-analysis&#34;&gt;Initial Analysis&lt;/h2&gt;
&lt;p&gt;We started off by performing a literature review on current research in the field of fashion with respect to deep learning. We looked at previous attempts of learning attributes from fashion images, modelling trends as timeseries data, fashion image encodings, object detection, etc.&lt;/p&gt;
&lt;p&gt;After spending some time on our research, we split the problem into the following
subproblems to tackle independently:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Data Collection&lt;/li&gt;
&lt;li&gt;Object Detection&lt;/li&gt;
&lt;li&gt;Attribute/Feature learning&lt;/li&gt;
&lt;li&gt;Ranking&lt;/li&gt;
&lt;li&gt;Grouping (trending/lagging)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;data-collection&#34;&gt;Data Collection&lt;/h2&gt;
&lt;p&gt;According to the problem statement, we had to extract data from e-Commerce sites and other fashion portals and magazines. We tried our best to include data from all those categories to ensure we had a balanced dataset for our classification and ranking later on. After scouring the web for some good resources, we finally settled on the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Flipkart&lt;/li&gt;
&lt;li&gt;Amazon&lt;/li&gt;
&lt;li&gt;Pinterest (curated collections of fashion trends)&lt;/li&gt;
&lt;li&gt;Vogue India&lt;/li&gt;
&lt;li&gt;Myntra&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We felt this combination of multipurpose e-Commerce sites, well established fashion magazines, social network sites and dedicated fashion shopping sites would ensure we had good representation from all sectors. We collected an average of around 600 images from each website, giving us a total of 3000 to work with.
Web scraping was done in Python using the Selenium framework. The scripts used to scrape data from any website were pretty similar and any new sites could be added with minor modifications, hence this step was easily scalable.
From e-commerce sites, we scraped the images, product names, ratings and the number of reviews to with ranking later on. From the other portals, we extracted just the images.&lt;/p&gt;
&lt;h2 id=&#34;object-detection&#34;&gt;Object Detection&lt;/h2&gt;
&lt;p&gt;One of the biggest problems we faced when extracting images from fashion magazines and social media sites is that they don’t limit themselves to just t-shirts. When they put out a catalogue/collection, it has everything ranging from skirts to sweaters to scarves. Furthermore, even in pictures where the shirt was the highlight, other features such as the model’s pose, skin colour and distance from the camera could confuse our model in the later stages of this project. Keeping all this in mind, we decided to use an object detection model to filter our data to ensure we had only pictures of t-shirts. Additionally, we cropped the images according to their bounding boxes to counter the other aforementioned problems.
This was done using a pretrained YOLOv3 model trained on the DeepFashion2 dataset, implemented using PyTorch.&lt;/p&gt;
&lt;h2 id=&#34;attributefeature-learning&#34;&gt;Attribute/Feature learning&lt;/h2&gt;
&lt;p&gt;This is where we faced our major setback. Our initial plan was to train a model to learn the attributes (neck type, sleeve length, patterns, etc) and to return them back for later use. We were then going to perform FP growth on our set of attributes of each image to obtain the frequent itemsets which would correspond to the most common combination of features and hence, trending/popular styles.&lt;/p&gt;
&lt;p&gt;It didn&amp;rsquo;t work out however, as we couldn’t find an appropriate dataset to work with such a task given our time constraints so we had to try out our backup plan.&lt;/p&gt;
&lt;p&gt;Our plan involved getting numeric encodings for the fashion images in place of the attribute list and performing clustering on the encodings. The largest clusters would correspond to the most popular types of clothes, and similarly, the smallest clusters would represent the lagging ones, assuming our calculated encodings are a fair representation of the original image. Since we were working with just images (unlabeled) data, we had to devise an unsupervised approach for learning the image encodings. After considering various options, we decided to go ahead using an autoencoder based on a CNN architecture. We did this for 2 major reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Convolutional layers would help notice particular features of t-shirts such as the necktype length and patterns if any&lt;/li&gt;
&lt;li&gt;We can insight on how accurate our encodings to reconstruct the image&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here’s a summary of the model we used:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-frequent-itemset-mining&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/flipkart-grid/1_hud8ee6b60bc41ae2f19ca216ed0cbf7e3_180328_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1265&#34; height=&#34;177&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Frequent Itemset Mining
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;We then plotted some of the reconstructed images side by side with their original counterparts and got pretty good results considering the simplicity of the network and size of the dataset. The encodings were able to capture some important features of the clothes in question.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-model-architecture&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/flipkart-grid/2_huf8d20e2591676411ac2452c151a71b7c_32522_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;352&#34; height=&#34;613&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Model Architecture
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;ranking&#34;&gt;Ranking&lt;/h2&gt;
&lt;p&gt;As far as e-commerce sites go, there are 2 main criteria used to determine how “good” a product is – the number of reviews and the rating it has. What would you consider to be better? 10 reviews with a 5-star rating? Or 50 reviews with a 4.7-star rating? This was the major question we had to answer to be able to rank these products properly. We needed an effective way of combining these 2 into one reliable metric. After doing some research on this area and tying out different methods of combing them, we settled with an approach based on  a Bayesian view of the beta distribution, described beautifully in this video by &lt;a href =&#34; https://www.youtube.com/watch?v=8idr1WZ1A7Q&amp;feature=emb_logo&#34;&gt;3blue1brown&lt;/a&gt;
We used this principle to come up with our own “Popularity Metric” which was calculated as follows:&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-reconstructed-images-from-encodings&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/flipkart-grid/3_hua9d9b1a4f907d92d1804ac04cfd1acaa_208064_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;1093&#34; height=&#34;234&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Reconstructed Images from encodings
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;We now had a mechanism to compare and rank products effectively and a way to calculate accurate image encodings. We used both of these to train a model which predicts the Popularity Metric of a given clothing item given an input as the image encoding. We envisioned such a model to be extremely useful for designers that are looking for insight as to how their clothes might fair if they were put up for sale on e-commerce websites. Furthermore, the Popularity Metric could be calculated for all the images from magazines and portals like Vogue and Pinterest, so those products can be ranked and compared too!
The architecture, simplified pipeline and a screenshot of the program in action are shared below.&lt;/p&gt;





  
  











&lt;figure id=&#34;figure-popularity-metric&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/flipkart-grid/4_hu67e707b293e8bb2e83945f78f4b29e05_7331_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;238&#34; height=&#34;51&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Popularity Metric
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;center&gt; (n = number of reviews, s = star rating ) &lt;/center&gt;





  
  











&lt;figure id=&#34;figure-popularity-metric-model&#34;&gt;



  &lt;img data-src=&#34;https://archana1998.github.io/post/flipkart-grid/5_hu1fc31ec500f87e3d3c214e201fb0429c_18291_2000x2000_fit_lanczos_2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;540&#34; height=&#34;306&#34;&gt;



  
  
  &lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; class=&#34;numbered&#34;&gt;
    Popularity Metric Model
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;grouping&#34;&gt;Grouping&lt;/h2&gt;
&lt;p&gt;Since the FP growth idea fell through the roof, we went with clustering as our method of choice for grouping products in such a way that we can obtain the trending and lagging items. To ensure our clustering was done well, we experimented on a variety of clustering algorithms and chose the one with the highest silhouette coefficient. The algorithms tested were –&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;K means&lt;/li&gt;
&lt;li&gt;Gaussian mixture model&lt;/li&gt;
&lt;li&gt;DBSCAN&lt;/li&gt;
&lt;li&gt;Mini batch k means&lt;/li&gt;
&lt;li&gt;Spectral clustering&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Among those, K means had the highest silhouette efficient so we went ahead with that. We then split the data into clusters according to how many images were being considered for clustering (no. of clusters = no. of images/10). The products in the largest cluster could be inferred as the trending/popular products and those in the smallest clusters would be lagging products. We gave the user the option to spec which sources they wanted to consider for their clustering, giving them more flexibility with regards to analyzing what’s not and what’s not (what’s trending on Vogue might not be popular on Amazon).&lt;/p&gt;
&lt;p&gt;To conclude, we were able to come up with a way to rank products properly and to group them based on whether they are trending or lagging. We also ensured that our solution is scalable on 2 fronts:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Getting more data can be done easily with minor modifications to the existing script&lt;/li&gt;
&lt;li&gt;We can expand to different product verticals by changing the object of interest in the object detection model&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The link to the GitHub Repo is at the top of this page.
Hope you found this interesting, thanks for reading!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Deep Learning requires rethinking generalization</title>
      <link>https://archana1998.github.io/post/regularization/</link>
      <pubDate>Sat, 18 Jul 2020 14:11:22 +0530</pubDate>
      <guid>https://archana1998.github.io/post/regularization/</guid>
      <description>&lt;p&gt;This is a review of the ICLR 2017 paper by Zhang et. al. titled &amp;ldquo;Understanding Deep Learning requires rethinking generalization&amp;quot;&lt;a href = &#34;https://bengio.abracadoudou.com/cv/publications/pdf/zhang_2017_iclr.pdf&#34;&gt; Link to paper &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The paper starts off with aiming to provide an introspection into what distinguishes networks that generalize well, from those who don’t.&lt;/p&gt;
&lt;p&gt;One of the experiments conducted in the studies of the paper, is checking how well neural networks adapt to training when labels are randomized. Their findings establish that when the true data is completely randomly labelled, the training error that results is zero. Observations from this indicate that the effective capacity of neural networks is more than enough to memorize the entire dataset. Randomization of the labels is only a transformation of the data, and other learning parameters are constant and unchanged still. The resulting training time also increases by only a small factor. However, when this trained network is tested, it does badly. This indicates that just by randomizing labels, the generalization error can shoot up significantly without changing any other parameters of the experiment like the size of the model, the optimizer etc.&lt;/p&gt;
&lt;p&gt;Another experiment conducted was that when the ground truth images were switched with random noise. This resulted in the networks training to zero training error, even faster than the case with the random labels. Varying the amount of randomization resulted in a steady deterioration of the generalization error, as the noise level increased. There were a wide variety of changes introduced into the dataset, that played with degrees and kinds of randomization with the pixels and labels. All of this still resulted in the networks able to fit the training data perfectly. A key takeaway from this is that the neural networks are able to capture the signals remaining in the data, while fitting the noise and randomization with brute force. The question that still remains unanswered after this is why some models generalize better than others, because it is evident that some decisions made while constructing model architectures do make a difference in its ability to generalize.&lt;/p&gt;
&lt;p&gt;Traditional approaches in statistical learning theory such as Rademacher complexity, VC dimension and uniform stability are threatened by the randomization experiments performed.&lt;/p&gt;
&lt;p&gt;Three specific regularizers are then considered to note the impact of explicit regularization, data augmentation, weight decay and dropout. These are tried out on Inception, Alexnet and MLPs on the CIFAR10 dataset, and later with ImageNet. Regularization helps to improve generalization performance, but the models still generalize well enough with the regularizers turned off. It was then inferred that this is more of a tuning parameter than a fundamental cause of good generalization. A similar result was noted with implicit regularization.&lt;/p&gt;
&lt;p&gt;An interesting result proved in the paper was that there two layer depth networks of linear size, that can represent any labelling of the training data. A parallel approach in trying to understand the source of regularization for linear models was also not easy to point out.
To sum up, this paper presents a thorough insight into how empirically easy optimization does not imply good regularization, and effective capacity of network architectures is better understood and defined.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
